{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJj4Ga6rZ7NYVzq99HVtsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment1_NB_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykgkcRVuf_ff"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_validate\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l31ENtp0gCLt"
      },
      "source": [
        "my_tar = tarfile.open('/content/lingspam_public.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n",
        "train_path = '/content/lingspam_public/lemm_stop/part1'  # for training      #spams: spmsg*.txt\n",
        "test_path = '/content/lingspam_public/lemm_stop/part10'   # for testing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7NlgNF1gEYW",
        "outputId": "23126ecc-0654-4521-f8b0-d40977978d4b"
      },
      "source": [
        "def to_dict (path):\n",
        "  data_dict = dict()\n",
        "  data_dict[1] = []\n",
        "  data_dict[0] = []\n",
        "  for file in os.listdir(path):  \n",
        "    doc = open (path + '/'+ file, 'r')\n",
        "    if 'spmsg' in file:\n",
        "      data_dict[1].append(doc.read())\n",
        "    else:\n",
        "      data_dict[0].append(doc.read())\n",
        "  print ('number of spams: {}'.format(len(data_dict[1])))\n",
        "  print ('number of not_spams: {}'.format(len(data_dict[0])))\n",
        "  n_docs = len(os.listdir(path))\n",
        "  return data_dict, n_docs\n",
        "\n",
        "print('training set:')\n",
        "training, n_docs_train = to_dict (train_path)\n",
        "print('number of doc: {}'.format(n_docs_train))\n",
        "\n",
        "print('\\ntesting set:')\n",
        "testing, n_docs_test = to_dict (test_path)\n",
        "print('number of doc: {}'.format(n_docs_test))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set:\n",
            "number of spams: 48\n",
            "number of not_spams: 241\n",
            "number of doc: 289\n",
            "\n",
            "testing set:\n",
            "number of spams: 49\n",
            "number of not_spams: 242\n",
            "number of doc: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki_OX4JlgHb0",
        "outputId": "5d3f8420-6f50-4c38-b5b4-fce4bcf5e340"
      },
      "source": [
        "def tokenizer (doc):\n",
        "  doc = doc.lower()\n",
        "  doc_clean = doc.lower()\n",
        "  return re.split(\"\\W+\", doc)\n",
        "\n",
        "def to_bow (data_dict):\n",
        "  bows = {}\n",
        "  bows[0], bows[1] = [], []\n",
        "  for doc in data_dict[0]:\n",
        "    bows[0].extend(tokenizer(doc))\n",
        "  for doc in data_dict[1]:\n",
        "    bows[1].extend(tokenizer(doc))\n",
        "  return bows\n",
        "\n",
        "def train_NB (training):\n",
        "  bows = to_bow (training)\n",
        "  set_V = set(bows[1] + bows[0])\n",
        "  print(len(bows[1] + bows[0]), len(set_V))\n",
        "  results={}\n",
        "  for c in training.keys():\n",
        "    results[c]={}\n",
        "    loglikelihood_c = {}\n",
        "    n_c = len(training[c])\n",
        "    logprior_c = np.log(n_c/n_docs_train)\n",
        "    print('log prior class {}: {}'. format(c, logprior_c))\n",
        "    count_w_c = {}\n",
        "    for doc in training[c]:\n",
        "      for token in tokenizer(doc):\n",
        "        count_w_c[token] = count_w_c.get(token, 0)+1\n",
        "    print ('length of word count {}: {}'.format(c, len(count_w_c.keys())))\n",
        "    for w in set_V:\n",
        "      if w in count_w_c.keys():\n",
        "        count_w = count_w_c[w]\n",
        "      else:\n",
        "        count_w = 0\n",
        "      loglikelihood_w = np.log(count_w+1/(sum(count_w_c.values())+len(set_V)))\n",
        "      loglikelihood_c[w] = loglikelihood_w\n",
        "\n",
        "    results[c]['likelihood_w']=loglikelihood_c\n",
        "    print('sum word count in {}: {}'. format(c, sum(count_w_c.values())))\n",
        "    results[c]['logprior_c']=logprior_c\n",
        "    results[c]['set_V']=set_V\n",
        "    print('length of set V: ', len(set_V))\n",
        "  #print(results[1]['likelihood_w'])\n",
        "  return results\n",
        "\n",
        "train_result = train_NB (training)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68474 11315\n",
            "log prior class 1: -1.7952256772045412\n",
            "length of word count 1: 2972\n",
            "sum word count in 1: 21015\n",
            "length of set V:  11315\n",
            "log prior class 0: -0.18162975462177716\n",
            "length of word count 0: 10031\n",
            "sum word count in 0: 47459\n",
            "length of set V:  11315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0qcXlnBkAOd",
        "outputId": "11c7ce8b-7e99-464e-d50d-6e24fb48f434"
      },
      "source": [
        "train_result[1].keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['likelihood_w', 'logprior_c', 'set_V'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2GZmGBNgNFK",
        "outputId": "6ecfcb1e-f58a-4efe-f53f-8ee98bdf6862"
      },
      "source": [
        "X_train = training[0]+ training[1]\n",
        "y_train = [0]*len(training[0]) + [1]*len(training[1])\n",
        "\n",
        "X_test = testing[0]+ testing[1]\n",
        "y_true = [0]*len(testing[0]) + [1]*len(testing[1])\n",
        "def test_NB (testing):\n",
        "  prediction = []\n",
        "  X_test = testing[0]+ testing[1]\n",
        "  for doc in X_test:\n",
        "    spam_loglikelihoods, nonspam_loglikelihoods = [], []\n",
        "    spam_score=0\n",
        "    nonspam_score = 0\n",
        "    for w in tokenizer (doc):\n",
        "      if w not in train_result[0]['set_V']: continue\n",
        "      if w in train_result[1]['likelihood_w'].keys(): \n",
        "        spam_score += train_result[1]['likelihood_w'][w] \n",
        "      else:\n",
        "        spam_score += 0\n",
        "      if w in train_result[0]['likelihood_w'].keys(): \n",
        "        nonspam_score += train_result[0]['likelihood_w'][w] \n",
        "      else:\n",
        "        nonspam_score += 0\n",
        "    \n",
        "    #   spam_loglikelihoods.append(train_result[1]['likelihood_w'][w] if w in train_result[1]['likelihood_w'].keys() else 0)\n",
        "    #   nonspam_loglikelihoods.append(train_result[0]['likelihood_w'][w] if w in train_result[0]['likelihood_w'].keys() else 0)\n",
        "    # spam_score += sum(spam_loglikelihoods)\n",
        "    # nonspam_score += sum(nonspam_loglikelihoods)\n",
        "\n",
        "    spam_logprior = train_result[1]['logprior_c']\n",
        "    nonspam_logprior = train_result[0]['logprior_c']\n",
        "    spam_score += spam_logprior\n",
        "    nonspam_score += nonspam_logprior\n",
        "    if spam_score > nonspam_score:\n",
        "      prediction.append(1)\n",
        "    else:\n",
        "      prediction.append(0)\n",
        "  return prediction\n",
        "\n",
        "y_pred = test_NB (testing)\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, test_NB(training))))\n",
        "print(classification_report(y_train, test_NB(training)))\n",
        "\n",
        "print('\\naccuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training set: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       241\n",
            "           1       1.00      1.00      1.00        48\n",
            "\n",
            "    accuracy                           1.00       289\n",
            "   macro avg       1.00      1.00      1.00       289\n",
            "weighted avg       1.00      1.00      1.00       289\n",
            "\n",
            "\n",
            "accuracy on test set: 0.9037800687285223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95       242\n",
            "           1       0.96      0.45      0.61        49\n",
            "\n",
            "    accuracy                           0.90       291\n",
            "   macro avg       0.93      0.72      0.78       291\n",
            "weighted avg       0.91      0.90      0.89       291\n",
            "\n"
          ]
        }
      ]
    }
  ]
}