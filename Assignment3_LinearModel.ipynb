{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbDFGYniXNjxkzyliIjn+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment3_LinearModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZTlx8cNJtee",
        "outputId": "6e524cbe-dcc7-4c74-c898-c9da89d76ad4"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc2pTdPrJTp8"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q_HpztUInR9",
        "outputId": "36491ba9-bfe2-4d41-9570-e1ef718eee55"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-16 01:19:48--  http://www.cs.cornell.edu/people/pabo/movie-review-data/scale_data.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4029756 (3.8M) [application/x-gzip]\n",
            "Saving to: ‘scale_data.tar.gz.6’\n",
            "\n",
            "scale_data.tar.gz.6 100%[===================>]   3.84M  2.47MB/s    in 1.6s    \n",
            "\n",
            "2021-10-16 01:19:50 (2.47 MB/s) - ‘scale_data.tar.gz.6’ saved [4029756/4029756]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxlFbhDlJcT7"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/scale_data.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "oXoCmCDBNUac",
        "outputId": "e3b3ecb1-3d5e-4cb0-f677-f19e10c0d322"
      },
      "source": [
        "# Reformatting the dataset into a single dataframe for our convinience\n",
        "id, review, rating, cl_3, cl_4 = [],[],[],[], []\n",
        "for root in glob.glob ('/content/scaledata/*'):\n",
        "  for path in glob.glob(root+'/*'): \n",
        "      fo = open(path)\n",
        "      doc = fo.read()\n",
        "      if 'id' in path:\n",
        "        id.extend(doc.split('\\n'))\n",
        "      elif 'subj' in path:\n",
        "          review.extend(doc.split('\\n'))\n",
        "      elif 'rating' in path:\n",
        "        rating.extend(doc.split('\\n'))\n",
        "      elif '3class' in path:\n",
        "        cl_3.extend(doc.split('\\n'))\n",
        "      else:\n",
        "        cl_4.extend(doc.split('\\n'))\n",
        "\n",
        "print (len(id), len(review), len(rating), len(cl_3), len(cl_4))\n",
        "df = pd.DataFrame(zip(id, review, rating, cl_3, cl_4), columns = ['id', 'review', 'rating', '3class', '4class'])\n",
        "\n",
        "# Using these 2 columns to build a dataset for a rating-based regressor\n",
        "new_df = df[['review', 'rating']].dropna()\n",
        "new_df = new_df[new_df['rating'] != '']\n",
        "new_df['rating'] =new_df['rating'].apply(lambda x: float(x))\n",
        "\n",
        "# Using these 2 columns to build a dataset for a 3-class classifier\n",
        "data = df[['review', '3class']].dropna()\n",
        "data = data[data['3class']!='']\n",
        "data\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5010 5010 5010 5010 5010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>3class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in my opinion , a movie reviewer's most import...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you can watch this movie , that is based on a ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>this is asking a lot to believe , and though i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no heroes and no story are the main attributes...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is not an art movie , yet i saw it an art...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5004</th>\n",
              "      <td>the conventional wisdom is that movie sequels ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5005</th>\n",
              "      <td>nicolas roeg's mesmerizing 1971 film walkabout...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5006</th>\n",
              "      <td>the movie air force one should require a docto...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5007</th>\n",
              "      <td>\" well , jones , at least you haven't forgotte...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5008</th>\n",
              "      <td>in a time of bloated productions where special...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5006 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 review 3class\n",
              "0     in my opinion , a movie reviewer's most import...      0\n",
              "1     you can watch this movie , that is based on a ...      0\n",
              "2     this is asking a lot to believe , and though i...      0\n",
              "3     no heroes and no story are the main attributes...      0\n",
              "4     this is not an art movie , yet i saw it an art...      0\n",
              "...                                                 ...    ...\n",
              "5004  the conventional wisdom is that movie sequels ...      2\n",
              "5005  nicolas roeg's mesmerizing 1971 film walkabout...      2\n",
              "5006  the movie air force one should require a docto...      2\n",
              "5007  \" well , jones , at least you haven't forgotte...      2\n",
              "5008  in a time of bloated productions where special...      2\n",
              "\n",
              "[5006 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvO0V2e8Oqso"
      },
      "source": [
        "# Splitting the data and vectorizing it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqCYX1GyNL0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c94759-6ee4-45d8-a24e-7551bf1d52f8"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split (data['review'], data['3class'] , train_size = 0.8, random_state = 42, shuffle = True, stratify=data['3class'])\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (4004,) (4004,)\n",
            "Shapes of X_test, y_test:  (1002,) (1002,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyzvklgbOvfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02df1ebc-b49c-4bdd-8633-8aa153df64f8"
      },
      "source": [
        "# Vectorizing the documents\n",
        "vectorizer = CountVectorizer(binary = True)\n",
        "X_train = vectorizer.fit_transform(X_train.to_list())\n",
        "X_test = vectorizer.transform(X_test.to_list())\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (4004, 38629) (4004,)\n",
            "Shapes of X_test, y_test:  (1002, 38629) (1002,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCkAJ00RH7D"
      },
      "source": [
        "# Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APzuE9I8hDnE"
      },
      "source": [
        "### Default model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWkVLHPYRLGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf94f41-4148-4f76-9f09-d75b930d2e00"
      },
      "source": [
        "\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average = 'weighted')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average = 'weighted')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred, average = 'weighted')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1\n",
        "\n",
        "def training (penalty=''): \n",
        "  if penalty is True:\n",
        "    SGD = SGDClassifier( penalty=penalty, shuffle=True).fit(X_train, y_train)\n",
        "  else:\n",
        "    SGD = SGDClassifier(shuffle=True).fit(X_train, y_train)\n",
        "  y_pred = SGD.predict(X_test)\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, SGD.predict(X_train), report = False)\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_pred, report=True)\n",
        "\n",
        "\n",
        "print('--------default sgd model-----------')\n",
        "training (penalty='default')\n",
        "\n",
        "penalties = ['l1','l2']\n",
        "for penalty in penalties:\n",
        "  print('\\n-------training linear model with penalty = {}'.format(penalty ))\n",
        "  training (penalty=penalty)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------default sgd model-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.658\n",
            "precision score: 0.657\n",
            "recall score: 0.658\n",
            "F1 score: 0.657\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.66      0.65       240\n",
            "           1       0.60      0.58      0.59       383\n",
            "           2       0.73      0.73      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.65      0.66      0.66      1002\n",
            "weighted avg       0.66      0.66      0.66      1002\n",
            "\n",
            "\n",
            "-------training linear model with penalty = l1\n",
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.664\n",
            "precision score: 0.664\n",
            "recall score: 0.664\n",
            "F1 score: 0.664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.64      0.65       240\n",
            "           1       0.60      0.61      0.60       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.66      0.66      0.66      1002\n",
            "weighted avg       0.66      0.66      0.66      1002\n",
            "\n",
            "\n",
            "-------training linear model with penalty = l2\n",
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.651\n",
            "precision score: 0.649\n",
            "recall score: 0.651\n",
            "F1 score: 0.648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.58      0.62       240\n",
            "           1       0.59      0.57      0.58       383\n",
            "           2       0.69      0.78      0.73       379\n",
            "\n",
            "    accuracy                           0.65      1002\n",
            "   macro avg       0.65      0.64      0.64      1002\n",
            "weighted avg       0.65      0.65      0.65      1002\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw8gnX8LhIpc"
      },
      "source": [
        "**1. What is the performance of the trained classifier on the test dataset?**\n",
        "The model has the overfitting issue. Although the model got maximum performance in the training set, it only got from 0.64 to 0.67 on F1 score. \n",
        "\n",
        "**2. Which regularization is performing the best on the test dataset: L1 or L2? Use the default settings.**\n",
        "As shown above, the default model with L1 regulazation achieved a better result, with a F1 score of 0.664 on the test set while using L2 regulation, the model got a slighly lower F1 score, 0.648."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGbctRPkhP_E"
      },
      "source": [
        "### Searching the best parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8hoW9GIeuZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f819bf1-8cc2-4fb4-f017-40d17b598501"
      },
      "source": [
        "# Set the parameters by cross-validation\n",
        "tuned_parameters = [{'alpha': [i for i in np.arange (0.0001, 0.01, 0.001 )]}] #, 'loss': ['log','hinge', 'perceptron']}]\n",
        "\n",
        "regularizations = ['l1', 'l2']\n",
        "for l in regularizations:\n",
        "  print ('---------with {}----------'.format(l))\n",
        "  clf = GridSearchCV(SGDClassifier(penalty = l, shuffle=True), tuned_parameters, scoring='f1_macro', cv = 5, refit = True, n_jobs = -1 )\n",
        "  clf.fit(X_train, y_train)\n",
        "  print('Best parameters set found on development set: {}'.format(clf.best_params_))\n",
        "  y_pred =  clf.predict(X_test)\n",
        "  printing_eval_scores (y_test, y_pred, report=True)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------with L1----------\n",
            "Best parameters set found on development set: {'alpha': 0.0001}\n",
            "accuracy score: 0.647\n",
            "precision score: 0.645\n",
            "recall score: 0.647\n",
            "F1 score: 0.646\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.61      0.63       240\n",
            "           1       0.58      0.58      0.58       383\n",
            "           2       0.70      0.74      0.72       379\n",
            "\n",
            "    accuracy                           0.65      1002\n",
            "   macro avg       0.65      0.64      0.64      1002\n",
            "weighted avg       0.65      0.65      0.65      1002\n",
            "\n",
            "---------with L2----------\n",
            "Best parameters set found on development set: {'alpha': 0.0091}\n",
            "accuracy score: 0.648\n",
            "precision score: 0.648\n",
            "recall score: 0.648\n",
            "F1 score: 0.648\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.61      0.62       240\n",
            "           1       0.58      0.59      0.59       383\n",
            "           2       0.73      0.73      0.73       379\n",
            "\n",
            "    accuracy                           0.65      1002\n",
            "   macro avg       0.65      0.64      0.64      1002\n",
            "weighted avg       0.65      0.65      0.65      1002\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvu--a2FwYvz"
      },
      "source": [
        "**3. We will use cross-validation to get the best value of the α parameter for regularization. What is the value of the α? Is regularization important? You can use 5-fold cross validation. Note that the cross-validation should be used ONLY on the training dataset.**\n",
        "\n",
        "We used gridsearch function built in Sklearn to search for the best α from 0.0001 to 0.01, with 5 folds on training set. We found that with L2 regularization, the model got the best performance on F1 score (0.648) with ***α=0.0091***. We also found that regularization affects selection of the best α because with L1 regularization, the best α is ***α=0.0001*** ; however, it seems L1 and L2 resulted in quite similar performances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmg1ajLk_qF2"
      },
      "source": [
        "**(4) What kind of multi-class strategy does SGDClassifier use?***\n",
        "\n",
        "SGDClassifier used “one versus all” (OVA) strategy to handle multi-class strategy. For each of K classes, the binary classifier will try to predict that class and the rest other K-1 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9NHpIk4_icm"
      },
      "source": [
        "**5. In any regularization: which features are ranked the highest? You can sort the feature weights.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF8SMd919XVH",
        "outputId": "ee200e68-2209-44b1-d612-cea85a94f848"
      },
      "source": [
        "# Printing features with highest weights.\n",
        "best_SGD = SGDClassifier( penalty='l2',  shuffle=True, alpha=0.0091).fit(X_train, y_train)\n",
        "y_pred = best_SGD.predict(X_test)\n",
        "weights = best_SGD.coef_\n",
        "weights"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.01614026, -0.03873663, -0.03550857, ..., -0.00080701,\n",
              "        -0.00161403,  0.00080701],\n",
              "       [ 0.04432735, -0.04116111, -0.01899744, ..., -0.02005285,\n",
              "         0.00633248, -0.01213725],\n",
              "       [-0.08745872,  0.062593  ,  0.05487606, ...,  0.01800621,\n",
              "         0.00342975,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k41E0kWSG7_p",
        "outputId": "910d807d-53c9-4c61-e11d-46f12823c191"
      },
      "source": [
        "features = vectorizer.get_feature_names()\n",
        "\n",
        "feature_weights = {}\n",
        "for i in range(len(weights)):\n",
        "  print('\\n==============================================')\n",
        "  print ('\\nclass: {}'.format(i+1))\n",
        "  for w in range(len(weights[i])):\n",
        "     feature_weights[features[w]] = weights[i][w] #np.abs\n",
        "  # Sorting the dictionary in descending order\n",
        "  sorted_feature_weights = {k:v for k, v in sorted(feature_weights.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "  # Print the weights learned for each class\n",
        "  print('10 most important features (in descending order): ')\n",
        "  for k, v in list(sorted_feature_weights.items())[:20]:\n",
        "    print ('{}: {:.5f}'. format(k,v))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================\n",
            "\n",
            "class: 1\n",
            "10 most important features (in descending order): \n",
            "bad: 0.28165\n",
            "boring: 0.25744\n",
            "ridiculous: 0.24614\n",
            "awake: 0.23807\n",
            "watchable: 0.22112\n",
            "suppose: 0.22031\n",
            "dull: 0.21628\n",
            "unfunny: 0.21224\n",
            "inane: 0.20740\n",
            "failed: 0.19449\n",
            "worst: 0.19288\n",
            "flat: 0.18238\n",
            "routine: 0.18077\n",
            "poorly: 0.17996\n",
            "remotely: 0.17916\n",
            "sitcom: 0.17431\n",
            "contrived: 0.17109\n",
            "peter: 0.16786\n",
            "attempt: 0.16786\n",
            "wasted: 0.16786\n",
            "\n",
            "==============================================\n",
            "\n",
            "class: 2\n",
            "10 most important features (in descending order): \n",
            "problems: 0.29393\n",
            "able: 0.26860\n",
            "unfortunately: 0.26333\n",
            "problem: 0.25752\n",
            "fits: 0.23377\n",
            "ve: 0.23272\n",
            "leads: 0.23008\n",
            "nice: 0.22797\n",
            "albeit: 0.22744\n",
            "add: 0.22691\n",
            "myself: 0.22639\n",
            "marriage: 0.22428\n",
            "changes: 0.22216\n",
            "thumbs: 0.22111\n",
            "entertaining: 0.21847\n",
            "though: 0.21636\n",
            "cute: 0.21583\n",
            "interesting: 0.20317\n",
            "cool: 0.19842\n",
            "in: 0.19525\n",
            "\n",
            "==============================================\n",
            "\n",
            "class: 3\n",
            "10 most important features (in descending order): \n",
            "great: 0.23580\n",
            "traditional: 0.22636\n",
            "perfect: 0.22036\n",
            "intelligent: 0.21522\n",
            "delightful: 0.21179\n",
            "favorite: 0.21179\n",
            "turns: 0.19807\n",
            "terrific: 0.19807\n",
            "strongly: 0.19635\n",
            "wonderfully: 0.19207\n",
            "intensity: 0.18778\n",
            "superb: 0.18092\n",
            "charming: 0.17749\n",
            "simple: 0.17492\n",
            "satisfying: 0.17492\n",
            "false: 0.17406\n",
            "world: 0.16977\n",
            "elements: 0.16720\n",
            "fascinating: 0.16463\n",
            "master: 0.16206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfTygJKs__w6"
      },
      "source": [
        "The above output includes the most important features according to weights. The higher learned weights, the more important the features are. Looking at the most important features in each class, we can see they make a lot of sense. For example, the most important features in the class 0 (negative) are negative sentiment words such as bad, boring, ridiculous, dull, unfunny, failed, worst, etc. while the positive class (class 2) includes very positive words as the most important features such as: great, perfect, intelligent, delightful,\n",
        "favorite, terrific, strongly, wonderfully, intensity, etc. The neutral class contains some possitive words, and some negative words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEf_4Mt6IBJC"
      },
      "source": [
        "**Q6. How does the hyperplane of the classifier look like?**\n",
        "Since there are 38629 features used in the model, it's hard to plot a hyperlane with these high dimensions. One way is using pca to reduce the number of features to 2 dimensions. However, for convinience, we can use decision_function, a function of SGD model in sklearn, to examine the distance of that samples to the hyperplane. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzfB1cFVIFBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dd120d-f0a3-4049-ebb8-65c8ede1f884"
      },
      "source": [
        "import seaborn as sns\n",
        "hyperplane = best_SGD.decision_function(X_train).T\n",
        "hyperplane"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.17831983, -1.09923255, -2.62206615, ..., -2.74634616,\n",
              "        -1.27677542, -1.30582789],\n",
              "       [ 0.935393  ,  0.95069649, -1.5506326 , ..., -1.12899506,\n",
              "        -1.13374441, -2.45512164],\n",
              "       [-1.00311192, -1.01683094,  2.39748871, ...,  1.17392414,\n",
              "         1.14991586,  1.58120737]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dddef30XpBNi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b0d1e19-b712-47d1-c403-1866f0371e44"
      },
      "source": [
        "for i in range(3):\n",
        "  sns.displot(hyperplane[i], kind = 'hist', legend=True)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLBP51tTRyuH"
      },
      "source": [
        "The histograms show the distribution of distances from data points to the hypelane. The negative and positive distances means which sides of the hypelane the data points belong to. It seems very few data points close to the hypelane. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCObzt9s8LNl"
      },
      "source": [
        "**Model with log loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_uRSKZiqoBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c5ac49-3626-499f-ec2a-2edcf095097c"
      },
      "source": [
        "log_SGD = SGDClassifier(loss='log', penalty='l2',  shuffle=True, alpha=0.0091).fit(X_train, y_train)\n",
        "y_pred = log_SGD.predict(X_test)\n",
        "printing_eval_scores (y_test, y_pred, report=True)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score: 0.667\n",
            "precision score: 0.677\n",
            "recall score: 0.667\n",
            "F1 score: 0.667\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.56      0.63       240\n",
            "           1       0.58      0.69      0.63       383\n",
            "           2       0.74      0.72      0.73       379\n",
            "\n",
            "    accuracy                           0.67      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.68      0.67      0.67      1002\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666,\n",
              " 0.6770963176166473,\n",
              " 0.6666666666666666,\n",
              " 0.6673483560745256)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zBP97kuVvsj"
      },
      "source": [
        "Compared the best model (F1 = 0.648) we built using loss = 'hinge' (SVM loss), the model with loss = 'log' (Logistic regression) performs better, with a F1 score = 0.667."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFMb2r5s8h_d"
      },
      "source": [
        "**8. Building a linear model to compare the result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NSbrMBo1j_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ecdb01-c6fc-43b4-c07f-e125478d7aca"
      },
      "source": [
        "new_df['rating'].describe()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5006.000000\n",
              "mean        0.581422\n",
              "std         0.181725\n",
              "min         0.000000\n",
              "25%         0.490000\n",
              "50%         0.600000\n",
              "75%         0.700000\n",
              "max         1.000000\n",
              "Name: rating, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkrIt25JtiVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "51608057-d247-43bb-b3d2-c24ed9d50e32"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.histplot(data = new_df, x = 'rating', bins = 20)\n",
        "plt.show()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ukaneBAJAo",
        "outputId": "c13a4ed4-9988-4a19-c369-423af5213ab2"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train_, X_test_, y_train_, y_test_ = train_test_split (new_df['review'], new_df['rating'] , train_size = 0.8, random_state = 42, shuffle = True)\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (4004, 38629) (4004,)\n",
            "Shapes of X_test, y_test:  (1002, 38629) (1002,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXqfjmvrAJAp",
        "outputId": "1ff41e0c-4b65-4794-cff8-4cd2d44c9dfe"
      },
      "source": [
        "# Vectorizing the documents\n",
        "vectorizer = CountVectorizer(binary = True)\n",
        "X_train_ = vectorizer.fit_transform(X_train_.to_list())\n",
        "X_test_ = vectorizer.transform(X_test_.to_list())\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (4004, 38629) (4004,)\n",
            "Shapes of X_test, y_test:  (1002, 38629) (1002,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E98URH1HCAJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c89af0-d727-487a-8236-0906b5c54f1c"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "reg = LinearRegression().fit(X_train_, y_train_)\n",
        "y_pred = reg.predict(X_test_)\n",
        "print('mean squared error: {:.3f}'.format(mean_squared_error(y_test_, y_pred)))\n",
        "print('r2 score: {:.3f}'.format(r2_score(y_test_, y_pred)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean squared error: 0.019\n",
            "r2 score: 0.413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsenvLLcWufC"
      },
      "source": [
        "Above is the performance of the linear regression model. The Mean Squared Error is close to 0 (0.019), however, the R2 is not good enough, 0.413. However, given different evaluation matrix, it is very hard to compare the performance of the regression model with the classification model we built before. Therefore, we can use the annotation scheme provided in README file to convert continous values in y_predicted into three categories as we had before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNIp41-2LsDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5917a5fb-d7a5-4d8f-bbd1-2ef478e4c5bb"
      },
      "source": [
        "# Encoding the y_predicted (continous values) into 3 classes\n",
        "y_pred_cl = []\n",
        "for i in y_pred:\n",
        "  if i <=0.4:\n",
        "    y_pred_cl.append(\"0\")\n",
        "  elif i >=0.7:\n",
        "    y_pred_cl.append(\"2\")\n",
        "  else:\n",
        "    y_pred_cl.append(\"1\")\n",
        "printing_eval_scores(y_test, y_pred_cl, report = True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score: 0.347\n",
            "precision score: 0.330\n",
            "recall score: 0.347\n",
            "F1 score: 0.315\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.15      0.18       240\n",
            "           1       0.38      0.63      0.47       383\n",
            "           2       0.35      0.18      0.24       379\n",
            "\n",
            "    accuracy                           0.35      1002\n",
            "   macro avg       0.32      0.32      0.30      1002\n",
            "weighted avg       0.33      0.35      0.32      1002\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3473053892215569,\n",
              " 0.33029549036644185,\n",
              " 0.3473053892215569,\n",
              " 0.31539444137412953)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUi5trNzY0k_"
      },
      "source": [
        "Looking at the performance of the model on F1 score, we can see the regression model performed much worse than the classification models we built before. This indicates an inconsistence between rating, and the classes assigned accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3hGX2fcxUf"
      },
      "source": [
        "# Implementing minibatches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvvbxyo8iRK9",
        "outputId": "44d99ba9-a852-476f-8a42-0a0dfb4c56a2"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "import random\n",
        "log_SGD_2 = SGDClassifier(loss='log', penalty='l2',  shuffle=True, alpha=0.0081).fit(X_train, y_train)\n",
        "n_iter = 10\n",
        "batch_size = 32\n",
        "F1_tr = []\n",
        "F1_test = []\n",
        "for n in range(n_iter):\n",
        "  print ('----------iteration {}-----------'.format(n+1))\n",
        "  n_instances, n_features = X_train.shape\n",
        "  i=0 \n",
        "  while i<= round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    log_SGD_2.partial_fit(X_train[m:n], y_train[m:n], classes=np.unique(y_train))\n",
        "    i+=1    \n",
        "  y_pred = log_SGD_2.predict(X_test)\n",
        "  print('Model performance on training set:')\n",
        "  _, _,_, f1_tr = printing_eval_scores (y_train, log_SGD_2.predict(X_train), report = False)\n",
        "  print('\\nModel performance on test set:')\n",
        "  _, _,_, f1_test = printing_eval_scores (y_test, y_pred, report = True)\n",
        "  F1_tr.append (f1_tr)\n",
        "  F1_test.append (f1_test)\n",
        "\n",
        "max_f1 = max(F1_test)\n",
        "idx_max_f1 = F1_test.index(max(F1_test))\n",
        "print('Best performance is in iteration {}: F1 = {:.3f}'.format(idx_max_f1 +1, max_f1))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------iteration 1-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.968\n",
            "precision score: 0.969\n",
            "recall score: 0.968\n",
            "F1 score: 0.968\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.666\n",
            "precision score: 0.678\n",
            "recall score: 0.666\n",
            "F1 score: 0.666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.53      0.63       240\n",
            "           1       0.58      0.68      0.63       383\n",
            "           2       0.73      0.73      0.73       379\n",
            "\n",
            "    accuracy                           0.67      1002\n",
            "   macro avg       0.69      0.65      0.66      1002\n",
            "weighted avg       0.68      0.67      0.67      1002\n",
            "\n",
            "----------iteration 2-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.968\n",
            "precision score: 0.970\n",
            "recall score: 0.968\n",
            "F1 score: 0.968\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.663\n",
            "precision score: 0.675\n",
            "recall score: 0.663\n",
            "F1 score: 0.663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.54      0.63       240\n",
            "           1       0.57      0.67      0.62       383\n",
            "           2       0.72      0.73      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.69      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 3-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.969\n",
            "precision score: 0.970\n",
            "recall score: 0.969\n",
            "F1 score: 0.969\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.663\n",
            "precision score: 0.674\n",
            "recall score: 0.663\n",
            "F1 score: 0.662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.53      0.63       240\n",
            "           1       0.58      0.67      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 4-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.968\n",
            "precision score: 0.969\n",
            "recall score: 0.968\n",
            "F1 score: 0.968\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.664\n",
            "precision score: 0.675\n",
            "recall score: 0.664\n",
            "F1 score: 0.664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.54      0.63       240\n",
            "           1       0.58      0.67      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.69      0.65      0.66      1002\n",
            "weighted avg       0.68      0.66      0.66      1002\n",
            "\n",
            "----------iteration 5-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.968\n",
            "precision score: 0.969\n",
            "recall score: 0.968\n",
            "F1 score: 0.968\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.664\n",
            "precision score: 0.676\n",
            "recall score: 0.664\n",
            "F1 score: 0.664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.54      0.63       240\n",
            "           1       0.58      0.67      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.69      0.65      0.66      1002\n",
            "weighted avg       0.68      0.66      0.66      1002\n",
            "\n",
            "----------iteration 6-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.969\n",
            "precision score: 0.970\n",
            "recall score: 0.969\n",
            "F1 score: 0.969\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.663\n",
            "precision score: 0.673\n",
            "recall score: 0.663\n",
            "F1 score: 0.662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.54      0.63       240\n",
            "           1       0.58      0.66      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 7-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.969\n",
            "precision score: 0.970\n",
            "recall score: 0.969\n",
            "F1 score: 0.969\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.663\n",
            "precision score: 0.674\n",
            "recall score: 0.663\n",
            "F1 score: 0.662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.54      0.63       240\n",
            "           1       0.58      0.67      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 8-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.969\n",
            "precision score: 0.970\n",
            "recall score: 0.969\n",
            "F1 score: 0.969\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.664\n",
            "precision score: 0.673\n",
            "recall score: 0.664\n",
            "F1 score: 0.663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.54      0.63       240\n",
            "           1       0.58      0.66      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 9-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.969\n",
            "precision score: 0.970\n",
            "recall score: 0.969\n",
            "F1 score: 0.969\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.664\n",
            "precision score: 0.671\n",
            "recall score: 0.664\n",
            "F1 score: 0.663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.55      0.63       240\n",
            "           1       0.58      0.65      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "----------iteration 10-----------\n",
            "Model performance on training set:\n",
            "accuracy score: 0.968\n",
            "precision score: 0.969\n",
            "recall score: 0.968\n",
            "F1 score: 0.968\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.662\n",
            "precision score: 0.671\n",
            "recall score: 0.662\n",
            "F1 score: 0.662\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.55      0.63       240\n",
            "           1       0.58      0.66      0.62       383\n",
            "           2       0.72      0.74      0.73       379\n",
            "\n",
            "    accuracy                           0.66      1002\n",
            "   macro avg       0.68      0.65      0.66      1002\n",
            "weighted avg       0.67      0.66      0.66      1002\n",
            "\n",
            "Best performance is in iteration 1: F1 = 0.666\n"
          ]
        }
      ]
    }
  ]
}