{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7fn915rE/0qt5tsYMb2sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment2_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRoN8MBWFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3a0a3f-386f-496c-fcf1-3799268e928e"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "#from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqCpCGDJrZz"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krvu3FbyUwOn"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/review_polarity.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCAKXzrfWYEc",
        "outputId": "6f708bf1-d327-4f1b-8ea6-3e15f979573c"
      },
      "source": [
        "# Exploring the data sizes\n",
        "\n",
        "paths_pos = glob.glob('/content/txt_sentoken/pos/*.txt')\n",
        "paths_neg = glob.glob('/content/txt_sentoken/neg/*.txt')\n",
        "pos_neg_paths = paths_pos + paths_neg\n",
        "\n",
        "n_pos = len(paths_pos)\n",
        "n_neg = len(paths_neg)\n",
        "\n",
        "print('the number of positive instances: {} \\nthe number of positive instances: {}'.format(n_pos, n_neg))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of positive instances: 1000 \n",
            "the number of positive instances: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_HBaup_JiGU"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-1M-pGYxd2",
        "outputId": "7eb3c115-f500-4590-b36f-3a8a7b09184b"
      },
      "source": [
        "# Exploring the words in the dataset\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenizer (doc):\n",
        "  #doc = doc.lower() # Lowercase documents\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "# def BOW (doc):\n",
        "#   bow = set()\n",
        "#   for token in tokenizer (doc):\n",
        "#     bow.add(token)\n",
        "#   return list(bow)\n",
        "\n",
        "#def word_counter (doc):\n",
        "\n",
        "def stopword_remover (bow):\n",
        "  filtered_bow = [w for w in bow if not w.lower() in stopwords]\n",
        "  return filtered_bow\n",
        "\n",
        "def top_freq_w (freq_dic, top_n, stopword_removing = ''):\n",
        "  sorted_dic = {k:v for k, v in sorted(freq_dic.items(), key = lambda item: item[1], reverse=True)}\n",
        "  if stopword_removing is False:\n",
        "    return {k:v for k, v in list(sorted_dic.items())[:top_n]}\n",
        "  elif stopword_removing is True:\n",
        "    filtered_dic = {k: v for k, v in sorted_dic.items() if k not in stopwords}\n",
        "    return {k:v for k, v in list(filtered_dic.items())[:top_n]}\n",
        "  \n",
        "\n",
        "\n",
        "word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  for token in tokenizer (doc):\n",
        "    word_freq[token] = word_freq.get(token,0)+1\n",
        "\n",
        "top_100_w = top_freq_w(word_freq, 100, stopword_removing = False) \n",
        "\n",
        "print('the number of unique words in the dataset: ', len(word_freq.keys()))\n",
        "print ('top 100 most frequent words:\\n', top_100_w )\n",
        "print('\\nthe number of words in the top 100 which are stopwords: ', len([w for w in top_100_w.keys() if w in stopwords]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "the number of unique words in the dataset:  39697\n",
            "top 100 most frequent words:\n",
            " {'the': 76529, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, 'is': 25195, 'in': 21822, 's': 18513, 'it': 16107, 'that': 15924, 'as': 11378, 'with': 10792, 'for': 9961, 'his': 9587, 'this': 9578, 'film': 9517, 'i': 8889, 'he': 8864, 'but': 8634, 'on': 7385, 'are': 6949, 't': 6410, 'by': 6261, 'be': 6174, 'one': 5852, 'movie': 5771, 'an': 5744, 'who': 5692, 'not': 5577, 'you': 5316, 'from': 4999, 'at': 4986, 'was': 4940, 'have': 4901, 'they': 4825, 'has': 4719, 'her': 4522, 'all': 4373, 'there': 3770, 'like': 3690, 'so': 3683, 'out': 3637, 'about': 3523, 'up': 3405, 'more': 3347, 'what': 3322, 'when': 3258, 'which': 3161, 'or': 3148, 'she': 3141, 'their': 3122, 'some': 2985, 'just': 2905, 'can': 2882, 'if': 2799, 'we': 2775, 'him': 2633, 'into': 2623, 'even': 2565, 'only': 2495, 'than': 2474, 'no': 2472, 'good': 2411, 'time': 2411, 'most': 2306, 'its': 2270, 'will': 2216, 'story': 2169, '': 2152, 'would': 2109, 'been': 2050, 'much': 2049, 'character': 2020, 'also': 1967, 'get': 1949, 'other': 1948, 'do': 1915, 'two': 1911, 'well': 1906, 'them': 1877, 'very': 1863, 'characters': 1859, 'first': 1836, 'after': 1762, 'see': 1749, 'way': 1693, 'because': 1684, 'make': 1642, 'life': 1586, 'off': 1581, 'too': 1577, 'any': 1574, 'does': 1568, 'really': 1558, 'had': 1546, 'while': 1539, 'films': 1536, 'how': 1517, 'plot': 1513, 'little': 1501}\n",
            "\n",
            "the number of words in the top 100 which are stopwords:  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "XZlPlHizshiT",
        "outputId": "2388b105-d733-4b88-8f4e-c68f0e08117b"
      },
      "source": [
        "# Reformating the dataset into csv for convenience \n",
        "def to_df (folder):\n",
        "  data_dic = {}\n",
        "  data_dic['doc'], data_dic['label'] = [], []\n",
        "  for file in folder:\n",
        "    fo = open(file)\n",
        "    doc = fo.read()\n",
        "    data_dic['doc'].append(doc)\n",
        "    if 'pos' in file:\n",
        "      data_dic['label'].append(1)\n",
        "    elif 'neg' in file:\n",
        "      data_dic['label'].append(0)\n",
        "    else:\n",
        "      print('error', file)\n",
        "  df = pd.DataFrame.from_dict(data_dic)\n",
        "  return df\n",
        "    \n",
        "data = to_df(pos_neg_paths)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredients : london gal , fate , true love , ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show , an almost perfectly accurate true ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after a stylistic detour with mrs . \\nparker a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>all great things come to an end , and the dot-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udall is a heartless man . \\nhe spends ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  label\n",
              "0  ingredients : london gal , fate , true love , ...      1\n",
              "1  quiz show , an almost perfectly accurate true ...      1\n",
              "2  after a stylistic detour with mrs . \\nparker a...      1\n",
              "3  all great things come to an end , and the dot-...      1\n",
              "4  melvin udall is a heartless man . \\nhe spends ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFEUMkgJYGz"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jGSEg3J8ktsC",
        "outputId": "d838aa46-c97a-4a24-fa86-2392ffeab079"
      },
      "source": [
        "# Data preprocessing\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "def preprocessor (text):\n",
        "  ## removing punctuations and characters\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # stripping\n",
        "  text = ' '.join([w.strip() for w in text.split()])\n",
        "  # print(text)\n",
        "  ## lowcasing\n",
        "  text = text.lower()\n",
        "  # ## removing stopword\n",
        "  text = stopword_remover (text.split())\n",
        "  # ##stemmming\n",
        "  text = [stemmer.stem(w) for w in text]\n",
        "  # ## lematization\n",
        "  text = [lemmatizer.lemmatize(w) for w in text]\n",
        "  return ' '.join([w for w in text])\n",
        "\n",
        "data['doc'] = data['doc'].apply(lambda x:  preprocessor (x) )\n",
        "data "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredi london gal fate true love run joke mon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show almost perfectli accur true stori ba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stylist detour mr parker viciou circl despit u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>great thing come end dotcom era embodi perfect...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udal heartless man spend day insid spac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>often similar littl boy lost park right ventur...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>13th warrior reek badli melodrama poor act car...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>accord hitchcock variou filmmak isol motel din...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>warn follow review contain spoiler cast gari s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>went saw film right call battlefield earth nev...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    doc  label\n",
              "0     ingredi london gal fate true love run joke mon...      1\n",
              "1     quiz show almost perfectli accur true stori ba...      1\n",
              "2     stylist detour mr parker viciou circl despit u...      1\n",
              "3     great thing come end dotcom era embodi perfect...      1\n",
              "4     melvin udal heartless man spend day insid spac...      1\n",
              "...                                                 ...    ...\n",
              "1995  often similar littl boy lost park right ventur...      0\n",
              "1996  13th warrior reek badli melodrama poor act car...      0\n",
              "1997  accord hitchcock variou filmmak isol motel din...      0\n",
              "1998  warn follow review contain spoiler cast gari s...      0\n",
              "1999  went saw film right call battlefield earth nev...      0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FEl86wJ2Zv"
      },
      "source": [
        "# Developing a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p5-fLZLetp"
      },
      "source": [
        "# Preparing vocabulary\n",
        "## As required, we will use 1000 most frequent word, excluding stopwords\n",
        "### Preprocessing data for building vocabulary\n",
        "cleaned_word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  cleaned_doc = preprocessor(doc)\n",
        "  for token in tokenizer (cleaned_doc):\n",
        "    cleaned_word_freq[token] = cleaned_word_freq.get(token,0)+1\n",
        "\n",
        "vocabulary = top_freq_w(cleaned_word_freq, 1000, stopword_removing = True) \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyAitNIFMKM1"
      },
      "source": [
        "## remove this code\n",
        "# Feature engineering\n",
        "# def feature_extractor (doc):\n",
        "#   doc_vec = []\n",
        "#   for feature in vocabulary.keys():\n",
        "#     feature_count = 0\n",
        "#     if feature in tokenizer (doc):\n",
        "#       feature_count+=1\n",
        "#     else:\n",
        "#       feature_count+=0\n",
        "#     doc_vec.append(feature_count) \n",
        "#   return doc_vec"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPM073ASYl5i"
      },
      "source": [
        "\n",
        "\n",
        "# def feature_extractor (doc):\n",
        "#   doc_vec = []\n",
        "#   token_list = tokenizer (doc)\n",
        "#   for feature in vocabulary.keys():\n",
        "#     # feature_count=0\n",
        "#     feature_count = token_list.count(feature)\n",
        "#     doc_vec.append(feature_count) \n",
        "#   return doc_vec\n",
        "\n",
        "# X = data['doc'].apply(lambda x: feature_extractor(x))\n",
        "# y = data['label']\n",
        "\n",
        "\n",
        "def feature_extractor (doc):\n",
        "  doc_vec = []\n",
        "  token_list = tokenizer (doc)\n",
        "  for feature in vocabulary.keys():\n",
        "    if feature in token_list:\n",
        "      doc_vec.append(1)\n",
        "    else:\n",
        "       doc_vec.append(0)\n",
        "  return doc_vec\n",
        "\n",
        "X = data['doc'].apply(lambda x: feature_extractor(x))\n",
        "y = data['label']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qW3hQL2GFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "98dfe539-8542-4293-d7cf-e77d55f2c48f"
      },
      "source": [
        "\n",
        "X = X.apply(pd.Series)\n",
        "X.columns = vocabulary.keys()\n",
        "X"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film</th>\n",
              "      <th>movi</th>\n",
              "      <th>one</th>\n",
              "      <th>like</th>\n",
              "      <th>charact</th>\n",
              "      <th>get</th>\n",
              "      <th>make</th>\n",
              "      <th>time</th>\n",
              "      <th>scene</th>\n",
              "      <th>even</th>\n",
              "      <th>good</th>\n",
              "      <th>play</th>\n",
              "      <th>stori</th>\n",
              "      <th>see</th>\n",
              "      <th>would</th>\n",
              "      <th>much</th>\n",
              "      <th>also</th>\n",
              "      <th>go</th>\n",
              "      <th>way</th>\n",
              "      <th>seem</th>\n",
              "      <th>look</th>\n",
              "      <th>end</th>\n",
              "      <th>two</th>\n",
              "      <th>take</th>\n",
              "      <th>first</th>\n",
              "      <th>come</th>\n",
              "      <th>well</th>\n",
              "      <th>work</th>\n",
              "      <th>thing</th>\n",
              "      <th>year</th>\n",
              "      <th>realli</th>\n",
              "      <th>plot</th>\n",
              "      <th>know</th>\n",
              "      <th>perform</th>\n",
              "      <th>littl</th>\n",
              "      <th>life</th>\n",
              "      <th>peopl</th>\n",
              "      <th>love</th>\n",
              "      <th>could</th>\n",
              "      <th>bad</th>\n",
              "      <th>...</th>\n",
              "      <th>rush</th>\n",
              "      <th>realist</th>\n",
              "      <th>scare</th>\n",
              "      <th>manner</th>\n",
              "      <th>command</th>\n",
              "      <th>standard</th>\n",
              "      <th>menac</th>\n",
              "      <th>spent</th>\n",
              "      <th>adam</th>\n",
              "      <th>agre</th>\n",
              "      <th>cinematographi</th>\n",
              "      <th>front</th>\n",
              "      <th>ground</th>\n",
              "      <th>budget</th>\n",
              "      <th>fairli</th>\n",
              "      <th>pair</th>\n",
              "      <th>virtual</th>\n",
              "      <th>suddenli</th>\n",
              "      <th>fantasi</th>\n",
              "      <th>connect</th>\n",
              "      <th>disturb</th>\n",
              "      <th>90</th>\n",
              "      <th>appropri</th>\n",
              "      <th>godzilla</th>\n",
              "      <th>brown</th>\n",
              "      <th>grant</th>\n",
              "      <th>cultur</th>\n",
              "      <th>greatest</th>\n",
              "      <th>store</th>\n",
              "      <th>trip</th>\n",
              "      <th>key</th>\n",
              "      <th>fascin</th>\n",
              "      <th>cute</th>\n",
              "      <th>brief</th>\n",
              "      <th>cameo</th>\n",
              "      <th>count</th>\n",
              "      <th>foot</th>\n",
              "      <th>addit</th>\n",
              "      <th>satir</th>\n",
              "      <th>bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      film  movi  one  like  charact  ...  count  foot  addit  satir  bug\n",
              "0        1     0    1     1        1  ...      0     0      0      0    0\n",
              "1        1     1    1     0        0  ...      0     0      0      0    0\n",
              "2        1     1    1     1        1  ...      0     0      0      0    0\n",
              "3        1     0    0     0        0  ...      0     0      0      0    0\n",
              "4        1     0    1     0        0  ...      0     0      0      0    0\n",
              "...    ...   ...  ...   ...      ...  ...    ...   ...    ...    ...  ...\n",
              "1995     1     1    1     0        1  ...      0     0      1      0    0\n",
              "1996     1     1    1     1        1  ...      0     0      0      0    0\n",
              "1997     1     0    1     0        0  ...      0     0      0      0    0\n",
              "1998     1     1    1     1        1  ...      0     1      0      0    0\n",
              "1999     1     1    1     1        0  ...      0     0      0      0    0\n",
              "\n",
              "[2000 rows x 1000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7yjtdsJ_ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a51c75-dd13-448a-db43-4631352e4843"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split (X[vocabulary.keys()], y , train_size = 0.8, random_state = 42, shuffle = True, stratify=data['label'])\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (1600, 1000) (1600,)\n",
            "Shapes of X_test, y_test:  (400, 1000) (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3koUq4K2EJLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d7c341-69d3-49b2-a257-9681d6409161"
      },
      "source": [
        "X_train.values"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSfRLnHvKT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c945429f-a76a-4a41-dee9-f6e206897fbb"
      },
      "source": [
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "\n",
        "\n",
        "\n",
        "def predict(X, weight, bias):\n",
        "    z = np.dot(weight, X.values.T) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    y_class = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "    return y_class\n",
        "\n",
        "def sigmoid (z):\n",
        "  p=1/(1+np.exp(-z))\n",
        "  return p\n",
        "\n",
        "# Printing model performance \n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  print('accuracy score: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "  print('precision score: {}'.format(sklearn.metrics.precision_score(y_true, y_pred)))\n",
        "  print('recall score: {}'.format(sklearn.metrics.recall_score(y_true, y_pred)))\n",
        "  print('F1 score: {}'.format(sklearn.metrics.f1_score(y_true, y_pred)))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "def computing_gradient (X, Y, weight, bias):\n",
        "  for x,y in zip (X.values, Y):\n",
        "    z = np.dot(weight, x) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "    d_weight = np.dot((y_pred - y), x)\n",
        "    d_bias = (y_pred - y)\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "  return weight, bias\n",
        "\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "# weight = None\n",
        "# bias = None\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  # for x,y in zip (X_train.values,y_train):\n",
        "  #   z = np.dot(weight, x) + bias\n",
        "  #   y_pred = sigmoid(z)\n",
        "  #   #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "  #   d_weight = np.dot((y_pred - y), x)\n",
        "  #   d_bias = (y_pred - y)\n",
        "  #   weight -= lr*d_weight\n",
        "  #   bias -= lr*d_bias\n",
        "  weight, bias = computing_gradient (X_train, y_train, weight, bias)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.8775\n",
            "precision score: 0.9428152492668622\n",
            "recall score: 0.80375\n",
            "F1 score: 0.8677462887989204\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.8554216867469879\n",
            "recall score: 0.71\n",
            "F1 score: 0.7759562841530054\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.88      0.81       200\n",
            "           1       0.86      0.71      0.78       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.92\n",
            "precision score: 0.9653739612188366\n",
            "recall score: 0.87125\n",
            "F1 score: 0.9159001314060446\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8\n",
            "precision score: 0.8448275862068966\n",
            "recall score: 0.735\n",
            "F1 score: 0.786096256684492\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81       200\n",
            "           1       0.84      0.73      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.925625\n",
            "precision score: 0.970954356846473\n",
            "recall score: 0.8775\n",
            "F1 score: 0.9218647406434667\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.805\n",
            "precision score: 0.8388888888888889\n",
            "recall score: 0.755\n",
            "F1 score: 0.7947368421052632\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.85      0.81       200\n",
            "           1       0.84      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.81      0.80       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.943125\n",
            "precision score: 0.9849521203830369\n",
            "recall score: 0.9\n",
            "F1 score: 0.9405617243631612\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8025\n",
            "precision score: 0.8342541436464088\n",
            "recall score: 0.755\n",
            "F1 score: 0.7926509186351706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.85      0.81       200\n",
            "           1       0.83      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.960625\n",
            "precision score: 0.9933065595716198\n",
            "recall score: 0.9275\n",
            "F1 score: 0.9592760180995475\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.7975\n",
            "precision score: 0.825136612021858\n",
            "recall score: 0.755\n",
            "F1 score: 0.7885117493472584\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.84      0.81       200\n",
            "           1       0.83      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.98375\n",
            "precision score: 0.9948849104859335\n",
            "recall score: 0.9725\n",
            "F1 score: 0.9835651074589128\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8075\n",
            "precision score: 0.8153846153846154\n",
            "recall score: 0.795\n",
            "F1 score: 0.8050632911392405\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       200\n",
            "           1       0.82      0.80      0.81       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.981875\n",
            "precision score: 1.0\n",
            "recall score: 0.96375\n",
            "F1 score: 0.9815404201145767\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.7975\n",
            "precision score: 0.8115183246073299\n",
            "recall score: 0.775\n",
            "F1 score: 0.792838874680307\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       200\n",
            "           1       0.81      0.78      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.9975\n",
            "precision score: 1.0\n",
            "recall score: 0.995\n",
            "F1 score: 0.9974937343358395\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8025\n",
            "precision score: 0.8071065989847716\n",
            "recall score: 0.795\n",
            "F1 score: 0.801007556675063\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80       200\n",
            "           1       0.81      0.80      0.80       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 1.0\n",
            "precision score: 1.0\n",
            "recall score: 1.0\n",
            "F1 score: 1.0\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.815\n",
            "precision score: 0.8088235294117647\n",
            "recall score: 0.825\n",
            "F1 score: 0.8168316831683168\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       200\n",
            "           1       0.81      0.82      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.99875\n",
            "precision score: 0.9975062344139651\n",
            "recall score: 1.0\n",
            "F1 score: 0.9987515605493134\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8025\n",
            "precision score: 0.784037558685446\n",
            "recall score: 0.835\n",
            "F1 score: 0.8087167070217918\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.80       200\n",
            "           1       0.78      0.83      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wviTYBaXOCxF"
      },
      "source": [
        "#Minibatch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0rgm4ULuTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a046a8f-9211-4011-888b-89acb5c0ce85"
      },
      "source": [
        "\n",
        "\n",
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "\n",
        "def computing_MiniBatch_gradient (X, Y, weight, bias, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i< round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "      sum_w += np.dot((y_pred - y), x)\n",
        "      sum_b += (y_pred - y)\n",
        "    d_weight = (1/batch_size)*sum_w\n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  weight, bias = computing_MiniBatch_gradient (X_train, y_train, weight, bias,  batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "  \n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.838125\n",
            "precision score: 0.7826541274817137\n",
            "recall score: 0.93625\n",
            "F1 score: 0.8525896414342629\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.788135593220339\n",
            "recall score: 0.93\n",
            "F1 score: 0.8532110091743119\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.75      0.82       200\n",
            "           1       0.79      0.93      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.873125\n",
            "precision score: 0.8312985571587126\n",
            "recall score: 0.93625\n",
            "F1 score: 0.8806584362139918\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.845\n",
            "precision score: 0.8108108108108109\n",
            "recall score: 0.9\n",
            "F1 score: 0.8530805687203792\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.79      0.84       200\n",
            "           1       0.81      0.90      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.8875\n",
            "precision score: 0.8514739229024944\n",
            "recall score: 0.93875\n",
            "F1 score: 0.8929845422116528\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8375\n",
            "precision score: 0.8082191780821918\n",
            "recall score: 0.885\n",
            "F1 score: 0.8448687350835322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.83       200\n",
            "           1       0.81      0.89      0.84       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.895\n",
            "precision score: 0.8640552995391705\n",
            "recall score: 0.9375\n",
            "F1 score: 0.8992805755395684\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8325\n",
            "precision score: 0.8036529680365296\n",
            "recall score: 0.88\n",
            "F1 score: 0.8400954653937948\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.82       200\n",
            "           1       0.80      0.88      0.84       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.905\n",
            "precision score: 0.875\n",
            "recall score: 0.945\n",
            "F1 score: 0.9086538461538461\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8425\n",
            "precision score: 0.815668202764977\n",
            "recall score: 0.885\n",
            "F1 score: 0.8489208633093525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.914375\n",
            "precision score: 0.8868144690781797\n",
            "recall score: 0.95\n",
            "F1 score: 0.9173204586602293\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8475\n",
            "precision score: 0.8232558139534883\n",
            "recall score: 0.885\n",
            "F1 score: 0.853012048192771\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.920625\n",
            "precision score: 0.8954171562867215\n",
            "recall score: 0.9525\n",
            "F1 score: 0.9230769230769229\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8475\n",
            "precision score: 0.8262910798122066\n",
            "recall score: 0.88\n",
            "F1 score: 0.8523002421307507\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.83      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.92625\n",
            "precision score: 0.902122641509434\n",
            "recall score: 0.95625\n",
            "F1 score: 0.9283980582524273\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.85\n",
            "precision score: 0.8333333333333334\n",
            "recall score: 0.875\n",
            "F1 score: 0.8536585365853658\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85       200\n",
            "           1       0.83      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.9325\n",
            "precision score: 0.9089834515366431\n",
            "recall score: 0.96125\n",
            "F1 score: 0.9343863912515189\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.845\n",
            "precision score: 0.8317307692307693\n",
            "recall score: 0.865\n",
            "F1 score: 0.8480392156862744\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       200\n",
            "           1       0.83      0.86      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.9375\n",
            "precision score: 0.9156769596199525\n",
            "recall score: 0.96375\n",
            "F1 score: 0.9390986601705239\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8475\n",
            "precision score: 0.8357487922705314\n",
            "recall score: 0.865\n",
            "F1 score: 0.8501228501228502\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       200\n",
            "           1       0.84      0.86      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSYOmfA1x70"
      },
      "source": [
        "# L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ESdBPV11xP",
        "outputId": "75c9f0ff-f68f-4014-d4f4-3fdba34d4d32"
      },
      "source": [
        "\n",
        "\n",
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "\n",
        "def MiniBatch_gradient_L2 (X, Y, weight, bias,alpha, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i< round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      w_i = np.dot((y_pred - y), x)\n",
        "      b_i = y_pred - y\n",
        "      sum_w +=  w_i  + (alpha * weight) # L2\n",
        "      sum_b += b_i\n",
        "    d_weight = (1/batch_size)*sum_w \n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "alpha = 0.01\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  weight, bias = MiniBatch_gradient_L2 (X_train, y_train, alpha = alpha,  weight=weight, bias=bias, batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.836875\n",
            "precision score: 0.781021897810219\n",
            "recall score: 0.93625\n",
            "F1 score: 0.8516202387720296\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.788135593220339\n",
            "recall score: 0.93\n",
            "F1 score: 0.8532110091743119\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.75      0.82       200\n",
            "           1       0.79      0.93      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.871875\n",
            "precision score: 0.8294573643410853\n",
            "recall score: 0.93625\n",
            "F1 score: 0.8796241926012919\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8475\n",
            "precision score: 0.8116591928251121\n",
            "recall score: 0.905\n",
            "F1 score: 0.8557919621749409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.79      0.84       200\n",
            "           1       0.81      0.91      0.86       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.886875\n",
            "precision score: 0.8497175141242937\n",
            "recall score: 0.94\n",
            "F1 score: 0.8925816023738872\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.8090909090909091\n",
            "recall score: 0.89\n",
            "F1 score: 0.8476190476190476\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.79      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.89375\n",
            "precision score: 0.8612385321100917\n",
            "recall score: 0.93875\n",
            "F1 score: 0.8983253588516746\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8375\n",
            "precision score: 0.8082191780821918\n",
            "recall score: 0.885\n",
            "F1 score: 0.8448687350835322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.83       200\n",
            "           1       0.81      0.89      0.84       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.901875\n",
            "precision score: 0.8716763005780347\n",
            "recall score: 0.9425\n",
            "F1 score: 0.9057057057057057\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.8119266055045872\n",
            "recall score: 0.885\n",
            "F1 score: 0.8468899521531099\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.91125\n",
            "precision score: 0.8799076212471132\n",
            "recall score: 0.9525\n",
            "F1 score: 0.9147659063625451\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.8119266055045872\n",
            "recall score: 0.885\n",
            "F1 score: 0.8468899521531099\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.91625\n",
            "precision score: 0.8872093023255814\n",
            "recall score: 0.95375\n",
            "F1 score: 0.919277108433735\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8425\n",
            "precision score: 0.815668202764977\n",
            "recall score: 0.885\n",
            "F1 score: 0.8489208633093525\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.920625\n",
            "precision score: 0.8926487747957993\n",
            "recall score: 0.95625\n",
            "F1 score: 0.923355461677731\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.84\n",
            "precision score: 0.8177570093457944\n",
            "recall score: 0.875\n",
            "F1 score: 0.8454106280193237\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.83       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.925\n",
            "precision score: 0.8981264637002342\n",
            "recall score: 0.95875\n",
            "F1 score: 0.9274486094316808\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8425\n",
            "precision score: 0.8215962441314554\n",
            "recall score: 0.875\n",
            "F1 score: 0.847457627118644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.93\n",
            "precision score: 0.9056603773584906\n",
            "recall score: 0.96\n",
            "F1 score: 0.9320388349514563\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8425\n",
            "precision score: 0.8215962441314554\n",
            "recall score: 0.875\n",
            "F1 score: 0.847457627118644\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rklpxbuq8pcR",
        "outputId": "9c3d1dad-ab12-45a6-b228-487a83260156"
      },
      "source": [
        "feature_weights = {}\n",
        "for i in range(len(weight)):\n",
        "  feature_weights[list(vocabulary.keys())[i]] = weight[i]\n",
        "\n",
        "sorted_feature_weights = {k:v for k, v in sorted(feature_weights.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Print the weights learned for each class\n",
        "print('The most important features of POSITIVE class: ')\n",
        "for k, v in list(sorted_feature_weights.items())[:100]:\n",
        "  print ('{}: {:.5f}'. format(k,v))\n",
        "\n",
        "print('\\n==============================================')\n",
        "print('The most important features of NEGATIVE class: ')\n",
        "for k, v in list(sorted_feature_weights.items())[:-100:-1]: \n",
        "  print ('{}: {:.5f}'. format(k,v))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most important features of POSITIVE class: \n",
            "sometim: 0.43061\n",
            "great: 0.38293\n",
            "hilari: 0.37989\n",
            "memor: 0.36271\n",
            "perform: 0.33587\n",
            "excel: 0.33369\n",
            "flaw: 0.33169\n",
            "perfect: 0.32485\n",
            "also: 0.32103\n",
            "job: 0.31929\n",
            "overal: 0.31293\n",
            "definit: 0.30809\n",
            "fun: 0.29313\n",
            "equal: 0.29114\n",
            "thank: 0.28403\n",
            "especi: 0.27571\n",
            "enjoy: 0.27373\n",
            "seen: 0.27150\n",
            "portray: 0.26862\n",
            "american: 0.26837\n",
            "perfectli: 0.26366\n",
            "true: 0.26285\n",
            "day: 0.26114\n",
            "ive: 0.25985\n",
            "deserv: 0.25361\n",
            "side: 0.25273\n",
            "fiction: 0.24805\n",
            "life: 0.24665\n",
            "brilliant: 0.24135\n",
            "solid: 0.23994\n",
            "best: 0.23715\n",
            "share: 0.23545\n",
            "differ: 0.23352\n",
            "le: 0.23341\n",
            "normal: 0.23141\n",
            "well: 0.23117\n",
            "today: 0.22643\n",
            "david: 0.22265\n",
            "yet: 0.22232\n",
            "visual: 0.22196\n",
            "period: 0.21835\n",
            "realist: 0.21709\n",
            "support: 0.21256\n",
            "surpris: 0.21253\n",
            "entertain: 0.21232\n",
            "throughout: 0.21105\n",
            "togeth: 0.21099\n",
            "oscar: 0.20809\n",
            "delight: 0.20595\n",
            "nice: 0.20379\n",
            "world: 0.20127\n",
            "sever: 0.20004\n",
            "see: 0.19681\n",
            "summer: 0.19663\n",
            "follow: 0.19639\n",
            "wonder: 0.19598\n",
            "peopl: 0.19528\n",
            "take: 0.19373\n",
            "allow: 0.19124\n",
            "histori: 0.19124\n",
            "extrem: 0.19061\n",
            "truli: 0.19053\n",
            "back: 0.18714\n",
            "may: 0.18627\n",
            "product: 0.18503\n",
            "although: 0.18420\n",
            "amaz: 0.18412\n",
            "year: 0.18359\n",
            "carri: 0.18259\n",
            "similar: 0.18078\n",
            "surprisingli: 0.18030\n",
            "ben: 0.18028\n",
            "detail: 0.17957\n",
            "mother: 0.17956\n",
            "hit: 0.17763\n",
            "four: 0.17670\n",
            "mind: 0.17653\n",
            "due: 0.17474\n",
            "pace: 0.17305\n",
            "ride: 0.17191\n",
            "artist: 0.17091\n",
            "sure: 0.16982\n",
            "eventu: 0.16892\n",
            "view: 0.16690\n",
            "natur: 0.16681\n",
            "class: 0.16568\n",
            "help: 0.16541\n",
            "bill: 0.16534\n",
            "kind: 0.16485\n",
            "bring: 0.16385\n",
            "attent: 0.16187\n",
            "without: 0.16149\n",
            "rememb: 0.16116\n",
            "chang: 0.15901\n",
            "simpl: 0.15688\n",
            "mix: 0.15684\n",
            "plenti: 0.15613\n",
            "strang: 0.15532\n",
            "realiti: 0.15521\n",
            "touch: 0.15367\n",
            "\n",
            "==============================================\n",
            "The most important features of NEGATIVE class: \n",
            "worst: -0.61153\n",
            "bad: -0.60038\n",
            "bore: -0.58913\n",
            "wast: -0.57007\n",
            "unfortun: -0.47323\n",
            "suppos: -0.43452\n",
            "stupid: -0.41247\n",
            "noth: -0.38797\n",
            "problem: -0.38168\n",
            "plot: -0.36026\n",
            "ridicul: -0.35430\n",
            "aw: -0.35206\n",
            "mess: -0.35195\n",
            "fail: -0.34606\n",
            "terribl: -0.34310\n",
            "attempt: -0.33959\n",
            "mayb: -0.33612\n",
            "potenti: -0.32893\n",
            "script: -0.32477\n",
            "reason: -0.30578\n",
            "fall: -0.30348\n",
            "poor: -0.29757\n",
            "clich: -0.29006\n",
            "tri: -0.28790\n",
            "dull: -0.28669\n",
            "writer: -0.28616\n",
            "none: -0.28510\n",
            "materi: -0.28453\n",
            "promis: -0.28287\n",
            "wors: -0.28251\n",
            "video: -0.27723\n",
            "lack: -0.26886\n",
            "neither: -0.25785\n",
            "idea: -0.25197\n",
            "could: -0.24933\n",
            "subplot: -0.24768\n",
            "anyway: -0.24458\n",
            "joke: -0.24013\n",
            "guess: -0.23695\n",
            "would: -0.23505\n",
            "talent: -0.23123\n",
            "paul: -0.23121\n",
            "middl: -0.22512\n",
            "pain: -0.22455\n",
            "interest: -0.22240\n",
            "look: -0.21941\n",
            "throw: -0.21500\n",
            "moment: -0.21492\n",
            "appar: -0.21302\n",
            "director: -0.20974\n",
            "save: -0.20787\n",
            "el: -0.20618\n",
            "catch: -0.20454\n",
            "point: -0.20453\n",
            "better: -0.20087\n",
            "project: -0.19598\n",
            "given: -0.19330\n",
            "act: -0.18796\n",
            "isnt: -0.18178\n",
            "tv: -0.18165\n",
            "action: -0.18145\n",
            "couldnt: -0.18005\n",
            "sinc: -0.17930\n",
            "abil: -0.17929\n",
            "play: -0.17764\n",
            "wasnt: -0.17524\n",
            "trailer: -0.17158\n",
            "cant: -0.16861\n",
            "pas: -0.16676\n",
            "wait: -0.16645\n",
            "complet: -0.16420\n",
            "quickli: -0.16293\n",
            "career: -0.16244\n",
            "giant: -0.16151\n",
            "appear: -0.16070\n",
            "bare: -0.15557\n",
            "pretti: -0.15400\n",
            "west: -0.15391\n",
            "filmmak: -0.15386\n",
            "big: -0.15272\n",
            "head: -0.15187\n",
            "basic: -0.15161\n",
            "least: -0.15129\n",
            "like: -0.15058\n",
            "decent: -0.15040\n",
            "femal: -0.15000\n",
            "simpli: -0.14903\n",
            "write: -0.14901\n",
            "serious: -0.14778\n",
            "silli: -0.14704\n",
            "member: -0.14314\n",
            "1: -0.14128\n",
            "formula: -0.14112\n",
            "rule: -0.13980\n",
            "involv: -0.13959\n",
            "count: -0.13943\n",
            "made: -0.13939\n",
            "10: -0.13905\n",
            "annoy: -0.13802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXiMnHRdBDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f245f5b8-bd00-4d2e-be71-10312ca1ed85"
      },
      "source": [
        "# Sklearn Logistic Regression Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "sk_lr = LogisticRegression(solver='lbfgs', max_iter=150).fit(X_train, y_train )\n",
        "y_predict = sk_lr.predict(X_test)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, sk_lr.predict(X_train))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 1.0\n",
            "precision score: 1.0\n",
            "recall score: 1.0\n",
            "F1 score: 1.0\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.8275\n",
            "precision score: 0.8291457286432161\n",
            "recall score: 0.825\n",
            "F1 score: 0.8270676691729322\n"
          ]
        }
      ]
    }
  ]
}