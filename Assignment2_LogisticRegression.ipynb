{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNREnTK7RVKleXMwvEwoPKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment2_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRoN8MBWFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424fa736-3dda-405f-82f9-09150463d34d"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "#from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqCpCGDJrZz"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krvu3FbyUwOn"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/review_polarity.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCAKXzrfWYEc",
        "outputId": "c23e3d28-2867-4a7e-a386-ef8ab7b847d8"
      },
      "source": [
        "# Exploring the data sizes\n",
        "\n",
        "paths_pos = glob.glob('/content/txt_sentoken/pos/*.txt')\n",
        "paths_neg = glob.glob('/content/txt_sentoken/neg/*.txt')\n",
        "pos_neg_paths = paths_pos + paths_neg\n",
        "\n",
        "n_pos = len(paths_pos)\n",
        "n_neg = len(paths_neg)\n",
        "\n",
        "print('the number of positive instances: {} \\nthe number of positive instances: {}'.format(n_pos, n_neg))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of positive instances: 1000 \n",
            "the number of positive instances: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_HBaup_JiGU"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-1M-pGYxd2",
        "outputId": "b08b0a86-cb3d-4d73-94ea-50b92c0178b3"
      },
      "source": [
        "# Exploring the words in the dataset\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenizer (doc):\n",
        "  #doc = doc.lower() # Lowercase documents\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "# def BOW (doc):\n",
        "#   bow = set()\n",
        "#   for token in tokenizer (doc):\n",
        "#     bow.add(token)\n",
        "#   return list(bow)\n",
        "\n",
        "#def word_counter (doc):\n",
        "\n",
        "def stopword_remover (bow):\n",
        "  filtered_bow = [w for w in bow if not w.lower() in stopwords]\n",
        "  return filtered_bow\n",
        "\n",
        "def top_freq_w (freq_dic, top_n, stopword_removing = ''):\n",
        "  sorted_dic = {k:v for k, v in sorted(freq_dic.items(), key = lambda item: item[1], reverse=True)}\n",
        "  if stopword_removing is False:\n",
        "    return {k:v for k, v in list(sorted_dic.items())[:top_n]}\n",
        "  elif stopword_removing is True:\n",
        "    filtered_dic = {k: v for k, v in sorted_dic.items() if k not in stopwords}\n",
        "    return {k:v for k, v in list(filtered_dic.items())[:top_n]}\n",
        "  \n",
        "\n",
        "\n",
        "word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  for token in tokenizer (doc):\n",
        "    word_freq[token] = word_freq.get(token,0)+1\n",
        "\n",
        "top_100_w = top_freq_w(word_freq, 100, stopword_removing = False) \n",
        "\n",
        "print('the number of unique words in the dataset: ', len(word_freq.keys()))\n",
        "print ('top 100 most frequent words:\\n', top_100_w )\n",
        "print('\\nthe number of words in the top 100 which are stopwords: ', len([w for w in top_100_w.keys() if w in stopwords]))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "the number of unique words in the dataset:  39697\n",
            "top 100 most frequent words:\n",
            " {'the': 76529, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, 'is': 25195, 'in': 21822, 's': 18513, 'it': 16107, 'that': 15924, 'as': 11378, 'with': 10792, 'for': 9961, 'his': 9587, 'this': 9578, 'film': 9517, 'i': 8889, 'he': 8864, 'but': 8634, 'on': 7385, 'are': 6949, 't': 6410, 'by': 6261, 'be': 6174, 'one': 5852, 'movie': 5771, 'an': 5744, 'who': 5692, 'not': 5577, 'you': 5316, 'from': 4999, 'at': 4986, 'was': 4940, 'have': 4901, 'they': 4825, 'has': 4719, 'her': 4522, 'all': 4373, 'there': 3770, 'like': 3690, 'so': 3683, 'out': 3637, 'about': 3523, 'up': 3405, 'more': 3347, 'what': 3322, 'when': 3258, 'which': 3161, 'or': 3148, 'she': 3141, 'their': 3122, 'some': 2985, 'just': 2905, 'can': 2882, 'if': 2799, 'we': 2775, 'him': 2633, 'into': 2623, 'even': 2565, 'only': 2495, 'than': 2474, 'no': 2472, 'good': 2411, 'time': 2411, 'most': 2306, 'its': 2270, 'will': 2216, 'story': 2169, '': 2152, 'would': 2109, 'been': 2050, 'much': 2049, 'character': 2020, 'also': 1967, 'get': 1949, 'other': 1948, 'do': 1915, 'two': 1911, 'well': 1906, 'them': 1877, 'very': 1863, 'characters': 1859, 'first': 1836, 'after': 1762, 'see': 1749, 'way': 1693, 'because': 1684, 'make': 1642, 'life': 1586, 'off': 1581, 'too': 1577, 'any': 1574, 'does': 1568, 'really': 1558, 'had': 1546, 'while': 1539, 'films': 1536, 'how': 1517, 'plot': 1513, 'little': 1501}\n",
            "\n",
            "the number of words in the top 100 which are stopwords:  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "XZlPlHizshiT",
        "outputId": "73297dc3-e205-4d10-ff96-53b1992945db"
      },
      "source": [
        "# Reformating the dataset into csv for convenience \n",
        "def to_df (folder):\n",
        "  data_dic = {}\n",
        "  data_dic['doc'], data_dic['label'] = [], []\n",
        "  for file in folder:\n",
        "    fo = open(file)\n",
        "    doc = fo.read()\n",
        "    data_dic['doc'].append(doc)\n",
        "    if 'pos' in file:\n",
        "      data_dic['label'].append(1)\n",
        "    elif 'neg' in file:\n",
        "      data_dic['label'].append(0)\n",
        "    else:\n",
        "      print('error', file)\n",
        "  df = pd.DataFrame.from_dict(data_dic)\n",
        "  return df\n",
        "    \n",
        "data = to_df(pos_neg_paths)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredients : london gal , fate , true love , ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show , an almost perfectly accurate true ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after a stylistic detour with mrs . \\nparker a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>all great things come to an end , and the dot-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udall is a heartless man . \\nhe spends ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  label\n",
              "0  ingredients : london gal , fate , true love , ...      1\n",
              "1  quiz show , an almost perfectly accurate true ...      1\n",
              "2  after a stylistic detour with mrs . \\nparker a...      1\n",
              "3  all great things come to an end , and the dot-...      1\n",
              "4  melvin udall is a heartless man . \\nhe spends ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFEUMkgJYGz"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jGSEg3J8ktsC",
        "outputId": "62f81d32-8f27-4640-eff6-4a795ef4f80f"
      },
      "source": [
        "# Data preprocessing\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "def preprocessor (text):\n",
        "  ## removing punctuations and characters\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # stripping\n",
        "  text = ' '.join([w.strip() for w in text.split()])\n",
        "  # print(text)\n",
        "  ## lowcasing\n",
        "  text = text.lower()\n",
        "  # ## removing stopword\n",
        "  text = stopword_remover (text.split())\n",
        "  # ##stemmming\n",
        "  text = [stemmer.stem(w) for w in text]\n",
        "  # ## lematization\n",
        "  text = [lemmatizer.lemmatize(w) for w in text]\n",
        "  return ' '.join([w for w in text])\n",
        "\n",
        "data['doc'] = data['doc'].apply(lambda x:  preprocessor (x) )\n",
        "data "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredi london gal fate true love run joke mon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show almost perfectli accur true stori ba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stylist detour mr parker viciou circl despit u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>great thing come end dotcom era embodi perfect...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udal heartless man spend day insid spac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>often similar littl boy lost park right ventur...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>13th warrior reek badli melodrama poor act car...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>accord hitchcock variou filmmak isol motel din...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>warn follow review contain spoiler cast gari s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>went saw film right call battlefield earth nev...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    doc  label\n",
              "0     ingredi london gal fate true love run joke mon...      1\n",
              "1     quiz show almost perfectli accur true stori ba...      1\n",
              "2     stylist detour mr parker viciou circl despit u...      1\n",
              "3     great thing come end dotcom era embodi perfect...      1\n",
              "4     melvin udal heartless man spend day insid spac...      1\n",
              "...                                                 ...    ...\n",
              "1995  often similar littl boy lost park right ventur...      0\n",
              "1996  13th warrior reek badli melodrama poor act car...      0\n",
              "1997  accord hitchcock variou filmmak isol motel din...      0\n",
              "1998  warn follow review contain spoiler cast gari s...      0\n",
              "1999  went saw film right call battlefield earth nev...      0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FEl86wJ2Zv"
      },
      "source": [
        "# Developing a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p5-fLZLetp"
      },
      "source": [
        "# Preparing vocabulary\n",
        "## As required, we will use 1000 most frequent word, excluding stopwords\n",
        "### Preprocessing data for building vocabulary\n",
        "cleaned_word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  cleaned_doc = preprocessor(doc)\n",
        "  for token in tokenizer (cleaned_doc):\n",
        "    cleaned_word_freq[token] = cleaned_word_freq.get(token,0)+1\n",
        "\n",
        "vocabulary = top_freq_w(cleaned_word_freq, 1000, stopword_removing = True) \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPM073ASYl5i"
      },
      "source": [
        "# Feature engineering\n",
        "# def feature_extractor (doc):\n",
        "#   doc_vec = []\n",
        "#   for feature in vocabulary.keys():\n",
        "#     feature_count = 0\n",
        "#     if feature in tokenizer (doc):\n",
        "#       feature_count+=1\n",
        "#     else:\n",
        "#       feature_count+=0\n",
        "#     doc_vec.append(feature_count) \n",
        "#   return doc_vec\n",
        "\n",
        "def feature_extractor (doc):\n",
        "  doc_vec = []\n",
        "  token_list = tokenizer (doc)\n",
        "  for feature in vocabulary.keys():\n",
        "    # feature_count=0\n",
        "    feature_count = token_list.count(feature)\n",
        "    doc_vec.append(feature_count) \n",
        "  return doc_vec\n",
        "\n",
        "X = data['doc'].apply(lambda x: feature_extractor(x))\n",
        "y = data['label']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qW3hQL2GFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "38531e75-7ee0-4fe5-e3ed-b9375ea1e2f1"
      },
      "source": [
        "\n",
        "X = X.apply(pd.Series)\n",
        "X.columns = vocabulary.keys()\n",
        "X"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film</th>\n",
              "      <th>movi</th>\n",
              "      <th>one</th>\n",
              "      <th>like</th>\n",
              "      <th>charact</th>\n",
              "      <th>get</th>\n",
              "      <th>make</th>\n",
              "      <th>time</th>\n",
              "      <th>scene</th>\n",
              "      <th>even</th>\n",
              "      <th>good</th>\n",
              "      <th>play</th>\n",
              "      <th>stori</th>\n",
              "      <th>see</th>\n",
              "      <th>would</th>\n",
              "      <th>much</th>\n",
              "      <th>also</th>\n",
              "      <th>go</th>\n",
              "      <th>way</th>\n",
              "      <th>seem</th>\n",
              "      <th>look</th>\n",
              "      <th>end</th>\n",
              "      <th>two</th>\n",
              "      <th>take</th>\n",
              "      <th>first</th>\n",
              "      <th>come</th>\n",
              "      <th>well</th>\n",
              "      <th>work</th>\n",
              "      <th>thing</th>\n",
              "      <th>year</th>\n",
              "      <th>realli</th>\n",
              "      <th>plot</th>\n",
              "      <th>know</th>\n",
              "      <th>perform</th>\n",
              "      <th>littl</th>\n",
              "      <th>life</th>\n",
              "      <th>peopl</th>\n",
              "      <th>love</th>\n",
              "      <th>could</th>\n",
              "      <th>bad</th>\n",
              "      <th>...</th>\n",
              "      <th>rush</th>\n",
              "      <th>realist</th>\n",
              "      <th>scare</th>\n",
              "      <th>manner</th>\n",
              "      <th>command</th>\n",
              "      <th>standard</th>\n",
              "      <th>menac</th>\n",
              "      <th>spent</th>\n",
              "      <th>adam</th>\n",
              "      <th>agre</th>\n",
              "      <th>cinematographi</th>\n",
              "      <th>front</th>\n",
              "      <th>ground</th>\n",
              "      <th>budget</th>\n",
              "      <th>fairli</th>\n",
              "      <th>pair</th>\n",
              "      <th>virtual</th>\n",
              "      <th>suddenli</th>\n",
              "      <th>fantasi</th>\n",
              "      <th>connect</th>\n",
              "      <th>disturb</th>\n",
              "      <th>90</th>\n",
              "      <th>appropri</th>\n",
              "      <th>godzilla</th>\n",
              "      <th>brown</th>\n",
              "      <th>grant</th>\n",
              "      <th>cultur</th>\n",
              "      <th>greatest</th>\n",
              "      <th>store</th>\n",
              "      <th>trip</th>\n",
              "      <th>key</th>\n",
              "      <th>fascin</th>\n",
              "      <th>cute</th>\n",
              "      <th>brief</th>\n",
              "      <th>cameo</th>\n",
              "      <th>count</th>\n",
              "      <th>foot</th>\n",
              "      <th>addit</th>\n",
              "      <th>satir</th>\n",
              "      <th>bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      film  movi  one  like  charact  ...  count  foot  addit  satir  bug\n",
              "0        3     0    4     1        2  ...      0     0      0      0    0\n",
              "1        2     1    1     0        0  ...      0     0      0      0    0\n",
              "2        4     4    3     2        7  ...      0     0      0      0    0\n",
              "3        5     0    0     0        0  ...      0     0      0      0    0\n",
              "4        2     0    2     0        0  ...      0     0      0      0    0\n",
              "...    ...   ...  ...   ...      ...  ...    ...   ...    ...    ...  ...\n",
              "1995    10     1    5     0        2  ...      0     0      1      0    0\n",
              "1996     6     1    2     2        1  ...      0     0      0      0    0\n",
              "1997     3     0    2     0        0  ...      0     0      0      0    0\n",
              "1998    14     4    8     5        6  ...      0     1      0      0    0\n",
              "1999    11     2    2     3        0  ...      0     0      0      0    0\n",
              "\n",
              "[2000 rows x 1000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7yjtdsJ_ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f599d4a8-d0ba-49a4-883b-90a4341e562e"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split (X[vocabulary.keys()], y , train_size = 0.8, random_state = 42, shuffle = True, stratify=data['label'])\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (1600, 1000) (1600,)\n",
            "Shapes of X_test, y_test:  (400, 1000) (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3koUq4K2EJLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a486b32-9530-498e-de95-4e7a9c1ea992"
      },
      "source": [
        "X_train.values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10,  1,  1, ...,  0,  0,  0],\n",
              "       [ 6,  0,  4, ...,  0,  0,  0],\n",
              "       [ 0,  3,  4, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [ 5,  5,  2, ...,  0,  0,  0],\n",
              "       [ 5,  2,  3, ...,  0,  0,  0],\n",
              "       [ 4,  3,  6, ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSfRLnHvKT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7525440-56db-4c73-8711-acddf2dadb5a"
      },
      "source": [
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = None\n",
        "bias = None\n",
        "def computing_gradient (X, Y):\n",
        "  weight = np.zeros(X.shape[1])\n",
        "  bias = 0\n",
        "  for iter in range(n_iter):\n",
        "    for x,y in zip (X.values,Y):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "      d_weight = np.dot((y_pred - y), x)\n",
        "      d_bias = (y_pred - y)\n",
        "      weight -= lr*d_weight\n",
        "      bias -= lr*d_bias\n",
        "  return  weight, bias\n",
        "\n",
        "def predict(X, weight, bias):\n",
        "    z = np.dot(weight, X.values.T) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    y_class = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "    return y_class\n",
        "\n",
        "def sigmoid (z):\n",
        "  p=1/(1+np.exp(-z))\n",
        "  return p\n",
        "\n",
        "# Printing model performance \n",
        "def printing_eval_scores (y_true, y_pred):\n",
        "  print('accuracy score: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "  print('precision score: {}'.format(sklearn.metrics.precision_score(y_true, y_pred)))\n",
        "  print('recall score: {}'.format(sklearn.metrics.recall_score(y_true, y_pred)))\n",
        "  print('F1 score: {}'.format(sklearn.metrics.f1_score(y_true, y_pred)))\n",
        "  print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "weight, bias = computing_gradient (X_train, y_train)\n",
        "y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, predict(X_train,weight, bias))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 0.965625\n",
            "precision score: 0.9408284023668639\n",
            "recall score: 0.99375\n",
            "F1 score: 0.966565349544073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96       800\n",
            "           1       0.94      0.99      0.97       800\n",
            "\n",
            "    accuracy                           0.97      1600\n",
            "   macro avg       0.97      0.97      0.97      1600\n",
            "weighted avg       0.97      0.97      0.97      1600\n",
            "\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.7925\n",
            "precision score: 0.76\n",
            "recall score: 0.855\n",
            "F1 score: 0.8047058823529413\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.73      0.78       200\n",
            "           1       0.76      0.85      0.80       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.79      0.79       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wviTYBaXOCxF"
      },
      "source": [
        "#Minibatch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0rgm4ULuTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d9e9db-5ebd-41a3-80be-8557d9c8902c"
      },
      "source": [
        "\n",
        "\n",
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = None\n",
        "bias = None\n",
        "def computing_MiniBatch_gradient (X, Y, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  weight = np.zeros(n_features)\n",
        "  bias = 0\n",
        "  for iter in range(n_iter):\n",
        "    i=0 \n",
        "    while i< round(n_instances/batch_size):\n",
        "      m = batch_size * i\n",
        "      n = m + batch_size\n",
        "      sum_w = 0\n",
        "      sum_b = 0\n",
        "      for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "        z = np.dot(weight, x) + bias\n",
        "        y_pred = sigmoid(z)\n",
        "        #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "        sum_w += np.dot((y_pred - y), x)\n",
        "        sum_b += (y_pred - y)\n",
        "      d_weight = (1/batch_size)*sum_w\n",
        "      d_bias = (1/batch_size)*sum_b\n",
        "      weight -= lr*d_weight\n",
        "      bias -= lr*d_bias\n",
        "      i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "\n",
        "weight, bias = computing_MiniBatch_gradient (X_train, y_train, batch_size = 32)\n",
        "y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, predict(X_train,weight, bias))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 0.945\n",
            "precision score: 0.9129930394431555\n",
            "recall score: 0.98375\n",
            "F1 score: 0.9470517448856799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94       800\n",
            "           1       0.91      0.98      0.95       800\n",
            "\n",
            "    accuracy                           0.94      1600\n",
            "   macro avg       0.95      0.95      0.94      1600\n",
            "weighted avg       0.95      0.94      0.94      1600\n",
            "\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.83\n",
            "precision score: 0.7844827586206896\n",
            "recall score: 0.91\n",
            "F1 score: 0.8425925925925926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.82       200\n",
            "           1       0.78      0.91      0.84       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSYOmfA1x70"
      },
      "source": [
        "# L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ESdBPV11xP",
        "outputId": "826c5177-961d-447d-dc05-f70c1bc7541b"
      },
      "source": [
        "\n",
        "\n",
        "# class LogisticRegression ():\n",
        "#   def __init__ (lr = '', n_iter = 10):\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "alpha = 0.01\n",
        "weight = None\n",
        "bias = None\n",
        "def MiniBatch_gradient_L2 (X, Y, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  weight = np.zeros(n_features)\n",
        "  bias = 0\n",
        "  for iter in range(n_iter):\n",
        "    i=0 \n",
        "    while i< round(n_instances/batch_size):\n",
        "      m = batch_size * i\n",
        "      n = m + batch_size\n",
        "      sum_w = 0\n",
        "      sum_b = 0\n",
        "      for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "        z = np.dot(weight, x) + bias\n",
        "        y_pred = sigmoid(z)\n",
        "        w_i = np.dot((y_pred - y), x)\n",
        "        b_i = y_pred - y\n",
        "        sum_w +=  w_i + (alpha * w_i) # L2\n",
        "        sum_b += b_i\n",
        "      d_weight = (1/batch_size)*sum_w\n",
        "      d_bias = (1/batch_size)*sum_b\n",
        "      weight -= lr*d_weight\n",
        "      bias -= lr*d_bias\n",
        "      i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "\n",
        "weight, bias = MiniBatch_gradient_L2 (X_train, y_train, batch_size = 32)\n",
        "y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, predict(X_train,weight, bias))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 0.945625\n",
            "precision score: 0.9140534262485482\n",
            "recall score: 0.98375\n",
            "F1 score: 0.9476219145093318\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.94       800\n",
            "           1       0.91      0.98      0.95       800\n",
            "\n",
            "    accuracy                           0.95      1600\n",
            "   macro avg       0.95      0.95      0.95      1600\n",
            "weighted avg       0.95      0.95      0.95      1600\n",
            "\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.83\n",
            "precision score: 0.7844827586206896\n",
            "recall score: 0.91\n",
            "F1 score: 0.8425925925925926\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.75      0.82       200\n",
            "           1       0.78      0.91      0.84       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rklpxbuq8pcR",
        "outputId": "daecdf04-767b-4ab9-d9cd-64d705189b8f"
      },
      "source": [
        "feature_weights = {}\n",
        "for i in range(len(weight)):\n",
        "  feature_weights[list(vocabulary.keys())[i]] = weight[i]\n",
        "\n",
        "sorted_feature_weights = {k:v for k, v in sorted(feature_weights.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Print the weights learned for each class\n",
        "print('The most important features of POSITIVE class: ')\n",
        "for k, v in list(sorted_feature_weights.items())[:100]:\n",
        "  print ('{}: {:.5f}'. format(k,v))\n",
        "\n",
        "print('\\n==============================================')\n",
        "print('The most important features of NEGATIVE class: ')\n",
        "for k, v in list(sorted_feature_weights.items())[:-100:-1]: \n",
        "  print ('{}: {:.5f}'. format(k,v))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most important features of POSITIVE class: \n",
            "fun: 0.42265\n",
            "great: 0.40735\n",
            "enjoy: 0.37480\n",
            "sometim: 0.36070\n",
            "excel: 0.35632\n",
            "definit: 0.34150\n",
            "true: 0.33130\n",
            "seen: 0.32220\n",
            "surpris: 0.31124\n",
            "job: 0.30855\n",
            "hilari: 0.30084\n",
            "portray: 0.29643\n",
            "polit: 0.29020\n",
            "flaw: 0.28833\n",
            "memor: 0.28279\n",
            "natur: 0.27820\n",
            "perform: 0.27736\n",
            "fiction: 0.27619\n",
            "sever: 0.27515\n",
            "thank: 0.27465\n",
            "togeth: 0.27424\n",
            "especi: 0.27290\n",
            "entertain: 0.27184\n",
            "perfectli: 0.26457\n",
            "quit: 0.26265\n",
            "le: 0.26241\n",
            "equal: 0.25962\n",
            "also: 0.25837\n",
            "overal: 0.25551\n",
            "put: 0.25370\n",
            "hit: 0.25263\n",
            "see: 0.24814\n",
            "perfect: 0.24477\n",
            "good: 0.24223\n",
            "deserv: 0.24109\n",
            "view: 0.23855\n",
            "cameron: 0.23639\n",
            "american: 0.23426\n",
            "yet: 0.23051\n",
            "without: 0.22816\n",
            "well: 0.22788\n",
            "first: 0.22531\n",
            "normal: 0.22400\n",
            "take: 0.22366\n",
            "david: 0.22162\n",
            "ive: 0.22049\n",
            "relationship: 0.21709\n",
            "solid: 0.21437\n",
            "differ: 0.21420\n",
            "realiti: 0.21351\n",
            "share: 0.20827\n",
            "carri: 0.20585\n",
            "color: 0.20498\n",
            "visual: 0.20132\n",
            "summer: 0.19974\n",
            "back: 0.19740\n",
            "final: 0.19689\n",
            "million: 0.19630\n",
            "truman: 0.19475\n",
            "local: 0.19335\n",
            "throughout: 0.19274\n",
            "delight: 0.19271\n",
            "plenti: 0.19151\n",
            "song: 0.19112\n",
            "period: 0.19047\n",
            "wonder: 0.19023\n",
            "dark: 0.18725\n",
            "mother: 0.18475\n",
            "peopl: 0.18233\n",
            "genr: 0.18070\n",
            "crime: 0.18020\n",
            "money: 0.17980\n",
            "life: 0.17790\n",
            "deliv: 0.17540\n",
            "everyth: 0.17536\n",
            "rais: 0.17466\n",
            "rush: 0.17291\n",
            "ben: 0.17267\n",
            "chan: 0.17171\n",
            "father: 0.17113\n",
            "pictur: 0.17069\n",
            "due: 0.17018\n",
            "realist: 0.16939\n",
            "recommend: 0.16668\n",
            "mind: 0.16620\n",
            "keep: 0.16598\n",
            "eventu: 0.16460\n",
            "day: 0.16427\n",
            "secret: 0.16406\n",
            "slightli: 0.16277\n",
            "allow: 0.16251\n",
            "follow: 0.16197\n",
            "year: 0.16157\n",
            "lie: 0.16090\n",
            "flick: 0.15964\n",
            "fine: 0.15944\n",
            "bit: 0.15887\n",
            "bond: 0.15842\n",
            "laugh: 0.15757\n",
            "alway: 0.15582\n",
            "\n",
            "==============================================\n",
            "The most important features of NEGATIVE class: \n",
            "worst: -0.58406\n",
            "bad: -0.57883\n",
            "bore: -0.55509\n",
            "wast: -0.51944\n",
            "unfortun: -0.49776\n",
            "suppos: -0.48447\n",
            "stupid: -0.39647\n",
            "noth: -0.38982\n",
            "potenti: -0.38613\n",
            "attempt: -0.36410\n",
            "fall: -0.35159\n",
            "subplot: -0.33403\n",
            "aw: -0.33351\n",
            "ridicul: -0.32261\n",
            "fail: -0.31535\n",
            "clich: -0.30777\n",
            "could: -0.30013\n",
            "terribl: -0.29810\n",
            "dull: -0.29430\n",
            "materi: -0.29399\n",
            "given: -0.28751\n",
            "mess: -0.28536\n",
            "better: -0.28285\n",
            "look: -0.28058\n",
            "guess: -0.27861\n",
            "mayb: -0.27848\n",
            "poor: -0.27492\n",
            "1: -0.27129\n",
            "would: -0.26683\n",
            "plot: -0.26516\n",
            "eddi: -0.25038\n",
            "west: -0.24881\n",
            "none: -0.24645\n",
            "reason: -0.24175\n",
            "neither: -0.24161\n",
            "save: -0.23631\n",
            "promis: -0.22995\n",
            "video: -0.22952\n",
            "anyway: -0.22625\n",
            "isnt: -0.22580\n",
            "talk: -0.22491\n",
            "problem: -0.22425\n",
            "wors: -0.22118\n",
            "woman: -0.21526\n",
            "script: -0.21373\n",
            "writer: -0.21341\n",
            "talent: -0.20894\n",
            "lack: -0.20324\n",
            "joke: -0.20241\n",
            "club: -0.20095\n",
            "simpli: -0.20024\n",
            "gun: -0.19965\n",
            "annoy: -0.19953\n",
            "throw: -0.19871\n",
            "wait: -0.19702\n",
            "moment: -0.19669\n",
            "catch: -0.19490\n",
            "appear: -0.19300\n",
            "origin: -0.19124\n",
            "4: -0.19026\n",
            "made: -0.18984\n",
            "involv: -0.18933\n",
            "appar: -0.18922\n",
            "tv: -0.18802\n",
            "even: -0.18607\n",
            "interest: -0.18582\n",
            "abil: -0.18574\n",
            "dumb: -0.18511\n",
            "pas: -0.18428\n",
            "3: -0.18251\n",
            "version: -0.17988\n",
            "either: -0.17974\n",
            "sinc: -0.17486\n",
            "minut: -0.17453\n",
            "presenc: -0.17408\n",
            "trailer: -0.17285\n",
            "tri: -0.17238\n",
            "girl: -0.16942\n",
            "girlfriend: -0.16757\n",
            "gag: -0.16405\n",
            "point: -0.16308\n",
            "saw: -0.15965\n",
            "result: -0.15964\n",
            "written: -0.15853\n",
            "quickli: -0.15796\n",
            "return: -0.15685\n",
            "rest: -0.15673\n",
            "complet: -0.15659\n",
            "idea: -0.15569\n",
            "second: -0.15427\n",
            "confus: -0.15321\n",
            "horribl: -0.15307\n",
            "must: -0.15265\n",
            "couldnt: -0.15220\n",
            "mar: -0.15123\n",
            "career: -0.14976\n",
            "mention: -0.14867\n",
            "batman: -0.14866\n",
            "paul: -0.14805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXiMnHRdBDL",
        "outputId": "a2d44eda-3040-4f51-a098-6c521f536614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sklearn Logistic Regression Model\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sk_lr = SGDClassifier(penalty='l2', alpha = 0.001,random_state=42,  loss = 'log' ).partial_fit(X_train, y_train, classes = np.unique(y_train) )\n",
        "y_predict = sk_lr.predict(X_test)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, sk_lr.predict(X_train))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 0.77\n",
            "precision score: 0.932\n",
            "recall score: 0.5825\n",
            "F1 score: 0.7169230769230771\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.96      0.81       800\n",
            "           1       0.93      0.58      0.72       800\n",
            "\n",
            "    accuracy                           0.77      1600\n",
            "   macro avg       0.81      0.77      0.76      1600\n",
            "weighted avg       0.81      0.77      0.76      1600\n",
            "\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.7275\n",
            "precision score: 0.9174311926605505\n",
            "recall score: 0.5\n",
            "F1 score: 0.6472491909385113\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.95      0.78       200\n",
            "           1       0.92      0.50      0.65       200\n",
            "\n",
            "    accuracy                           0.73       400\n",
            "   macro avg       0.79      0.73      0.71       400\n",
            "weighted avg       0.79      0.73      0.71       400\n",
            "\n"
          ]
        }
      ]
    }
  ]
}