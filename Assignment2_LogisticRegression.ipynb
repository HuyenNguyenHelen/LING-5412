{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFnVkBAe6sc+agv2RbvQAe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment2_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRoN8MBWFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e2a3b16-cd94-4d54-ee3d-4f57f5741d1f"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "#from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqCpCGDJrZz"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krvu3FbyUwOn"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/review_polarity.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCAKXzrfWYEc",
        "outputId": "1516836b-1fe8-4b38-a14f-863f985c3705"
      },
      "source": [
        "# Exploring the data sizes\n",
        "\n",
        "paths_pos = glob.glob('/content/txt_sentoken/pos/*.txt')\n",
        "paths_neg = glob.glob('/content/txt_sentoken/neg/*.txt')\n",
        "pos_neg_paths = paths_pos + paths_neg\n",
        "\n",
        "n_pos = len(paths_pos)\n",
        "n_neg = len(paths_neg)\n",
        "\n",
        "print('the number of positive instances: {} \\nthe number of positive instances: {}'.format(n_pos, n_neg))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of positive instances: 1000 \n",
            "the number of positive instances: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_HBaup_JiGU"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-1M-pGYxd2",
        "outputId": "b5cae51d-e520-4191-a53d-4fd75e50725f"
      },
      "source": [
        "# Exploring the words in the dataset\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenizer (doc):\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "def stopword_remover (bow):\n",
        "  filtered_bow = [w for w in bow if not w.lower() in stopwords]\n",
        "  return filtered_bow\n",
        "\n",
        "def top_freq_w (freq_dic, top_n, stopword_removing = ''):\n",
        "  sorted_dic = {k:v for k, v in sorted(freq_dic.items(), key = lambda item: item[1], reverse=True)}\n",
        "  if stopword_removing is False:\n",
        "    return {k:v for k, v in list(sorted_dic.items())[:top_n]}\n",
        "  elif stopword_removing is True:\n",
        "    filtered_dic = {k: v for k, v in sorted_dic.items() if k not in stopwords}\n",
        "    return {k:v for k, v in list(filtered_dic.items())[:top_n]}\n",
        "  \n",
        "\n",
        "word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  for token in tokenizer (doc):\n",
        "    word_freq[token] = word_freq.get(token,0)+1\n",
        "\n",
        "top_100_w = top_freq_w(word_freq, 100, stopword_removing = False) \n",
        "\n",
        "print('\\nthe number of unique words in the dataset: ', len(word_freq.keys()))\n",
        "print ('\\ntop 100 most frequent words:\\n', top_100_w )\n",
        "print('\\nthe number of words in the top 100 which are stopwords: ', len([w for w in top_100_w.keys() if w in stopwords]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "the number of unique words in the dataset:  39697\n",
            "\n",
            "top 100 most frequent words:\n",
            " {'the': 76529, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, 'is': 25195, 'in': 21822, 's': 18513, 'it': 16107, 'that': 15924, 'as': 11378, 'with': 10792, 'for': 9961, 'his': 9587, 'this': 9578, 'film': 9517, 'i': 8889, 'he': 8864, 'but': 8634, 'on': 7385, 'are': 6949, 't': 6410, 'by': 6261, 'be': 6174, 'one': 5852, 'movie': 5771, 'an': 5744, 'who': 5692, 'not': 5577, 'you': 5316, 'from': 4999, 'at': 4986, 'was': 4940, 'have': 4901, 'they': 4825, 'has': 4719, 'her': 4522, 'all': 4373, 'there': 3770, 'like': 3690, 'so': 3683, 'out': 3637, 'about': 3523, 'up': 3405, 'more': 3347, 'what': 3322, 'when': 3258, 'which': 3161, 'or': 3148, 'she': 3141, 'their': 3122, 'some': 2985, 'just': 2905, 'can': 2882, 'if': 2799, 'we': 2775, 'him': 2633, 'into': 2623, 'even': 2565, 'only': 2495, 'than': 2474, 'no': 2472, 'time': 2411, 'good': 2411, 'most': 2306, 'its': 2270, 'will': 2216, 'story': 2169, '': 2152, 'would': 2109, 'been': 2050, 'much': 2049, 'character': 2020, 'also': 1967, 'get': 1949, 'other': 1948, 'do': 1915, 'two': 1911, 'well': 1906, 'them': 1877, 'very': 1863, 'characters': 1859, 'first': 1836, 'after': 1762, 'see': 1749, 'way': 1693, 'because': 1684, 'make': 1642, 'life': 1586, 'off': 1581, 'too': 1577, 'any': 1574, 'does': 1568, 'really': 1558, 'had': 1546, 'while': 1539, 'films': 1536, 'how': 1517, 'plot': 1513, 'little': 1501}\n",
            "\n",
            "the number of words in the top 100 which are stopwords:  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "XZlPlHizshiT",
        "outputId": "57a4ec73-fda5-417a-9da4-8bc3a5503c91"
      },
      "source": [
        "# Reformating the dataset into csv for convenience \n",
        "\n",
        "def to_df (folder):\n",
        "  data_dic = {}\n",
        "  data_dic['doc'], data_dic['label'] = [], []\n",
        "  for file in folder:\n",
        "    fo = open(file)\n",
        "    doc = fo.read()\n",
        "    data_dic['doc'].append(doc)\n",
        "    if 'pos' in file:\n",
        "      data_dic['label'].append(1)\n",
        "    elif 'neg' in file:\n",
        "      data_dic['label'].append(0)\n",
        "    else:\n",
        "      print('error', file)\n",
        "  df = pd.DataFrame.from_dict(data_dic)\n",
        "  return df\n",
        "    \n",
        "data = to_df(pos_neg_paths)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the wake of the smashing success of \" rumbl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ingredients : little orphan boy , rural grandp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after watching the first ten minutes of this j...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen , star of many a brian depalma movie in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in \" the sweet hereafter , \" writer/director a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  label\n",
              "0  in the wake of the smashing success of \" rumbl...      1\n",
              "1  ingredients : little orphan boy , rural grandp...      1\n",
              "2  after watching the first ten minutes of this j...      1\n",
              "3  allen , star of many a brian depalma movie in ...      1\n",
              "4  in \" the sweet hereafter , \" writer/director a...      1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFEUMkgJYGz"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jGSEg3J8ktsC",
        "outputId": "9080758e-6143-4049-d4d4-f7f2914c345a"
      },
      "source": [
        "# Data preprocessing\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def preprocessor (text):\n",
        "  ## removing punctuations and characters\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # stripping\n",
        "  text = ' '.join([w.strip() for w in text.split()])\n",
        "  ## lowcasing\n",
        "  text = text.lower()\n",
        "  # ## removing stopword\n",
        "  text = stopword_remover (text.split())\n",
        "  # ##stemmming\n",
        "  text = [stemmer.stem(w) for w in text]\n",
        "  # ## lematization\n",
        "  text = [lemmatizer.lemmatize(w) for w in text]\n",
        "  return ' '.join([w for w in text])\n",
        "\n",
        "data['doc'] = data['doc'].apply(lambda x:  preprocessor (x) )\n",
        "data "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wake smash success rumbl bronx look like jacki...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ingredi littl orphan boy rural grandpar mounta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>watch first ten minut japanes film never eat b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen star mani brian depalma movi earli eight...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sweet hereaft writerdirector atom egoyan take ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>last carri movi discount carri columbu carri m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>one indic bad film hype rememb film case box h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>film mean well pushi promot belabor point sent...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>start littl mermaid recent lion king walt disn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>son share pervers predilect bad movi amus ente...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    doc  label\n",
              "0     wake smash success rumbl bronx look like jacki...      1\n",
              "1     ingredi littl orphan boy rural grandpar mounta...      1\n",
              "2     watch first ten minut japanes film never eat b...      1\n",
              "3     allen star mani brian depalma movi earli eight...      1\n",
              "4     sweet hereaft writerdirector atom egoyan take ...      1\n",
              "...                                                 ...    ...\n",
              "1995  last carri movi discount carri columbu carri m...      0\n",
              "1996  one indic bad film hype rememb film case box h...      0\n",
              "1997  film mean well pushi promot belabor point sent...      0\n",
              "1998  start littl mermaid recent lion king walt disn...      0\n",
              "1999  son share pervers predilect bad movi amus ente...      0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FEl86wJ2Zv"
      },
      "source": [
        "# Building a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJ_OjUQy0iY"
      },
      "source": [
        "### Preparing vocabulary/feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p5-fLZLetp"
      },
      "source": [
        "\n",
        "## As required, we will use 1000 most frequent word, excluding stopwords\n",
        "\n",
        "cleaned_word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  # Cleaning documents before extracting features\n",
        "  cleaned_doc = preprocessor(doc)\n",
        "  # Getting terms and their frequency \n",
        "  for token in tokenizer (cleaned_doc):\n",
        "    cleaned_word_freq[token] = cleaned_word_freq.get(token,0)+1\n",
        "\n",
        "# Getting 1000 terms with highest frequency, excluding stopwords\n",
        "vocabulary = top_freq_w(cleaned_word_freq, 1000, stopword_removing = True) \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW4AHP3v0UPg"
      },
      "source": [
        "### Representing documents based on extracted features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPM073ASYl5i"
      },
      "source": [
        "# binary representing documents based on occurrance of features in documents\n",
        "def doc_representor (doc):\n",
        "  doc_vec = []\n",
        "  token_list = tokenizer (doc)\n",
        "  for feature in vocabulary.keys():\n",
        "    if feature in token_list:\n",
        "      doc_vec.append(1)\n",
        "    else:\n",
        "       doc_vec.append(0)\n",
        "  return doc_vec\n",
        "\n",
        "X = data['doc'].apply(lambda x: doc_representor(x))\n",
        "y = data['label']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qW3hQL2GFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "e785c378-bed2-485d-89ac-58183e0d398e"
      },
      "source": [
        "# Visualize the data after representing\n",
        "X = X.apply(pd.Series)\n",
        "X.columns = vocabulary.keys()\n",
        "print(X.shape)\n",
        "X"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film</th>\n",
              "      <th>movi</th>\n",
              "      <th>one</th>\n",
              "      <th>like</th>\n",
              "      <th>charact</th>\n",
              "      <th>get</th>\n",
              "      <th>make</th>\n",
              "      <th>time</th>\n",
              "      <th>scene</th>\n",
              "      <th>even</th>\n",
              "      <th>good</th>\n",
              "      <th>play</th>\n",
              "      <th>stori</th>\n",
              "      <th>see</th>\n",
              "      <th>would</th>\n",
              "      <th>much</th>\n",
              "      <th>also</th>\n",
              "      <th>go</th>\n",
              "      <th>way</th>\n",
              "      <th>seem</th>\n",
              "      <th>look</th>\n",
              "      <th>end</th>\n",
              "      <th>two</th>\n",
              "      <th>take</th>\n",
              "      <th>first</th>\n",
              "      <th>come</th>\n",
              "      <th>well</th>\n",
              "      <th>work</th>\n",
              "      <th>thing</th>\n",
              "      <th>year</th>\n",
              "      <th>realli</th>\n",
              "      <th>plot</th>\n",
              "      <th>know</th>\n",
              "      <th>perform</th>\n",
              "      <th>littl</th>\n",
              "      <th>life</th>\n",
              "      <th>peopl</th>\n",
              "      <th>love</th>\n",
              "      <th>could</th>\n",
              "      <th>bad</th>\n",
              "      <th>...</th>\n",
              "      <th>lover</th>\n",
              "      <th>island</th>\n",
              "      <th>scare</th>\n",
              "      <th>spent</th>\n",
              "      <th>cinematographi</th>\n",
              "      <th>agre</th>\n",
              "      <th>manner</th>\n",
              "      <th>command</th>\n",
              "      <th>standard</th>\n",
              "      <th>menac</th>\n",
              "      <th>adam</th>\n",
              "      <th>front</th>\n",
              "      <th>fairli</th>\n",
              "      <th>budget</th>\n",
              "      <th>ground</th>\n",
              "      <th>brown</th>\n",
              "      <th>appropri</th>\n",
              "      <th>pair</th>\n",
              "      <th>disturb</th>\n",
              "      <th>virtual</th>\n",
              "      <th>connect</th>\n",
              "      <th>suddenli</th>\n",
              "      <th>grant</th>\n",
              "      <th>fantasi</th>\n",
              "      <th>90</th>\n",
              "      <th>godzilla</th>\n",
              "      <th>cultur</th>\n",
              "      <th>cameo</th>\n",
              "      <th>count</th>\n",
              "      <th>store</th>\n",
              "      <th>brief</th>\n",
              "      <th>cute</th>\n",
              "      <th>key</th>\n",
              "      <th>fascin</th>\n",
              "      <th>greatest</th>\n",
              "      <th>trip</th>\n",
              "      <th>bug</th>\n",
              "      <th>foot</th>\n",
              "      <th>addit</th>\n",
              "      <th>satir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      film  movi  one  like  charact  ...  trip  bug  foot  addit  satir\n",
              "0        1     1    1     1        1  ...     0    0     0      0      0\n",
              "1        1     1    1     1        1  ...     0    0     0      0      0\n",
              "2        1     0    1     1        0  ...     0    0     0      0      0\n",
              "3        1     1    1     1        0  ...     0    0     0      0      0\n",
              "4        1     1    1     1        1  ...     0    0     0      0      0\n",
              "...    ...   ...  ...   ...      ...  ...   ...  ...   ...    ...    ...\n",
              "1995     1     1    0     1        0  ...     0    0     0      0      0\n",
              "1996     1     1    1     0        1  ...     0    0     0      0      0\n",
              "1997     1     1    1     0        1  ...     0    0     0      0      0\n",
              "1998     1     1    1     1        1  ...     0    0     0      0      0\n",
              "1999     0     1    1     0        0  ...     0    0     0      0      0\n",
              "\n",
              "[2000 rows x 1000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7yjtdsJ_ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c5afbf-e0f6-4a45-c3dd-a6989cdd4f78"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split (X[vocabulary.keys()], y , train_size = 0.8, random_state = 42, shuffle = True, stratify=data['label'])\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (1600, 1000) (1600,)\n",
            "Shapes of X_test, y_test:  (400, 1000) (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTzyxg0x1m3y"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSfRLnHvKT7"
      },
      "source": [
        "# Writing functions\n",
        "\n",
        "def sigmoid (z):\n",
        "  p=1/(1+np.exp(-z))\n",
        "  return p\n",
        "\n",
        "\n",
        "def predict(X, weight, bias):\n",
        "    z = np.dot(weight, X.values.T) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    y_class = [1 if i >= 0.5 else 0 for i in y_pred]\n",
        "    return y_class\n",
        "\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred)\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred)\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1\n",
        "  \n",
        "\n",
        "\n",
        "def computing_gradient (X, Y, weight, bias):\n",
        "  for x,y in zip (X.values, Y):\n",
        "    z = np.dot(weight, x) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "    d_weight = np.dot((y_pred - y), x)\n",
        "    d_bias = (y_pred - y)\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "  return weight, bias\n",
        "\n",
        "def computing_MiniBatch_gradient (X, Y, weight, bias, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i<= round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "      sum_w += np.dot((y_pred - y), x)\n",
        "      sum_b += (y_pred - y)\n",
        "    d_weight = (1/batch_size)*sum_w\n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "def MiniBatch_gradient_L2 (X, Y, weight, bias,alpha, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i<= round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      w_i = np.dot((y_pred - y), x)\n",
        "      b_i = y_pred - y\n",
        "      sum_w +=  w_i  + (alpha * weight) # L2\n",
        "      sum_b += b_i\n",
        "    d_weight = (1/batch_size)*sum_w \n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWScj3p-39kj"
      },
      "source": [
        "#### Initial LR model\n",
        "updating weight and bias with Gradient descent every instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8h5nbJd37UT",
        "outputId": "87ec9c24-25fb-485a-d736-3462ab7da1ed"
      },
      "source": [
        "# Training the model, updating weight and bias every instances\n",
        "\n",
        "print('Training LR.......................................................')\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================iteration %s=======================' % str(iter+1))\n",
        "  weight, bias = computing_gradient (X_train, y_train, weight, bias)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LR.......................................................\n",
            "\n",
            "====================iteration 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.902\n",
            "precision score: 0.893\n",
            "recall score: 0.915\n",
            "F1 score: 0.904\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.762\n",
            "precision score: 0.733\n",
            "recall score: 0.825\n",
            "F1 score: 0.776\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.70      0.75       200\n",
            "           1       0.73      0.82      0.78       200\n",
            "\n",
            "    accuracy                           0.76       400\n",
            "   macro avg       0.77      0.76      0.76       400\n",
            "weighted avg       0.77      0.76      0.76       400\n",
            "\n",
            "\n",
            "====================iteration 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.932\n",
            "precision score: 0.959\n",
            "recall score: 0.902\n",
            "F1 score: 0.930\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.757\n",
            "precision score: 0.775\n",
            "recall score: 0.725\n",
            "F1 score: 0.749\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.79      0.77       200\n",
            "           1       0.78      0.72      0.75       200\n",
            "\n",
            "    accuracy                           0.76       400\n",
            "   macro avg       0.76      0.76      0.76       400\n",
            "weighted avg       0.76      0.76      0.76       400\n",
            "\n",
            "\n",
            "====================iteration 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.949\n",
            "precision score: 0.974\n",
            "recall score: 0.922\n",
            "F1 score: 0.947\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.790\n",
            "precision score: 0.805\n",
            "recall score: 0.765\n",
            "F1 score: 0.785\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.80       200\n",
            "           1       0.81      0.77      0.78       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.964\n",
            "precision score: 0.946\n",
            "recall score: 0.984\n",
            "F1 score: 0.964\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.797\n",
            "precision score: 0.762\n",
            "recall score: 0.865\n",
            "F1 score: 0.810\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       200\n",
            "           1       0.76      0.86      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================iteration 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.979\n",
            "precision score: 0.976\n",
            "recall score: 0.981\n",
            "F1 score: 0.979\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.785\n",
            "precision score: 0.777\n",
            "recall score: 0.800\n",
            "F1 score: 0.788\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78       200\n",
            "           1       0.78      0.80      0.79       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.78       400\n",
            "weighted avg       0.79      0.79      0.78       400\n",
            "\n",
            "\n",
            "====================iteration 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.983\n",
            "precision score: 0.971\n",
            "recall score: 0.995\n",
            "F1 score: 0.983\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.787\n",
            "precision score: 0.760\n",
            "recall score: 0.840\n",
            "F1 score: 0.798\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.73      0.78       200\n",
            "           1       0.76      0.84      0.80       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.993\n",
            "precision score: 0.993\n",
            "recall score: 0.994\n",
            "F1 score: 0.993\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.785\n",
            "precision score: 0.782\n",
            "recall score: 0.790\n",
            "F1 score: 0.786\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.78       200\n",
            "           1       0.78      0.79      0.79       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.78       400\n",
            "weighted avg       0.79      0.79      0.78       400\n",
            "\n",
            "\n",
            "====================iteration 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.996\n",
            "precision score: 0.993\n",
            "recall score: 1.000\n",
            "F1 score: 0.996\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.773\n",
            "recall score: 0.835\n",
            "F1 score: 0.803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.76      0.79       200\n",
            "           1       0.77      0.83      0.80       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.999\n",
            "precision score: 0.998\n",
            "recall score: 1.000\n",
            "F1 score: 0.999\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.790\n",
            "precision score: 0.761\n",
            "recall score: 0.845\n",
            "F1 score: 0.801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.73      0.78       200\n",
            "           1       0.76      0.84      0.80       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 10=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.998\n",
            "precision score: 0.996\n",
            "recall score: 1.000\n",
            "F1 score: 0.998\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.785\n",
            "precision score: 0.752\n",
            "recall score: 0.850\n",
            "F1 score: 0.798\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.72      0.77       200\n",
            "           1       0.75      0.85      0.80       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.78      0.78       400\n",
            "weighted avg       0.79      0.79      0.78       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wviTYBaXOCxF"
      },
      "source": [
        "#### Minibatch training\n",
        "updating weight and bias with Gradient descent every batch-size = 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0rgm4ULuTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a49ce2-a1b7-4b7d-e478-00e67665ce7e"
      },
      "source": [
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================iteration %s=======================' % str(iter+1))\n",
        "  weight, bias = computing_MiniBatch_gradient (X_train, y_train, weight, bias,  batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "  \n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================iteration 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.858\n",
            "precision score: 0.812\n",
            "recall score: 0.931\n",
            "F1 score: 0.868\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.770\n",
            "precision score: 0.709\n",
            "recall score: 0.915\n",
            "F1 score: 0.799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.62      0.73       200\n",
            "           1       0.71      0.92      0.80       200\n",
            "\n",
            "    accuracy                           0.77       400\n",
            "   macro avg       0.79      0.77      0.77       400\n",
            "weighted avg       0.79      0.77      0.77       400\n",
            "\n",
            "\n",
            "====================iteration 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.879\n",
            "precision score: 0.849\n",
            "recall score: 0.922\n",
            "F1 score: 0.884\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.746\n",
            "recall score: 0.895\n",
            "F1 score: 0.814\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.69      0.77       200\n",
            "           1       0.75      0.90      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.79      0.79       400\n",
            "weighted avg       0.81      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.901\n",
            "precision score: 0.875\n",
            "recall score: 0.935\n",
            "F1 score: 0.904\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.748\n",
            "recall score: 0.890\n",
            "F1 score: 0.813\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.70      0.77       200\n",
            "           1       0.75      0.89      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.79      0.79       400\n",
            "weighted avg       0.81      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.911\n",
            "precision score: 0.888\n",
            "recall score: 0.940\n",
            "F1 score: 0.913\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.800\n",
            "precision score: 0.756\n",
            "recall score: 0.885\n",
            "F1 score: 0.816\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.78       200\n",
            "           1       0.76      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================iteration 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.917\n",
            "precision score: 0.892\n",
            "recall score: 0.950\n",
            "F1 score: 0.920\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.805\n",
            "precision score: 0.761\n",
            "recall score: 0.890\n",
            "F1 score: 0.820\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       200\n",
            "           1       0.76      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.81      0.80       400\n",
            "\n",
            "\n",
            "====================iteration 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.926\n",
            "precision score: 0.903\n",
            "recall score: 0.955\n",
            "F1 score: 0.928\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.810\n",
            "precision score: 0.765\n",
            "recall score: 0.895\n",
            "F1 score: 0.825\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       200\n",
            "           1       0.76      0.90      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.932\n",
            "precision score: 0.912\n",
            "recall score: 0.958\n",
            "F1 score: 0.934\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.807\n",
            "precision score: 0.762\n",
            "recall score: 0.895\n",
            "F1 score: 0.823\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       200\n",
            "           1       0.76      0.90      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.937\n",
            "precision score: 0.917\n",
            "recall score: 0.961\n",
            "F1 score: 0.938\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.815\n",
            "precision score: 0.772\n",
            "recall score: 0.895\n",
            "F1 score: 0.829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.73      0.80       200\n",
            "           1       0.77      0.90      0.83       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.940\n",
            "precision score: 0.921\n",
            "recall score: 0.963\n",
            "F1 score: 0.941\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.812\n",
            "precision score: 0.771\n",
            "recall score: 0.890\n",
            "F1 score: 0.826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.73      0.80       200\n",
            "           1       0.77      0.89      0.83       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 10=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.941\n",
            "precision score: 0.921\n",
            "recall score: 0.964\n",
            "F1 score: 0.942\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.810\n",
            "precision score: 0.767\n",
            "recall score: 0.890\n",
            "F1 score: 0.824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.73      0.79       200\n",
            "           1       0.77      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSYOmfA1x70"
      },
      "source": [
        "#### L2 Regularization\n",
        "Implementing L2 regulation\n",
        "Training on minibatch size = 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ESdBPV11xP",
        "outputId": "cfb7d54d-bd27-4481-b21f-0b7166c77973"
      },
      "source": [
        "\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "alpha = 0.01\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================iteration %s=======================' % str(iter+1))\n",
        "  weight, bias = MiniBatch_gradient_L2 (X_train, y_train, alpha = alpha,  weight=weight, bias=bias, batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================iteration 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.854\n",
            "precision score: 0.807\n",
            "recall score: 0.931\n",
            "F1 score: 0.865\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.770\n",
            "precision score: 0.709\n",
            "recall score: 0.915\n",
            "F1 score: 0.799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.62      0.73       200\n",
            "           1       0.71      0.92      0.80       200\n",
            "\n",
            "    accuracy                           0.77       400\n",
            "   macro avg       0.79      0.77      0.77       400\n",
            "weighted avg       0.79      0.77      0.77       400\n",
            "\n",
            "\n",
            "====================iteration 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.879\n",
            "precision score: 0.848\n",
            "recall score: 0.924\n",
            "F1 score: 0.885\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.790\n",
            "precision score: 0.740\n",
            "recall score: 0.895\n",
            "F1 score: 0.810\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.69      0.77       200\n",
            "           1       0.74      0.90      0.81       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.79      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.896\n",
            "precision score: 0.868\n",
            "recall score: 0.934\n",
            "F1 score: 0.899\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.792\n",
            "precision score: 0.745\n",
            "recall score: 0.890\n",
            "F1 score: 0.811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.77       200\n",
            "           1       0.74      0.89      0.81       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.79      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.907\n",
            "precision score: 0.883\n",
            "recall score: 0.940\n",
            "F1 score: 0.910\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.748\n",
            "recall score: 0.890\n",
            "F1 score: 0.813\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.70      0.77       200\n",
            "           1       0.75      0.89      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.79      0.79       400\n",
            "weighted avg       0.81      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================iteration 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.914\n",
            "precision score: 0.890\n",
            "recall score: 0.946\n",
            "F1 score: 0.917\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.800\n",
            "precision score: 0.752\n",
            "recall score: 0.895\n",
            "F1 score: 0.817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.70      0.78       200\n",
            "           1       0.75      0.90      0.82       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================iteration 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.921\n",
            "precision score: 0.895\n",
            "recall score: 0.953\n",
            "F1 score: 0.923\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.807\n",
            "precision score: 0.764\n",
            "recall score: 0.890\n",
            "F1 score: 0.822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       200\n",
            "           1       0.76      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.928\n",
            "precision score: 0.905\n",
            "recall score: 0.956\n",
            "F1 score: 0.930\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.807\n",
            "precision score: 0.759\n",
            "recall score: 0.900\n",
            "F1 score: 0.824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.71      0.79       200\n",
            "           1       0.76      0.90      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.932\n",
            "precision score: 0.910\n",
            "recall score: 0.959\n",
            "F1 score: 0.934\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.802\n",
            "precision score: 0.755\n",
            "recall score: 0.895\n",
            "F1 score: 0.819\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.71      0.78       200\n",
            "           1       0.76      0.90      0.82       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================iteration 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.937\n",
            "precision score: 0.916\n",
            "recall score: 0.963\n",
            "F1 score: 0.938\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.807\n",
            "precision score: 0.764\n",
            "recall score: 0.890\n",
            "F1 score: 0.822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       200\n",
            "           1       0.76      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================iteration 10=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.940\n",
            "precision score: 0.921\n",
            "recall score: 0.963\n",
            "F1 score: 0.941\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.810\n",
            "precision score: 0.767\n",
            "recall score: 0.890\n",
            "F1 score: 0.824\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.73      0.79       200\n",
            "           1       0.77      0.89      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1US6HbNNw4Nf"
      },
      "source": [
        "## Observing importance features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rklpxbuq8pcR",
        "outputId": "3e8da66f-a976-4f2a-fb1e-3bce19dca01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Getting weights of features that have been trained \n",
        "## store in a dictionary\n",
        "feature_weights = {}\n",
        "for i in range(len(weight)):\n",
        "  feature_weights[list(vocabulary.keys())[i]] = weight[i]\n",
        "\n",
        "# Sorting the dictionary in descending order\n",
        "sorted_feature_weights = {k:v for k, v in sorted(feature_weights.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Print the weights learned for each class\n",
        "print('50 most important features of POSITIVE class (in descending order): ')\n",
        "for k, v in list(sorted_feature_weights.items())[:50]:\n",
        "  print ('{}: {:.5f}'. format(k,v))\n",
        "\n",
        "print('\\n==============================================')\n",
        "print('50 most important features of NEGATIVE class (in descending order): ')\n",
        "for k, v in list(sorted_feature_weights.items())[:-50:-1]: \n",
        "  print ('{}: {:.5f}'. format(k,v))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 most important features of POSITIVE class (in descending order): \n",
            "hilari: 0.34366\n",
            "also: 0.31770\n",
            "enjoy: 0.31536\n",
            "sometim: 0.29092\n",
            "great: 0.28758\n",
            "job: 0.27214\n",
            "definit: 0.26284\n",
            "memor: 0.26126\n",
            "detail: 0.25713\n",
            "american: 0.24504\n",
            "town: 0.23757\n",
            "flaw: 0.23496\n",
            "thank: 0.23475\n",
            "overal: 0.23364\n",
            "perfect: 0.23317\n",
            "excel: 0.23114\n",
            "differ: 0.22875\n",
            "peopl: 0.22551\n",
            "life: 0.22498\n",
            "perfectli: 0.22478\n",
            "extrem: 0.22446\n",
            "especi: 0.22370\n",
            "quit: 0.21631\n",
            "intens: 0.21435\n",
            "equal: 0.21363\n",
            "simpl: 0.21202\n",
            "brilliant: 0.20991\n",
            "mani: 0.20946\n",
            "support: 0.20518\n",
            "although: 0.20270\n",
            "entertain: 0.20231\n",
            "view: 0.20026\n",
            "oscar: 0.19661\n",
            "fun: 0.19367\n",
            "portray: 0.19331\n",
            "follow: 0.19329\n",
            "surpris: 0.19282\n",
            "perform: 0.19209\n",
            "fiction: 0.18957\n",
            "normal: 0.18723\n",
            "best: 0.18224\n",
            "seen: 0.17853\n",
            "throughout: 0.17761\n",
            "deserv: 0.17689\n",
            "prove: 0.17525\n",
            "ben: 0.17513\n",
            "delight: 0.17475\n",
            "deliv: 0.17467\n",
            "david: 0.17462\n",
            "rais: 0.16749\n",
            "\n",
            "==============================================\n",
            "50 most important features of NEGATIVE class (in descending order): \n",
            "bad: -0.55933\n",
            "worst: -0.46728\n",
            "bore: -0.45559\n",
            "wast: -0.45004\n",
            "noth: -0.40442\n",
            "plot: -0.40055\n",
            "suppos: -0.35364\n",
            "fail: -0.34211\n",
            "ridicul: -0.34085\n",
            "stupid: -0.33583\n",
            "unfortun: -0.30590\n",
            "script: -0.30392\n",
            "aw: -0.30335\n",
            "poor: -0.29929\n",
            "potenti: -0.29373\n",
            "materi: -0.27655\n",
            "terribl: -0.27386\n",
            "tv: -0.25936\n",
            "attempt: -0.25777\n",
            "save: -0.25369\n",
            "dull: -0.24876\n",
            "writer: -0.24335\n",
            "subplot: -0.23853\n",
            "mayb: -0.23750\n",
            "better: -0.22964\n",
            "promis: -0.22588\n",
            "reason: -0.22385\n",
            "project: -0.22384\n",
            "problem: -0.22276\n",
            "none: -0.22204\n",
            "talent: -0.21963\n",
            "mess: -0.21866\n",
            "care: -0.21559\n",
            "video: -0.21346\n",
            "tri: -0.21329\n",
            "wors: -0.21230\n",
            "pain: -0.20946\n",
            "lack: -0.20197\n",
            "neither: -0.19999\n",
            "least: -0.19508\n",
            "guess: -0.19278\n",
            "could: -0.18980\n",
            "look: -0.18684\n",
            "point: -0.18354\n",
            "got: -0.18299\n",
            "cant: -0.18152\n",
            "clich: -0.17959\n",
            "appear: -0.17737\n",
            "dialogu: -0.17526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RieB4dmq65zt"
      },
      "source": [
        "# Checking Sklearn logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXiMnHRdBDL",
        "outputId": "bf732467-a31a-4ba1-ffa4-91e0346ec1ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Sklearn Logistic Regression Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "sk_lr = LogisticRegression(solver='lbfgs', max_iter=150).fit(X_train, y_train )\n",
        "y_predict = sk_lr.predict(X_test)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, sk_lr.predict(X_train))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict, report = True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.787\n",
            "precision score: 0.778\n",
            "recall score: 0.805\n",
            "F1 score: 0.791\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.78       200\n",
            "           1       0.78      0.81      0.79       200\n",
            "\n",
            "    accuracy                           0.79       400\n",
            "   macro avg       0.79      0.79      0.79       400\n",
            "weighted avg       0.79      0.79      0.79       400\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7875, 0.7777777777777778, 0.805, 0.7911547911547911)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}