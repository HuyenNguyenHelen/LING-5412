{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1I/yVWZ4fDCt/qLpH693D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment2_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIRoN8MBWFLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd1e567-1a9f-4ba3-c426-b6dde06d54ed"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import tarfile\n",
        "import glob\n",
        "import re\n",
        "import pandas as pd\n",
        "#from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqCpCGDJrZz"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krvu3FbyUwOn"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/review_polarity.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCAKXzrfWYEc",
        "outputId": "b8cdefe7-7e8e-4cd1-afc2-de814f1bf6eb"
      },
      "source": [
        "# Exploring the data sizes\n",
        "\n",
        "paths_pos = glob.glob('/content/txt_sentoken/pos/*.txt')\n",
        "paths_neg = glob.glob('/content/txt_sentoken/neg/*.txt')\n",
        "pos_neg_paths = paths_pos + paths_neg\n",
        "\n",
        "n_pos = len(paths_pos)\n",
        "n_neg = len(paths_neg)\n",
        "\n",
        "print('the number of positive instances: {} \\nthe number of positive instances: {}'.format(n_pos, n_neg))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of positive instances: 1000 \n",
            "the number of positive instances: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_HBaup_JiGU"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-1M-pGYxd2",
        "outputId": "f87a1540-536c-41ac-b23b-8ce20523968f"
      },
      "source": [
        "# Exploring the words in the dataset\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def tokenizer (doc):\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "def stopword_remover (bow):\n",
        "  filtered_bow = [w for w in bow if not w.lower() in stopwords]\n",
        "  return filtered_bow\n",
        "\n",
        "def top_freq_w (freq_dic, top_n, stopword_removing = ''):\n",
        "  sorted_dic = {k:v for k, v in sorted(freq_dic.items(), key = lambda item: item[1], reverse=True)}\n",
        "  if stopword_removing is False:\n",
        "    return {k:v for k, v in list(sorted_dic.items())[:top_n]}\n",
        "  elif stopword_removing is True:\n",
        "    filtered_dic = {k: v for k, v in sorted_dic.items() if k not in stopwords}\n",
        "    return {k:v for k, v in list(filtered_dic.items())[:top_n]}\n",
        "  \n",
        "\n",
        "word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  for token in tokenizer (doc):\n",
        "    word_freq[token] = word_freq.get(token,0)+1\n",
        "\n",
        "top_100_w = top_freq_w(word_freq, 100, stopword_removing = False) \n",
        "\n",
        "print('\\nthe number of unique words in the dataset: ', len(word_freq.keys()))\n",
        "print ('\\ntop 100 most frequent words:\\n', top_100_w )\n",
        "print('\\nthe number of words in the top 100 which are stopwords: ', len([w for w in top_100_w.keys() if w in stopwords]))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "the number of unique words in the dataset:  39697\n",
            "\n",
            "top 100 most frequent words:\n",
            " {'the': 76529, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, 'is': 25195, 'in': 21822, 's': 18513, 'it': 16107, 'that': 15924, 'as': 11378, 'with': 10792, 'for': 9961, 'his': 9587, 'this': 9578, 'film': 9517, 'i': 8889, 'he': 8864, 'but': 8634, 'on': 7385, 'are': 6949, 't': 6410, 'by': 6261, 'be': 6174, 'one': 5852, 'movie': 5771, 'an': 5744, 'who': 5692, 'not': 5577, 'you': 5316, 'from': 4999, 'at': 4986, 'was': 4940, 'have': 4901, 'they': 4825, 'has': 4719, 'her': 4522, 'all': 4373, 'there': 3770, 'like': 3690, 'so': 3683, 'out': 3637, 'about': 3523, 'up': 3405, 'more': 3347, 'what': 3322, 'when': 3258, 'which': 3161, 'or': 3148, 'she': 3141, 'their': 3122, 'some': 2985, 'just': 2905, 'can': 2882, 'if': 2799, 'we': 2775, 'him': 2633, 'into': 2623, 'even': 2565, 'only': 2495, 'than': 2474, 'no': 2472, 'good': 2411, 'time': 2411, 'most': 2306, 'its': 2270, 'will': 2216, 'story': 2169, '': 2152, 'would': 2109, 'been': 2050, 'much': 2049, 'character': 2020, 'also': 1967, 'get': 1949, 'other': 1948, 'do': 1915, 'two': 1911, 'well': 1906, 'them': 1877, 'very': 1863, 'characters': 1859, 'first': 1836, 'after': 1762, 'see': 1749, 'way': 1693, 'because': 1684, 'make': 1642, 'life': 1586, 'off': 1581, 'too': 1577, 'any': 1574, 'does': 1568, 'really': 1558, 'had': 1546, 'while': 1539, 'films': 1536, 'how': 1517, 'plot': 1513, 'little': 1501}\n",
            "\n",
            "the number of words in the top 100 which are stopwords:  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "XZlPlHizshiT",
        "outputId": "767c63fe-0308-4767-f1ca-de6194e3a7a7"
      },
      "source": [
        "# Reformating the dataset into csv for convenience \n",
        "\n",
        "def to_df (folder):\n",
        "  data_dic = {}\n",
        "  data_dic['doc'], data_dic['label'] = [], []\n",
        "  for file in folder:\n",
        "    fo = open(file)\n",
        "    doc = fo.read()\n",
        "    data_dic['doc'].append(doc)\n",
        "    if 'pos' in file:\n",
        "      data_dic['label'].append(1)\n",
        "    elif 'neg' in file:\n",
        "      data_dic['label'].append(0)\n",
        "    else:\n",
        "      print('error', file)\n",
        "  df = pd.DataFrame.from_dict(data_dic)\n",
        "  return df\n",
        "    \n",
        "data = to_df(pos_neg_paths)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredients : london gal , fate , true love , ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show , an almost perfectly accurate true ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>after a stylistic detour with mrs . \\nparker a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>all great things come to an end , and the dot-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udall is a heartless man . \\nhe spends ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 doc  label\n",
              "0  ingredients : london gal , fate , true love , ...      1\n",
              "1  quiz show , an almost perfectly accurate true ...      1\n",
              "2  after a stylistic detour with mrs . \\nparker a...      1\n",
              "3  all great things come to an end , and the dot-...      1\n",
              "4  melvin udall is a heartless man . \\nhe spends ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFEUMkgJYGz"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jGSEg3J8ktsC",
        "outputId": "f2dddee7-4719-49db-a77c-0b410687c789"
      },
      "source": [
        "# Data preprocessing\n",
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def preprocessor (text):\n",
        "  ## removing punctuations and characters\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  # stripping\n",
        "  text = ' '.join([w.strip() for w in text.split()])\n",
        "  ## lowcasing\n",
        "  text = text.lower()\n",
        "  # ## removing stopword\n",
        "  text = stopword_remover (text.split())\n",
        "  # ##stemmming\n",
        "  text = [stemmer.stem(w) for w in text]\n",
        "  # ## lematization\n",
        "  text = [lemmatizer.lemmatize(w) for w in text]\n",
        "  return ' '.join([w for w in text])\n",
        "\n",
        "data['doc'] = data['doc'].apply(lambda x:  preprocessor (x) )\n",
        "data "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ingredi london gal fate true love run joke mon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quiz show almost perfectli accur true stori ba...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stylist detour mr parker viciou circl despit u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>great thing come end dotcom era embodi perfect...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>melvin udal heartless man spend day insid spac...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>often similar littl boy lost park right ventur...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>13th warrior reek badli melodrama poor act car...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>accord hitchcock variou filmmak isol motel din...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>warn follow review contain spoiler cast gari s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>went saw film right call battlefield earth nev...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    doc  label\n",
              "0     ingredi london gal fate true love run joke mon...      1\n",
              "1     quiz show almost perfectli accur true stori ba...      1\n",
              "2     stylist detour mr parker viciou circl despit u...      1\n",
              "3     great thing come end dotcom era embodi perfect...      1\n",
              "4     melvin udal heartless man spend day insid spac...      1\n",
              "...                                                 ...    ...\n",
              "1995  often similar littl boy lost park right ventur...      0\n",
              "1996  13th warrior reek badli melodrama poor act car...      0\n",
              "1997  accord hitchcock variou filmmak isol motel din...      0\n",
              "1998  warn follow review contain spoiler cast gari s...      0\n",
              "1999  went saw film right call battlefield earth nev...      0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4FEl86wJ2Zv"
      },
      "source": [
        "# Building a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuJ_OjUQy0iY"
      },
      "source": [
        "### Preparing vocabulary/feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10p5-fLZLetp"
      },
      "source": [
        "\n",
        "## As required, we will use 1000 most frequent word, excluding stopwords\n",
        "\n",
        "cleaned_word_freq = {}\n",
        "for path in pos_neg_paths:\n",
        "  fo = open(path)\n",
        "  doc = fo.read()\n",
        "  # Cleaning documents before extracting features\n",
        "  cleaned_doc = preprocessor(doc)\n",
        "  # Getting terms and their frequency \n",
        "  for token in tokenizer (cleaned_doc):\n",
        "    cleaned_word_freq[token] = cleaned_word_freq.get(token,0)+1\n",
        "\n",
        "# Getting 1000 terms with highest frequency, excluding stopwords\n",
        "vocabulary = top_freq_w(cleaned_word_freq, 1000, stopword_removing = True) \n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW4AHP3v0UPg"
      },
      "source": [
        "### Representing documents based on extracted features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPM073ASYl5i"
      },
      "source": [
        "# binary representing documents based on occurrance of features in documents\n",
        "def doc_representor (doc):\n",
        "  doc_vec = []\n",
        "  token_list = tokenizer (doc)\n",
        "  for feature in vocabulary.keys():\n",
        "    if feature in token_list:\n",
        "      doc_vec.append(1)\n",
        "    else:\n",
        "       doc_vec.append(0)\n",
        "  return doc_vec\n",
        "\n",
        "X = data['doc'].apply(lambda x: doc_representor(x))\n",
        "y = data['label']"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qW3hQL2GFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "57aae10c-5e03-4b71-c3ce-9e8f22e178ef"
      },
      "source": [
        "# Visualize the data after representing\n",
        "X = X.apply(pd.Series)\n",
        "X.columns = vocabulary.keys()\n",
        "print(X.shape)\n",
        "X"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>film</th>\n",
              "      <th>movi</th>\n",
              "      <th>one</th>\n",
              "      <th>like</th>\n",
              "      <th>charact</th>\n",
              "      <th>get</th>\n",
              "      <th>make</th>\n",
              "      <th>time</th>\n",
              "      <th>scene</th>\n",
              "      <th>even</th>\n",
              "      <th>good</th>\n",
              "      <th>play</th>\n",
              "      <th>stori</th>\n",
              "      <th>see</th>\n",
              "      <th>would</th>\n",
              "      <th>much</th>\n",
              "      <th>also</th>\n",
              "      <th>go</th>\n",
              "      <th>way</th>\n",
              "      <th>seem</th>\n",
              "      <th>look</th>\n",
              "      <th>end</th>\n",
              "      <th>two</th>\n",
              "      <th>take</th>\n",
              "      <th>first</th>\n",
              "      <th>come</th>\n",
              "      <th>well</th>\n",
              "      <th>work</th>\n",
              "      <th>thing</th>\n",
              "      <th>year</th>\n",
              "      <th>realli</th>\n",
              "      <th>plot</th>\n",
              "      <th>know</th>\n",
              "      <th>perform</th>\n",
              "      <th>littl</th>\n",
              "      <th>life</th>\n",
              "      <th>peopl</th>\n",
              "      <th>love</th>\n",
              "      <th>could</th>\n",
              "      <th>bad</th>\n",
              "      <th>...</th>\n",
              "      <th>rush</th>\n",
              "      <th>realist</th>\n",
              "      <th>scare</th>\n",
              "      <th>manner</th>\n",
              "      <th>command</th>\n",
              "      <th>standard</th>\n",
              "      <th>menac</th>\n",
              "      <th>spent</th>\n",
              "      <th>adam</th>\n",
              "      <th>agre</th>\n",
              "      <th>cinematographi</th>\n",
              "      <th>front</th>\n",
              "      <th>ground</th>\n",
              "      <th>budget</th>\n",
              "      <th>fairli</th>\n",
              "      <th>pair</th>\n",
              "      <th>virtual</th>\n",
              "      <th>suddenli</th>\n",
              "      <th>fantasi</th>\n",
              "      <th>connect</th>\n",
              "      <th>disturb</th>\n",
              "      <th>90</th>\n",
              "      <th>appropri</th>\n",
              "      <th>godzilla</th>\n",
              "      <th>brown</th>\n",
              "      <th>grant</th>\n",
              "      <th>cultur</th>\n",
              "      <th>greatest</th>\n",
              "      <th>store</th>\n",
              "      <th>trip</th>\n",
              "      <th>key</th>\n",
              "      <th>fascin</th>\n",
              "      <th>cute</th>\n",
              "      <th>brief</th>\n",
              "      <th>cameo</th>\n",
              "      <th>count</th>\n",
              "      <th>foot</th>\n",
              "      <th>addit</th>\n",
              "      <th>satir</th>\n",
              "      <th>bug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      film  movi  one  like  charact  ...  count  foot  addit  satir  bug\n",
              "0        1     0    1     1        1  ...      0     0      0      0    0\n",
              "1        1     1    1     0        0  ...      0     0      0      0    0\n",
              "2        1     1    1     1        1  ...      0     0      0      0    0\n",
              "3        1     0    0     0        0  ...      0     0      0      0    0\n",
              "4        1     0    1     0        0  ...      0     0      0      0    0\n",
              "...    ...   ...  ...   ...      ...  ...    ...   ...    ...    ...  ...\n",
              "1995     1     1    1     0        1  ...      0     0      1      0    0\n",
              "1996     1     1    1     1        1  ...      0     0      0      0    0\n",
              "1997     1     0    1     0        0  ...      0     0      0      0    0\n",
              "1998     1     1    1     1        1  ...      0     1      0      0    0\n",
              "1999     1     1    1     1        0  ...      0     0      0      0    0\n",
              "\n",
              "[2000 rows x 1000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA7yjtdsJ_ur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27cb966a-459e-4ee8-deac-222b58d8882e"
      },
      "source": [
        "# Spliting the dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split (X[vocabulary.keys()], y , train_size = 0.8, random_state = 42, shuffle = True, stratify=data['label'])\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (1600, 1000) (1600,)\n",
            "Shapes of X_test, y_test:  (400, 1000) (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTzyxg0x1m3y"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSfRLnHvKT7"
      },
      "source": [
        "# Writing functions\n",
        "\n",
        "def sigmoid (z):\n",
        "  p=1/(1+np.exp(-z))\n",
        "  return p\n",
        "\n",
        "\n",
        "def predict(X, weight, bias):\n",
        "    z = np.dot(weight, X.values.T) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    y_class = [1 if i >= 0.5 else 0 for i in y_pred]\n",
        "    return y_class\n",
        "\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  print('accuracy score: {:.3f}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "  print('precision score: {:.3f}'.format(sklearn.metrics.precision_score(y_true, y_pred)))\n",
        "  print('recall score: {:.3f}'.format(sklearn.metrics.recall_score(y_true, y_pred)))\n",
        "  print('F1 score: {:.3f}'.format(sklearn.metrics.f1_score(y_true, y_pred)))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "def computing_gradient (X, Y, weight, bias):\n",
        "  for x,y in zip (X.values, Y):\n",
        "    z = np.dot(weight, x) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "    d_weight = np.dot((y_pred - y), x)\n",
        "    d_bias = (y_pred - y)\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "  return weight, bias\n",
        "\n",
        "def computing_MiniBatch_gradient (X, Y, weight, bias, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i< round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      #loss = -(y*log(y_pred)+(1-y)*log(1-y_pred))\n",
        "      sum_w += np.dot((y_pred - y), x)\n",
        "      sum_b += (y_pred - y)\n",
        "    d_weight = (1/batch_size)*sum_w\n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n",
        "\n",
        "def MiniBatch_gradient_L2 (X, Y, weight, bias,alpha, batch_size):\n",
        "  n_instances, n_features = X.shape\n",
        "  i=0 \n",
        "  while i< round(n_instances/batch_size):\n",
        "    m = batch_size * i\n",
        "    n = m + batch_size\n",
        "    sum_w = 0\n",
        "    sum_b = 0\n",
        "    for x,y in zip (X[m:n].values,Y[m:n]):\n",
        "      z = np.dot(weight, x) + bias\n",
        "      y_pred = sigmoid(z)\n",
        "      w_i = np.dot((y_pred - y), x)\n",
        "      b_i = y_pred - y\n",
        "      sum_w +=  w_i  + (alpha * weight) # L2\n",
        "      sum_b += b_i\n",
        "    d_weight = (1/batch_size)*sum_w \n",
        "    d_bias = (1/batch_size)*sum_b\n",
        "    weight -= lr*d_weight\n",
        "    bias -= lr*d_bias\n",
        "    i+=1\n",
        "  return  weight, bias\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWScj3p-39kj"
      },
      "source": [
        "#### Initial LR model\n",
        "updating weight and bias with Gradient descent every instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8h5nbJd37UT",
        "outputId": "b57cc3f7-2994-4544-864e-e8d76d3e0fd7"
      },
      "source": [
        "# Training the model, updating weight and bias every instances\n",
        "\n",
        "print('Training LR.......................................................')\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  weight, bias = computing_gradient (X_train, y_train, weight, bias)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LR.......................................................\n",
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.877\n",
            "precision score: 0.943\n",
            "recall score: 0.804\n",
            "F1 score: 0.868\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.795\n",
            "precision score: 0.855\n",
            "recall score: 0.710\n",
            "F1 score: 0.776\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.88      0.81       200\n",
            "           1       0.86      0.71      0.78       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.79      0.79       400\n",
            "weighted avg       0.80      0.80      0.79       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.920\n",
            "precision score: 0.965\n",
            "recall score: 0.871\n",
            "F1 score: 0.916\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.800\n",
            "precision score: 0.845\n",
            "recall score: 0.735\n",
            "F1 score: 0.786\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81       200\n",
            "           1       0.84      0.73      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.926\n",
            "precision score: 0.971\n",
            "recall score: 0.877\n",
            "F1 score: 0.922\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.805\n",
            "precision score: 0.839\n",
            "recall score: 0.755\n",
            "F1 score: 0.795\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.85      0.81       200\n",
            "           1       0.84      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.81      0.80       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.943\n",
            "precision score: 0.985\n",
            "recall score: 0.900\n",
            "F1 score: 0.941\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.802\n",
            "precision score: 0.834\n",
            "recall score: 0.755\n",
            "F1 score: 0.793\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.85      0.81       200\n",
            "           1       0.83      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.81      0.80      0.80       400\n",
            "weighted avg       0.81      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.961\n",
            "precision score: 0.993\n",
            "recall score: 0.927\n",
            "F1 score: 0.959\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.797\n",
            "precision score: 0.825\n",
            "recall score: 0.755\n",
            "F1 score: 0.789\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.84      0.81       200\n",
            "           1       0.83      0.76      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.984\n",
            "precision score: 0.995\n",
            "recall score: 0.973\n",
            "F1 score: 0.984\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.807\n",
            "precision score: 0.815\n",
            "recall score: 0.795\n",
            "F1 score: 0.805\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       200\n",
            "           1       0.82      0.80      0.81       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.982\n",
            "precision score: 1.000\n",
            "recall score: 0.964\n",
            "F1 score: 0.982\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.797\n",
            "precision score: 0.812\n",
            "recall score: 0.775\n",
            "F1 score: 0.793\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       200\n",
            "           1       0.81      0.78      0.79       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.998\n",
            "precision score: 1.000\n",
            "recall score: 0.995\n",
            "F1 score: 0.997\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.802\n",
            "precision score: 0.807\n",
            "recall score: 0.795\n",
            "F1 score: 0.801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80       200\n",
            "           1       0.81      0.80      0.80       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.815\n",
            "precision score: 0.809\n",
            "recall score: 0.825\n",
            "F1 score: 0.817\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       200\n",
            "           1       0.81      0.82      0.82       200\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.82      0.81      0.81       400\n",
            "weighted avg       0.82      0.81      0.81       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.999\n",
            "precision score: 0.998\n",
            "recall score: 1.000\n",
            "F1 score: 0.999\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.802\n",
            "precision score: 0.784\n",
            "recall score: 0.835\n",
            "F1 score: 0.809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.77      0.80       200\n",
            "           1       0.78      0.83      0.81       200\n",
            "\n",
            "    accuracy                           0.80       400\n",
            "   macro avg       0.80      0.80      0.80       400\n",
            "weighted avg       0.80      0.80      0.80       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wviTYBaXOCxF"
      },
      "source": [
        "#### Minibatch training\n",
        "updating weight and bias with Gradient descent every batch-size = 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0rgm4ULuTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6750457-4db9-43e3-c3f3-7f1a76b3d9e4"
      },
      "source": [
        "lr = 0.1\n",
        "n_iter = 10\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  weight, bias = computing_MiniBatch_gradient (X_train, y_train, weight, bias,  batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "  \n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.838\n",
            "precision score: 0.783\n",
            "recall score: 0.936\n",
            "F1 score: 0.853\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.788\n",
            "recall score: 0.930\n",
            "F1 score: 0.853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.75      0.82       200\n",
            "           1       0.79      0.93      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.873\n",
            "precision score: 0.831\n",
            "recall score: 0.936\n",
            "F1 score: 0.881\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.845\n",
            "precision score: 0.811\n",
            "recall score: 0.900\n",
            "F1 score: 0.853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.79      0.84       200\n",
            "           1       0.81      0.90      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.887\n",
            "precision score: 0.851\n",
            "recall score: 0.939\n",
            "F1 score: 0.893\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.838\n",
            "precision score: 0.808\n",
            "recall score: 0.885\n",
            "F1 score: 0.845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.83       200\n",
            "           1       0.81      0.89      0.84       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.895\n",
            "precision score: 0.864\n",
            "recall score: 0.938\n",
            "F1 score: 0.899\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.833\n",
            "precision score: 0.804\n",
            "recall score: 0.880\n",
            "F1 score: 0.840\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.82       200\n",
            "           1       0.80      0.88      0.84       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.84      0.83      0.83       400\n",
            "weighted avg       0.84      0.83      0.83       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.905\n",
            "precision score: 0.875\n",
            "recall score: 0.945\n",
            "F1 score: 0.909\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.843\n",
            "precision score: 0.816\n",
            "recall score: 0.885\n",
            "F1 score: 0.849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.914\n",
            "precision score: 0.887\n",
            "recall score: 0.950\n",
            "F1 score: 0.917\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.848\n",
            "precision score: 0.823\n",
            "recall score: 0.885\n",
            "F1 score: 0.853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.921\n",
            "precision score: 0.895\n",
            "recall score: 0.953\n",
            "F1 score: 0.923\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.848\n",
            "precision score: 0.826\n",
            "recall score: 0.880\n",
            "F1 score: 0.852\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.83      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.926\n",
            "precision score: 0.902\n",
            "recall score: 0.956\n",
            "F1 score: 0.928\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.850\n",
            "precision score: 0.833\n",
            "recall score: 0.875\n",
            "F1 score: 0.854\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.82      0.85       200\n",
            "           1       0.83      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.932\n",
            "precision score: 0.909\n",
            "recall score: 0.961\n",
            "F1 score: 0.934\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.845\n",
            "precision score: 0.832\n",
            "recall score: 0.865\n",
            "F1 score: 0.848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       200\n",
            "           1       0.83      0.86      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.938\n",
            "precision score: 0.916\n",
            "recall score: 0.964\n",
            "F1 score: 0.939\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.848\n",
            "precision score: 0.836\n",
            "recall score: 0.865\n",
            "F1 score: 0.850\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       200\n",
            "           1       0.84      0.86      0.85       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSYOmfA1x70"
      },
      "source": [
        "#### L2 Regularization\n",
        "Implementing L2 regulation\n",
        "Training on minibatch size = 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6ESdBPV11xP",
        "outputId": "0da1a17e-3d98-47f9-fe5c-71a6f0f9b453"
      },
      "source": [
        "\n",
        "lr = 0.1\n",
        "n_iter = 10\n",
        "alpha = 0.01\n",
        "weight = np.zeros(X.shape[1])\n",
        "bias = 0\n",
        "for iter in range(n_iter):\n",
        "  print('\\n====================batch %s=======================' % iter)\n",
        "  weight, bias = MiniBatch_gradient_L2 (X_train, y_train, alpha = alpha,  weight=weight, bias=bias, batch_size = 32)\n",
        "  y_predict = predict(X_test,weight, bias)\n",
        "\n",
        "  # Model performing\n",
        "  ## on training set\n",
        "  print('Model performance on training set:')\n",
        "  printing_eval_scores (y_train, predict(X_train,weight, bias), report = False)\n",
        "\n",
        "  ## on test set\n",
        "  print('\\nModel performance on test set:')\n",
        "  printing_eval_scores (y_test, y_predict, report = True)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================batch 0=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.837\n",
            "precision score: 0.781\n",
            "recall score: 0.936\n",
            "F1 score: 0.852\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.788\n",
            "recall score: 0.930\n",
            "F1 score: 0.853\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.75      0.82       200\n",
            "           1       0.79      0.93      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 1=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.872\n",
            "precision score: 0.829\n",
            "recall score: 0.936\n",
            "F1 score: 0.880\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.848\n",
            "precision score: 0.812\n",
            "recall score: 0.905\n",
            "F1 score: 0.856\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.79      0.84       200\n",
            "           1       0.81      0.91      0.86       200\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n",
            "\n",
            "====================batch 2=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.887\n",
            "precision score: 0.850\n",
            "recall score: 0.940\n",
            "F1 score: 0.893\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.809\n",
            "recall score: 0.890\n",
            "F1 score: 0.848\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.79      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 3=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.894\n",
            "precision score: 0.861\n",
            "recall score: 0.939\n",
            "F1 score: 0.898\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.838\n",
            "precision score: 0.808\n",
            "recall score: 0.885\n",
            "F1 score: 0.845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.79      0.83       200\n",
            "           1       0.81      0.89      0.84       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 4=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.902\n",
            "precision score: 0.872\n",
            "recall score: 0.943\n",
            "F1 score: 0.906\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.812\n",
            "recall score: 0.885\n",
            "F1 score: 0.847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 5=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.911\n",
            "precision score: 0.880\n",
            "recall score: 0.953\n",
            "F1 score: 0.915\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.812\n",
            "recall score: 0.885\n",
            "F1 score: 0.847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83       200\n",
            "           1       0.81      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 6=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.916\n",
            "precision score: 0.887\n",
            "recall score: 0.954\n",
            "F1 score: 0.919\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.843\n",
            "precision score: 0.816\n",
            "recall score: 0.885\n",
            "F1 score: 0.849\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.84       200\n",
            "           1       0.82      0.89      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 7=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.921\n",
            "precision score: 0.893\n",
            "recall score: 0.956\n",
            "F1 score: 0.923\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.840\n",
            "precision score: 0.818\n",
            "recall score: 0.875\n",
            "F1 score: 0.845\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.83       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 8=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.925\n",
            "precision score: 0.898\n",
            "recall score: 0.959\n",
            "F1 score: 0.927\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.843\n",
            "precision score: 0.822\n",
            "recall score: 0.875\n",
            "F1 score: 0.847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "\n",
            "====================batch 9=======================\n",
            "Model performance on training set:\n",
            "accuracy score: 0.930\n",
            "precision score: 0.906\n",
            "recall score: 0.960\n",
            "F1 score: 0.932\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.843\n",
            "precision score: 0.822\n",
            "recall score: 0.875\n",
            "F1 score: 0.847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       200\n",
            "           1       0.82      0.88      0.85       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1US6HbNNw4Nf"
      },
      "source": [
        "## Observing importance features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rklpxbuq8pcR",
        "outputId": "18a28ac8-19cb-42c8-a492-fd8a626dd998"
      },
      "source": [
        "# Getting weights of features that have been trained \n",
        "## store in a dictionary\n",
        "feature_weights = {}\n",
        "for i in range(len(weight)):\n",
        "  feature_weights[list(vocabulary.keys())[i]] = weight[i]\n",
        "\n",
        "# Sorting the dictionary in descending order\n",
        "sorted_feature_weights = {k:v for k, v in sorted(feature_weights.items(), key = lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Print the weights learned for each class\n",
        "print('100 most important features of POSITIVE class (in descending order): ')\n",
        "for k, v in list(sorted_feature_weights.items())[:100]:\n",
        "  print ('{}: {:.5f}'. format(k,v))\n",
        "\n",
        "print('\\n==============================================')\n",
        "print('100 most important features of NEGATIVE class (in descending order): ')\n",
        "for k, v in list(sorted_feature_weights.items())[:-100:-1]: \n",
        "  print ('{}: {:.5f}'. format(k,v))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 most important features of POSITIVE class (in descending order): \n",
            "sometim: 0.34931\n",
            "great: 0.31821\n",
            "hilari: 0.30782\n",
            "memor: 0.29161\n",
            "perform: 0.28182\n",
            "excel: 0.27267\n",
            "also: 0.27129\n",
            "flaw: 0.26696\n",
            "perfect: 0.26632\n",
            "job: 0.25816\n",
            "overal: 0.25428\n",
            "definit: 0.24694\n",
            "equal: 0.23384\n",
            "fun: 0.23307\n",
            "especi: 0.23273\n",
            "enjoy: 0.22720\n",
            "thank: 0.22712\n",
            "seen: 0.22637\n",
            "american: 0.22533\n",
            "portray: 0.21947\n",
            "life: 0.21636\n",
            "perfectli: 0.21623\n",
            "true: 0.21604\n",
            "day: 0.21391\n",
            "ive: 0.21098\n",
            "deserv: 0.20709\n",
            "best: 0.20512\n",
            "side: 0.20154\n",
            "fiction: 0.19849\n",
            "brilliant: 0.19662\n",
            "differ: 0.19592\n",
            "well: 0.19140\n",
            "share: 0.19038\n",
            "solid: 0.19022\n",
            "le: 0.18659\n",
            "today: 0.18512\n",
            "normal: 0.18434\n",
            "visual: 0.18277\n",
            "yet: 0.18276\n",
            "david: 0.18025\n",
            "period: 0.17538\n",
            "entertain: 0.17533\n",
            "support: 0.17482\n",
            "realist: 0.17474\n",
            "throughout: 0.17419\n",
            "surpris: 0.17289\n",
            "oscar: 0.17175\n",
            "togeth: 0.17168\n",
            "delight: 0.17002\n",
            "world: 0.16947\n",
            "nice: 0.16465\n",
            "see: 0.16404\n",
            "take: 0.16401\n",
            "wonder: 0.16089\n",
            "sever: 0.16068\n",
            "histori: 0.15870\n",
            "follow: 0.15858\n",
            "peopl: 0.15667\n",
            "extrem: 0.15640\n",
            "may: 0.15552\n",
            "allow: 0.15549\n",
            "summer: 0.15441\n",
            "although: 0.15419\n",
            "truli: 0.15369\n",
            "amaz: 0.15136\n",
            "surprisingli: 0.14954\n",
            "year: 0.14938\n",
            "similar: 0.14914\n",
            "carri: 0.14896\n",
            "detail: 0.14830\n",
            "mother: 0.14819\n",
            "mind: 0.14633\n",
            "back: 0.14606\n",
            "ben: 0.14348\n",
            "product: 0.14126\n",
            "four: 0.14088\n",
            "pace: 0.14043\n",
            "artist: 0.14008\n",
            "view: 0.13990\n",
            "due: 0.13979\n",
            "ride: 0.13976\n",
            "natur: 0.13869\n",
            "bring: 0.13795\n",
            "hit: 0.13602\n",
            "help: 0.13586\n",
            "sure: 0.13475\n",
            "eventu: 0.13446\n",
            "bill: 0.13359\n",
            "without: 0.13345\n",
            "touch: 0.13281\n",
            "chang: 0.13267\n",
            "mani: 0.13228\n",
            "attent: 0.13197\n",
            "kind: 0.13175\n",
            "class: 0.13157\n",
            "simpl: 0.13056\n",
            "mix: 0.12997\n",
            "rememb: 0.12963\n",
            "realiti: 0.12918\n",
            "set: 0.12894\n",
            "\n",
            "==============================================\n",
            "100 most important features of NEGATIVE class (in descending order): \n",
            "bad: -0.50625\n",
            "worst: -0.49663\n",
            "bore: -0.47808\n",
            "wast: -0.46933\n",
            "unfortun: -0.38438\n",
            "suppos: -0.35352\n",
            "stupid: -0.33748\n",
            "noth: -0.31819\n",
            "problem: -0.31377\n",
            "plot: -0.29756\n",
            "ridicul: -0.29021\n",
            "mess: -0.28716\n",
            "aw: -0.28369\n",
            "fail: -0.28070\n",
            "attempt: -0.28044\n",
            "terribl: -0.27811\n",
            "mayb: -0.27451\n",
            "script: -0.26975\n",
            "potenti: -0.26503\n",
            "reason: -0.25034\n",
            "fall: -0.24032\n",
            "tri: -0.24031\n",
            "poor: -0.23921\n",
            "clich: -0.23851\n",
            "wors: -0.23334\n",
            "dull: -0.23241\n",
            "none: -0.23203\n",
            "materi: -0.23194\n",
            "promis: -0.22848\n",
            "writer: -0.22781\n",
            "video: -0.22411\n",
            "lack: -0.22122\n",
            "neither: -0.20846\n",
            "idea: -0.20729\n",
            "could: -0.20672\n",
            "anyway: -0.20129\n",
            "subplot: -0.19920\n",
            "guess: -0.19812\n",
            "joke: -0.19688\n",
            "talent: -0.19256\n",
            "would: -0.19248\n",
            "interest: -0.18377\n",
            "look: -0.18344\n",
            "paul: -0.18328\n",
            "pain: -0.18012\n",
            "middl: -0.17876\n",
            "appar: -0.17505\n",
            "director: -0.17449\n",
            "save: -0.17213\n",
            "point: -0.17068\n",
            "better: -0.16922\n",
            "throw: -0.16842\n",
            "el: -0.16737\n",
            "moment: -0.16405\n",
            "catch: -0.16183\n",
            "project: -0.16029\n",
            "given: -0.15722\n",
            "act: -0.15345\n",
            "isnt: -0.15301\n",
            "action: -0.15089\n",
            "tv: -0.14894\n",
            "play: -0.14780\n",
            "couldnt: -0.14677\n",
            "wasnt: -0.14487\n",
            "sinc: -0.14440\n",
            "abil: -0.14028\n",
            "trailer: -0.13960\n",
            "cant: -0.13854\n",
            "complet: -0.13705\n",
            "pas: -0.13404\n",
            "wait: -0.13300\n",
            "least: -0.13237\n",
            "career: -0.13158\n",
            "big: -0.13100\n",
            "pretti: -0.12987\n",
            "appear: -0.12905\n",
            "like: -0.12747\n",
            "quickli: -0.12696\n",
            "giant: -0.12690\n",
            "decent: -0.12506\n",
            "filmmak: -0.12457\n",
            "west: -0.12341\n",
            "bare: -0.12254\n",
            "femal: -0.12191\n",
            "basic: -0.12176\n",
            "silli: -0.12128\n",
            "simpli: -0.12081\n",
            "write: -0.12073\n",
            "head: -0.11958\n",
            "involv: -0.11522\n",
            "might: -0.11517\n",
            "formula: -0.11507\n",
            "serious: -0.11506\n",
            "made: -0.11382\n",
            "earli: -0.11309\n",
            "1: -0.11308\n",
            "happen: -0.11279\n",
            "annoy: -0.11247\n",
            "10: -0.11205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RieB4dmq65zt"
      },
      "source": [
        "# Checking Sklearn logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXiMnHRdBDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c20d7e-3aa5-4355-9478-f49afb003de8"
      },
      "source": [
        "# Sklearn Logistic Regression Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "sk_lr = LogisticRegression(solver='lbfgs', max_iter=150).fit(X_train, y_train )\n",
        "y_predict = sk_lr.predict(X_test)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, sk_lr.predict(X_train))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('Model performance on test set:')\n",
        "printing_eval_scores (y_test, y_predict, report = True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 1.000\n",
            "precision score: 1.000\n",
            "recall score: 1.000\n",
            "F1 score: 1.000\n",
            "\n",
            "===========================\n",
            "Model performance on test set:\n",
            "accuracy score: 0.828\n",
            "precision score: 0.829\n",
            "recall score: 0.825\n",
            "F1 score: 0.827\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       200\n",
            "           1       0.83      0.82      0.83       200\n",
            "\n",
            "    accuracy                           0.83       400\n",
            "   macro avg       0.83      0.83      0.83       400\n",
            "weighted avg       0.83      0.83      0.83       400\n",
            "\n"
          ]
        }
      ]
    }
  ]
}