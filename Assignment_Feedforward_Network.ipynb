{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Feedforward-Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7nJoZ/gp2okzqxdPvJmHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment_Feedforward_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybsamp8T__sG",
        "outputId": "39adc0f5-bf50-4cd1-89dc-af059d066b19"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import string\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEecDn_cAPUR"
      },
      "source": [
        "# Part 1: IMDB sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I37czSdvAJGh"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOTrU1EcAaJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c9a3e8-d818-4e73-d2d6-48d7c2519146"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "data = tf.keras.utils.get_file ('aclImdb_v1',\n",
        "                                url,\n",
        "                                untar = True,\n",
        "                                cache_dir = '.',\n",
        "                                cache_subdir = '')\n",
        "data_dir = os.path.join (os.path.dirname(data), 'aclImdb')\n",
        "print(os.listdir(data_dir))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['imdb.vocab', 'test', 'README', 'imdbEr.txt', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl4s0XGxECq4",
        "outputId": "28c25ee8-0c4a-4207-c129-9b3987e81eb9"
      },
      "source": [
        "train_dir = os.path.join (data_dir, 'train')\n",
        "print(os.listdir(train_dir))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'urls_neg.txt', 'urls_pos.txt', 'unsup', 'pos', 'urls_unsup.txt', 'unsupBow.feat', 'labeledBow.feat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5fHSce2Rk-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18e351b-403c-44bc-9fbe-398ae54e0ec1"
      },
      "source": [
        "# Loading data from the directory\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "raw_train = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                        batch_size =batch_size,\n",
        "                                                        validation_split = 0.2,\n",
        "                                                        subset = 'training',\n",
        "                                                        seed = seed)\n",
        "raw_val = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                      batch_size = batch_size,\n",
        "                                                      validation_split = 0.2,\n",
        "                                                      subset = 'validation',\n",
        "                                                      seed = seed)\n",
        "raw_test = tf.keras.utils.text_dataset_from_directory ('aclImdb/test',\n",
        "                                                       batch_size = batch_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuL_ipXkaAyv"
      },
      "source": [
        "# Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KVgg_ZWaFtU",
        "outputId": "2c8490bc-3153-4048-9f5d-d1cba5a97fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def custom_preprocessing (text):\n",
        "  lowercase = tf.strings.lower (text)\n",
        "  stripped_html = tf.strings.regex_replace (lowercase,'<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), \n",
        "                                  '')\n",
        "  \n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(standardize = custom_preprocessing,\n",
        "                                           max_tokens = max_features,\n",
        "                                           output_mode = 'int',\n",
        "                                           output_sequence_length = sequence_length)\n",
        "# Extracting features for vectorizing using training set\n",
        "train_text = raw_train.map (lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "# Defining a function for fitting vectorizer function/layer to vectorize text (review)\n",
        "def fitting_vectorizer (text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer (text), label\n",
        "\n",
        "# storing text batch and label batch\n",
        "text_batch, label_batch = next(iter(raw_train))\n",
        "\n",
        "## print an instance with vectorized review and label for observing\n",
        "print ('REVIEW:', text_batch[0])\n",
        "print('LABEL:', raw_train.class_names[label_batch[0]] )\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REVIEW: tf.Tensor(b\"I bought this thinking I was gonna see Olivier Gruner kick some butt for 90 minutes but no the reasonably impressive cast including Michael Madson, Michael Ironside and Martin Kove are all wasted in mere cameos as they make way for the introduction of new action star Dan Anderson. The film also known in another name as Last line of defence 2 an apparant sequal to another film Gruner was in as the lead as a different character. What is the point? the first which I have not seen is apparantly about an alien or something, this is completley different featuring a story about a disgraced army guy (can't remeber who he worked for) who needs to rasie 5 million for a special new cancer treatment that will save his son and other children. Michael Ironside plays a wealthy business man who is of course a dirty scoundral who is swimming in dirty ill-gotten money. He laughs at Dan Andersons character and refuses to give it. Dan decides to steal it from him with the help of some ex colleagues from the special forces and his new workmates in his office job. This film looks painfully cheap throughout, no explosions barely any action of which is wafer thin, with little special effects such as squibs and bullet holes neccassary to add depth to any action scene and fight choreography is uninspired as is the fact that there seems to be not one good stunt. All action flicks from the bid busdgets to home movies should have some sort of dangerous stunt in it. Dan Anderson is bad, I doubt he will evr work again, he can't act, I see no sign of any fighting ability, he is nothing more than a beefy bloke who has been scraped from the bottom of the action barrell. So many people could have been cast here, you look at the amount of actionb stars there are out there, even the like of Don the Dragon and Gary Daniels, Billy Blanks can act better than this guy plus they are all martial artists. The best acting in this probably comes from Gruner which is saying something, Madson and Ironside are wasted but are merely coasting through happy to pick up there cheques for minimal effort. The possibilities the plot had are wasted, the script is poor and the direction flat, there is nothing to hold your attention, certainly not the painfully wooden Anderson who is either an Ex-American football player or a stunt man turned actor. He's bid but in no way looks bad, he just doesn't have an action man look of toughness he can give out,Gruner has this and it makes you think he would have been so much better in the lead. Why in all honesty they made this movie is beyond me it is so uninspired. I have been tricked. The cover of the DVD I rented has Gruner, Madsen and Ironside on the front so I naturally thought this was an Olivier flick, by this fact I wasn't expecting too much but I was shocked at the appalling use of mis-advertising. 2/10 (the two is for idea, but there are no marks for the execution)\", shape=(), dtype=string)\n",
            "LABEL: unsup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdhUEflLG1-B",
        "outputId": "f7e1f238-6a3c-4255-ef73-5f44e85873c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# print an example of vectorized data\n",
        "print ('Vocabulary size: ', len(vectorize_layer.get_vocabulary()))\n",
        "for i in range (90, 100):\n",
        "  print ('{} ------> {}'.format(i, vectorize_layer.get_vocabulary()[i]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size:  10000\n",
            "90 ------> them\n",
            "91 ------> films\n",
            "92 ------> movies\n",
            "93 ------> then\n",
            "94 ------> make\n",
            "95 ------> made\n",
            "96 ------> way\n",
            "97 ------> could\n",
            "98 ------> too\n",
            "99 ------> after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49aXKTf-LYpE"
      },
      "source": [
        "train = raw_train.map(fitting_vectorizer)\n",
        "val = raw_val.map(fitting_vectorizer)\n",
        "test = raw_test.map(fitting_vectorizer)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqxSoCqORfY"
      },
      "source": [
        "# Configure the dataset for performance\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train = train.cache().prefetch (buffer_size = autotune)\n",
        "val = val.cache().prefetch (buffer_size = autotune)\n",
        "test = test.cache().prefetch (buffer_size = autotune)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoADSjBJMHL7"
      },
      "source": [
        "# Building a neural network classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnp8gN87MPaF",
        "outputId": "d5a90663-018e-42c8-8bfd-73b7b8b1d471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating the model\n",
        "embedding_dim = 16 \n",
        "model = tf.keras.Sequential([layers.Embedding(max_features + 1, embedding_dim),\n",
        "                             layers.Dropout(0.2),\n",
        "                             layers.GlobalAveragePooling1D(),\n",
        "                             layers.Dropout(0.2),\n",
        "                             layers.Dense(1)])\n",
        "print(model.summary())\n",
        "\n",
        "# configure the model uisng optimizer and loss function\n",
        "model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "              optimizer = 'adam',\n",
        "              metrics = tf.metrics.BinaryAccuracy(threshold = 0.0 )) ## Why threshold = 0.0??"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfPGwtO6d-iv",
        "outputId": "f4719970-2d8a-4dc6-b2e7-a4ddff1ee056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# training the model\n",
        "epochs = 10\n",
        "history = model.fit(train,\n",
        "                    validation_data = val,\n",
        "                    epochs = epochs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: -15.4567 - binary_accuracy: 0.1663 - val_loss: -43.8020 - val_binary_accuracy: 0.1681\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: -97.1702 - binary_accuracy: 0.1663 - val_loss: -153.9084 - val_binary_accuracy: 0.1681\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: -235.8746 - binary_accuracy: 0.1663 - val_loss: -312.2938 - val_binary_accuracy: 0.1681\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -421.1499 - binary_accuracy: 0.1663 - val_loss: -514.2200 - val_binary_accuracy: 0.1681\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -650.7604 - binary_accuracy: 0.1663 - val_loss: -758.3814 - val_binary_accuracy: 0.1681\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -922.1535 - binary_accuracy: 0.1663 - val_loss: -1043.9900 - val_binary_accuracy: 0.1681\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -1238.1796 - binary_accuracy: 0.1663 - val_loss: -1370.6998 - val_binary_accuracy: 0.1681\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -1594.6876 - binary_accuracy: 0.1663 - val_loss: -1738.1771 - val_binary_accuracy: 0.1681\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: -1995.7272 - binary_accuracy: 0.1663 - val_loss: -2146.2083 - val_binary_accuracy: 0.1681\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: -2435.0442 - binary_accuracy: 0.1663 - val_loss: -2594.2490 - val_binary_accuracy: 0.1681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5rw8yReoYN",
        "outputId": "d3d7da0c-56fc-42b6-f379-84821b7497da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# testing the model\n",
        "loss, accuracy = model.evaluate(test)\n",
        "print('Testing performance:\\n Loss: {} - Accuracy: {}'. format(loss, accuracy))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 8s 10ms/step - loss: 2658.4441 - binary_accuracy: 0.5000\n",
            "Testing performance:\n",
            " Loss: 2658.444091796875 - Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}