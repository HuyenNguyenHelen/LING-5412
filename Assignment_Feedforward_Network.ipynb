{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_Feedforward-Network.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5RLIK7HMf2mzvR+so9Jco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment_Feedforward_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybsamp8T__sG",
        "outputId": "4adfcd86-36c3-43f1-e73a-68fe6cada0b6"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import string\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "import pandas as pd\n",
        "import glob\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEecDn_cAPUR"
      },
      "source": [
        "# Part 1: IMDB sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I37czSdvAJGh"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOTrU1EcAaJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3746113-73e8-4e08-c5d1-8d39efa92a9a"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "data = tf.keras.utils.get_file ('aclImdb_v1',\n",
        "                                url,\n",
        "                                untar = True,\n",
        "                                cache_dir = '.',\n",
        "                                cache_subdir = '')\n",
        "data_dir = os.path.join (os.path.dirname(data), 'aclImdb')\n",
        "print(os.listdir(data_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['imdb.vocab', 'test', 'README', 'imdbEr.txt', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl4s0XGxECq4",
        "outputId": "9c88359a-1526-434e-b0ff-5237c4a5df6b"
      },
      "source": [
        "train_dir = os.path.join (data_dir, 'train')\n",
        "test_dir = os.path.join (data_dir, 'test')\n",
        "print(os.listdir(train_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'urls_neg.txt', 'urls_pos.txt', 'unsup', 'pos', 'urls_unsup.txt', 'unsupBow.feat', 'labeledBow.feat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqLbtPKfNc5Z"
      },
      "source": [
        "# We only use files in the two folders: pos, and neg, so let's remove other files\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5fHSce2Rk-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aba644b-53c3-4267-88fa-56c96266a848"
      },
      "source": [
        "# Loading data from the directory\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "raw_train = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                        batch_size =batch_size,\n",
        "                                                        validation_split = 0.2,\n",
        "                                                        subset = 'training',\n",
        "                                                        seed = seed)\n",
        "raw_val = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                      batch_size = batch_size,\n",
        "                                                      validation_split = 0.2,\n",
        "                                                      subset = 'validation',\n",
        "                                                      seed = seed)\n",
        "raw_test = tf.keras.utils.text_dataset_from_directory ('aclImdb/test',\n",
        "                                                       batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuL_ipXkaAyv"
      },
      "source": [
        "## Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KVgg_ZWaFtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96886626-ec97-4f1f-9386-98de4d5bd3b3"
      },
      "source": [
        "def custom_preprocessing (text):\n",
        "  lowercase = tf.strings.lower (text)\n",
        "  stripped_html = tf.strings.regex_replace (lowercase,'<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), \n",
        "                                  '')\n",
        "  \n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(standardize = custom_preprocessing,\n",
        "                                           max_tokens = max_features,\n",
        "                                           output_mode = 'int',\n",
        "                                           output_sequence_length = sequence_length)\n",
        "# Extracting features for vectorizing using training set\n",
        "train_text = raw_train.map (lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "# Defining a function for fitting vectorizer function/layer to vectorize text (review)\n",
        "def fitting_vectorizer (text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer (text), label\n",
        "\n",
        "# storing text batch and label batch\n",
        "text_batch, label_batch = next(iter(raw_train))\n",
        "\n",
        "## print an instance with vectorized review and label for observing\n",
        "print ('REVIEW:', text_batch[0])\n",
        "print('LABEL:', raw_train.class_names[label_batch[0]] )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REVIEW: tf.Tensor(b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.', shape=(), dtype=string)\n",
            "LABEL: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdhUEflLG1-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525acdb2-0d25-4b27-fddd-41d8727d6554"
      },
      "source": [
        "# print an example of vectorized data\n",
        "print ('Vocabulary size: ', len(vectorize_layer.get_vocabulary()))\n",
        "for i in range (90, 100):\n",
        "  print ('{} ------> {}'.format(i, vectorize_layer.get_vocabulary()[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size:  10000\n",
            "90 ------> made\n",
            "91 ------> movies\n",
            "92 ------> then\n",
            "93 ------> them\n",
            "94 ------> films\n",
            "95 ------> way\n",
            "96 ------> make\n",
            "97 ------> any\n",
            "98 ------> could\n",
            "99 ------> too\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49aXKTf-LYpE"
      },
      "source": [
        "train = raw_train.map(fitting_vectorizer)\n",
        "val = raw_val.map(fitting_vectorizer)\n",
        "test = raw_test.map(fitting_vectorizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqxSoCqORfY"
      },
      "source": [
        "# Configure the dataset for performance\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train = train.cache().prefetch (buffer_size = autotune)\n",
        "val = val.cache().prefetch (buffer_size = autotune)\n",
        "test = test.cache().prefetch (buffer_size = autotune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoADSjBJMHL7"
      },
      "source": [
        "## Building a neural network classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS4xOvZ_ZqLC"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='binary')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='binary')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='binary')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaffgzJaRMKe"
      },
      "source": [
        "### With different numbers of embedding dimentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnp8gN87MPaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334ef255-7055-43cd-abe8-04de5a753dcf"
      },
      "source": [
        "# Creating the model\n",
        "embedding_dim = [16, 28, 50]\n",
        "for n in embedding_dim:\n",
        "  print (\"========= embedding vectors'size= %s ============\" %n)\n",
        "  model = tf.keras.Sequential([layers.Embedding(max_features + 1, n, name=\"embedding\"),\n",
        "                              layers.Dropout(0.2),\n",
        "                              layers.GlobalAveragePooling1D(),\n",
        "                              layers.Dropout(0.2),\n",
        "                              layers.Dense(1, activation = 'sigmoid')])\n",
        "  print(model.summary())\n",
        "\n",
        "  # configure the model uisng optimizer and loss function\n",
        "  model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "                optimizer = 'adam',\n",
        "                metrics = tf.metrics.BinaryAccuracy(threshold = 0.5 )) \n",
        "  # training the model\n",
        "  epochs = 10\n",
        "  history = model.fit(train,\n",
        "                      validation_data = val,\n",
        "                      epochs = epochs)\n",
        "  # testing the model\n",
        "  # pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========= embedding vectors'size= 16 ============\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4994: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 10s 15ms/step - loss: 0.6636 - binary_accuracy: 0.6902 - val_loss: 0.6132 - val_binary_accuracy: 0.7704\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.5475 - binary_accuracy: 0.8018 - val_loss: 0.4969 - val_binary_accuracy: 0.8216\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4437 - binary_accuracy: 0.8448 - val_loss: 0.4190 - val_binary_accuracy: 0.8486\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3775 - binary_accuracy: 0.8651 - val_loss: 0.3728 - val_binary_accuracy: 0.8612\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.3351 - binary_accuracy: 0.8802 - val_loss: 0.3444 - val_binary_accuracy: 0.8674\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3051 - binary_accuracy: 0.8884 - val_loss: 0.3254 - val_binary_accuracy: 0.8716\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2804 - binary_accuracy: 0.8985 - val_loss: 0.3121 - val_binary_accuracy: 0.8734\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2619 - binary_accuracy: 0.9040 - val_loss: 0.3029 - val_binary_accuracy: 0.8766\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2460 - binary_accuracy: 0.9117 - val_loss: 0.2961 - val_binary_accuracy: 0.8782\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2321 - binary_accuracy: 0.9177 - val_loss: 0.2914 - val_binary_accuracy: 0.8786\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3103 - binary_accuracy: 0.8735\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.310 - Accuracy: 0.874\n",
            "accuracy score: 0.874\n",
            "precision score: 0.875\n",
            "recall score: 0.871\n",
            "F1 score: 0.873\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87     12500\n",
            "           1       0.88      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= embedding vectors'size= 28 ============\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 28)          280028    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 28)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 28)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 28)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 29        \n",
            "=================================================================\n",
            "Total params: 280,057\n",
            "Trainable params: 280,057\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 6s 8ms/step - loss: 0.6506 - binary_accuracy: 0.6966 - val_loss: 0.5791 - val_binary_accuracy: 0.7828\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.4983 - binary_accuracy: 0.8189 - val_loss: 0.4446 - val_binary_accuracy: 0.8364\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.3906 - binary_accuracy: 0.8591 - val_loss: 0.3752 - val_binary_accuracy: 0.8590\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.3323 - binary_accuracy: 0.8784 - val_loss: 0.3397 - val_binary_accuracy: 0.8686\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2951 - binary_accuracy: 0.8914 - val_loss: 0.3182 - val_binary_accuracy: 0.8732\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2677 - binary_accuracy: 0.9014 - val_loss: 0.3046 - val_binary_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2463 - binary_accuracy: 0.9093 - val_loss: 0.2961 - val_binary_accuracy: 0.8776\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2285 - binary_accuracy: 0.9176 - val_loss: 0.2907 - val_binary_accuracy: 0.8798\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2127 - binary_accuracy: 0.9236 - val_loss: 0.2877 - val_binary_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1994 - binary_accuracy: 0.9291 - val_loss: 0.2862 - val_binary_accuracy: 0.8802\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3078 - binary_accuracy: 0.8739\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.308 - Accuracy: 0.874\n",
            "accuracy score: 0.874\n",
            "precision score: 0.875\n",
            "recall score: 0.872\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87     12500\n",
            "           1       0.88      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= embedding vectors'size= 50 ============\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 50)          500050    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 50)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 500,101\n",
            "Trainable params: 500,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.6308 - binary_accuracy: 0.7114 - val_loss: 0.5350 - val_binary_accuracy: 0.7960\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.4457 - binary_accuracy: 0.8400 - val_loss: 0.3967 - val_binary_accuracy: 0.8508\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.3452 - binary_accuracy: 0.8726 - val_loss: 0.3415 - val_binary_accuracy: 0.8676\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2936 - binary_accuracy: 0.8910 - val_loss: 0.3147 - val_binary_accuracy: 0.8730\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.2609 - binary_accuracy: 0.9038 - val_loss: 0.3001 - val_binary_accuracy: 0.8762\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2355 - binary_accuracy: 0.9137 - val_loss: 0.2920 - val_binary_accuracy: 0.8792\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.2153 - binary_accuracy: 0.9218 - val_loss: 0.2878 - val_binary_accuracy: 0.8818\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.1980 - binary_accuracy: 0.9286 - val_loss: 0.2865 - val_binary_accuracy: 0.8806\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 8s 13ms/step - loss: 0.1827 - binary_accuracy: 0.9355 - val_loss: 0.2875 - val_binary_accuracy: 0.8826\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 9s 14ms/step - loss: 0.1692 - binary_accuracy: 0.9412 - val_loss: 0.2901 - val_binary_accuracy: 0.8830\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.3166 - binary_accuracy: 0.8720\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.317 - Accuracy: 0.872\n",
            "accuracy score: 0.872\n",
            "precision score: 0.873\n",
            "recall score: 0.871\n",
            "F1 score: 0.872\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57AmOkg819dP"
      },
      "source": [
        "From the performance of models with different embedding size (the number of dimmensions), we can see their performances are not much different. Among them, the model with embedding vectors'size= 28 has the highest performance (F1 score: 0.874), slightly higher than the models with embedding vectors'size of 16 or 50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvJLzGQbZZD8"
      },
      "source": [
        "### With different dropout "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn5GRRbvaB3k",
        "outputId": "b72dad33-dc11-4df8-b4ae-e56c6c774916"
      },
      "source": [
        "# Creating the model\n",
        "embedding_dim = 16\n",
        "dropouts = [0.0, 0.1, 0.2, 0.3]\n",
        "for i in dropouts:\n",
        "  print (\"========= dropout = %s ============\" %i)\n",
        "  model = tf.keras.Sequential([layers.Embedding(max_features + 1,embedding_dim,  name=\"embedding\"),\n",
        "                              layers.Dropout(i),\n",
        "                              layers.GlobalAveragePooling1D(),\n",
        "                              layers.Dropout(0.2),\n",
        "                              layers.Dense(1,activation = 'sigmoid')])\n",
        "  print(model.summary())\n",
        "\n",
        "  # configure the model uisng optimizer and loss function\n",
        "  model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "                optimizer = 'adam',\n",
        "                metrics = tf.metrics.BinaryAccuracy(threshold = 0.5 )) \n",
        "  # training the model\n",
        "  epochs = 10\n",
        "  history = model.fit(train,\n",
        "                      validation_data = val,\n",
        "                      epochs = epochs)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= dropout = 0.0 ============\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4994: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 3s 4ms/step - loss: 0.6624 - binary_accuracy: 0.6939 - val_loss: 0.6104 - val_binary_accuracy: 0.7748\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.5418 - binary_accuracy: 0.8044 - val_loss: 0.4902 - val_binary_accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.4356 - binary_accuracy: 0.8496 - val_loss: 0.4121 - val_binary_accuracy: 0.8500\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3699 - binary_accuracy: 0.8697 - val_loss: 0.3669 - val_binary_accuracy: 0.8610\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3272 - binary_accuracy: 0.8818 - val_loss: 0.3395 - val_binary_accuracy: 0.8696\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2963 - binary_accuracy: 0.8913 - val_loss: 0.3211 - val_binary_accuracy: 0.8724\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2731 - binary_accuracy: 0.9000 - val_loss: 0.3085 - val_binary_accuracy: 0.8746\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2536 - binary_accuracy: 0.9079 - val_loss: 0.2998 - val_binary_accuracy: 0.8762\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2375 - binary_accuracy: 0.9148 - val_loss: 0.2938 - val_binary_accuracy: 0.8772\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2227 - binary_accuracy: 0.9197 - val_loss: 0.2898 - val_binary_accuracy: 0.8796\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3085 - binary_accuracy: 0.8741\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.308 - Accuracy: 0.874\n",
            "accuracy score: 0.874\n",
            "precision score: 0.874\n",
            "recall score: 0.875\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= dropout = 0.1 ============\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_4 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.6607 - binary_accuracy: 0.6992 - val_loss: 0.6102 - val_binary_accuracy: 0.7758\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5440 - binary_accuracy: 0.8034 - val_loss: 0.4936 - val_binary_accuracy: 0.8226\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4396 - binary_accuracy: 0.8475 - val_loss: 0.4158 - val_binary_accuracy: 0.8488\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3727 - binary_accuracy: 0.8683 - val_loss: 0.3699 - val_binary_accuracy: 0.8624\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3304 - binary_accuracy: 0.8806 - val_loss: 0.3418 - val_binary_accuracy: 0.8708\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3009 - binary_accuracy: 0.8899 - val_loss: 0.3229 - val_binary_accuracy: 0.8730\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2771 - binary_accuracy: 0.8982 - val_loss: 0.3102 - val_binary_accuracy: 0.8734\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2566 - binary_accuracy: 0.9071 - val_loss: 0.3009 - val_binary_accuracy: 0.8780\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2404 - binary_accuracy: 0.9126 - val_loss: 0.2946 - val_binary_accuracy: 0.8780\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2264 - binary_accuracy: 0.9182 - val_loss: 0.2903 - val_binary_accuracy: 0.8796\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3093 - binary_accuracy: 0.8740\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.309 - Accuracy: 0.874\n",
            "accuracy score: 0.874\n",
            "precision score: 0.874\n",
            "recall score: 0.874\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= dropout = 0.2 ============\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_5 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6606 - binary_accuracy: 0.6933 - val_loss: 0.6090 - val_binary_accuracy: 0.7734\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5429 - binary_accuracy: 0.8027 - val_loss: 0.4934 - val_binary_accuracy: 0.8232\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4401 - binary_accuracy: 0.8478 - val_loss: 0.4168 - val_binary_accuracy: 0.8488\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3748 - binary_accuracy: 0.8676 - val_loss: 0.3714 - val_binary_accuracy: 0.8620\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3330 - binary_accuracy: 0.8806 - val_loss: 0.3432 - val_binary_accuracy: 0.8696\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3027 - binary_accuracy: 0.8906 - val_loss: 0.3246 - val_binary_accuracy: 0.8718\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2805 - binary_accuracy: 0.8984 - val_loss: 0.3114 - val_binary_accuracy: 0.8744\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2594 - binary_accuracy: 0.9062 - val_loss: 0.3023 - val_binary_accuracy: 0.8772\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2433 - binary_accuracy: 0.9122 - val_loss: 0.2956 - val_binary_accuracy: 0.8784\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2293 - binary_accuracy: 0.9179 - val_loss: 0.2910 - val_binary_accuracy: 0.8790\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3096 - binary_accuracy: 0.8738\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.310 - Accuracy: 0.874\n",
            "accuracy score: 0.874\n",
            "precision score: 0.872\n",
            "recall score: 0.876\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87     12500\n",
            "           1       0.87      0.88      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= dropout = 0.3 ============\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_6 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6684 - binary_accuracy: 0.6742 - val_loss: 0.6228 - val_binary_accuracy: 0.7672\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5587 - binary_accuracy: 0.7918 - val_loss: 0.5078 - val_binary_accuracy: 0.8152\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4544 - binary_accuracy: 0.8382 - val_loss: 0.4280 - val_binary_accuracy: 0.8448\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3864 - binary_accuracy: 0.8622 - val_loss: 0.3798 - val_binary_accuracy: 0.8584\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3435 - binary_accuracy: 0.8745 - val_loss: 0.3503 - val_binary_accuracy: 0.8656\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3126 - binary_accuracy: 0.8851 - val_loss: 0.3302 - val_binary_accuracy: 0.8706\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2893 - binary_accuracy: 0.8921 - val_loss: 0.3162 - val_binary_accuracy: 0.8722\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2696 - binary_accuracy: 0.9003 - val_loss: 0.3063 - val_binary_accuracy: 0.8744\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2535 - binary_accuracy: 0.9075 - val_loss: 0.2990 - val_binary_accuracy: 0.8780\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2382 - binary_accuracy: 0.9137 - val_loss: 0.2938 - val_binary_accuracy: 0.8784\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3119 - binary_accuracy: 0.8725\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.312 - Accuracy: 0.873\n",
            "accuracy score: 0.873\n",
            "precision score: 0.872\n",
            "recall score: 0.874\n",
            "F1 score: 0.873\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSKqpzc6Fns"
      },
      "source": [
        "Setting a dropout layer is for preventing overfitting, and save computational resource. We experiment different dropout sizes: no dropout, 10% dropout, 20%, and 30% dropout. Generally, the models with different dropout rates have quite similar performance. Dropout rate of 0.3 causes the performance to drop a bit (F1 score: 0.873) while no dropout, dropout rate of 0.1 and 0.2 do not affect the model performance regarding F1 score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te4tKnBY8mXP"
      },
      "source": [
        "### Adding a Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la1m7MZW9Ah5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0017b984-8f09-48d6-a999-418d0e580b75"
      },
      "source": [
        "# Creating the model\n",
        "embedding_dim = 16\n",
        "dropout =  0.1\n",
        "activations = ['relu', 'softmax', 'sigmoid']\n",
        "for f in activations:\n",
        "  print (\"========= activation function = %s ============\" %f)\n",
        "  model = tf.keras.Sequential([layers.Embedding(max_features + 1,embedding_dim,  name=\"embedding\"),\n",
        "                              layers.Dropout(dropout),\n",
        "                              layers.GlobalAveragePooling1D(),\n",
        "                              layers.Dropout(dropout),\n",
        "                              layers.Dense(32, activation= f),\n",
        "                              layers.Dense(1,activation = 'sigmoid')])\n",
        "  print(model.summary())\n",
        "\n",
        "  # configure the model uisng optimizer and loss function\n",
        "  model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "                optimizer = 'adam',\n",
        "                metrics = tf.metrics.BinaryAccuracy(threshold = 0.5 )) \n",
        "  # training the model\n",
        "  epochs = 10\n",
        "  history = model.fit(train,\n",
        "                      validation_data = val,\n",
        "                      epochs = epochs)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= activation function = relu ============\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_7 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,593\n",
            "Trainable params: 160,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4994: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5472 - binary_accuracy: 0.7424 - val_loss: 0.3561 - val_binary_accuracy: 0.8552\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2924 - binary_accuracy: 0.8837 - val_loss: 0.3020 - val_binary_accuracy: 0.8738\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2256 - binary_accuracy: 0.9131 - val_loss: 0.2988 - val_binary_accuracy: 0.8764\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1854 - binary_accuracy: 0.9327 - val_loss: 0.3118 - val_binary_accuracy: 0.8738\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1549 - binary_accuracy: 0.9448 - val_loss: 0.3342 - val_binary_accuracy: 0.8694\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1324 - binary_accuracy: 0.9553 - val_loss: 0.3559 - val_binary_accuracy: 0.8680\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1106 - binary_accuracy: 0.9660 - val_loss: 0.3798 - val_binary_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0940 - binary_accuracy: 0.9729 - val_loss: 0.4172 - val_binary_accuracy: 0.8658\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0795 - binary_accuracy: 0.9779 - val_loss: 0.4545 - val_binary_accuracy: 0.8648\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0665 - binary_accuracy: 0.9819 - val_loss: 0.4875 - val_binary_accuracy: 0.8670\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5721 - binary_accuracy: 0.8412\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.572 - Accuracy: 0.841\n",
            "accuracy score: 0.841\n",
            "precision score: 0.863\n",
            "recall score: 0.811\n",
            "F1 score: 0.836\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85     12500\n",
            "           1       0.86      0.81      0.84     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n",
            "========= activation function = softmax ============\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_8 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,593\n",
            "Trainable params: 160,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.6853 - binary_accuracy: 0.5964 - val_loss: 0.6577 - val_binary_accuracy: 0.7540\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5721 - binary_accuracy: 0.7983 - val_loss: 0.4850 - val_binary_accuracy: 0.8336\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4092 - binary_accuracy: 0.8590 - val_loss: 0.3706 - val_binary_accuracy: 0.8620\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3227 - binary_accuracy: 0.8820 - val_loss: 0.3266 - val_binary_accuracy: 0.8708\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2765 - binary_accuracy: 0.8991 - val_loss: 0.3092 - val_binary_accuracy: 0.8734\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2454 - binary_accuracy: 0.9122 - val_loss: 0.3016 - val_binary_accuracy: 0.8780\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2230 - binary_accuracy: 0.9223 - val_loss: 0.2994 - val_binary_accuracy: 0.8774\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2050 - binary_accuracy: 0.9290 - val_loss: 0.2993 - val_binary_accuracy: 0.8790\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1873 - binary_accuracy: 0.9375 - val_loss: 0.3012 - val_binary_accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1727 - binary_accuracy: 0.9437 - val_loss: 0.3049 - val_binary_accuracy: 0.8804\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3260 - binary_accuracy: 0.8718\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.326 - Accuracy: 0.872\n",
            "accuracy score: 0.872\n",
            "precision score: 0.877\n",
            "recall score: 0.864\n",
            "F1 score: 0.871\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.87     12500\n",
            "           1       0.88      0.86      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========= activation function = sigmoid ============\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_9 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,593\n",
            "Trainable params: 160,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6570 - binary_accuracy: 0.6431 - val_loss: 0.5662 - val_binary_accuracy: 0.7716\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.4427 - binary_accuracy: 0.8316 - val_loss: 0.3698 - val_binary_accuracy: 0.8532\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3137 - binary_accuracy: 0.8774 - val_loss: 0.3163 - val_binary_accuracy: 0.8698\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2624 - binary_accuracy: 0.8992 - val_loss: 0.2980 - val_binary_accuracy: 0.8760\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2297 - binary_accuracy: 0.9128 - val_loss: 0.2917 - val_binary_accuracy: 0.8788\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2043 - binary_accuracy: 0.9229 - val_loss: 0.2915 - val_binary_accuracy: 0.8794\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1832 - binary_accuracy: 0.9324 - val_loss: 0.2950 - val_binary_accuracy: 0.8830\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1648 - binary_accuracy: 0.9409 - val_loss: 0.3025 - val_binary_accuracy: 0.8828\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1486 - binary_accuracy: 0.9481 - val_loss: 0.3126 - val_binary_accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1350 - binary_accuracy: 0.9543 - val_loss: 0.3254 - val_binary_accuracy: 0.8794\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3642 - binary_accuracy: 0.8655\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.364 - Accuracy: 0.865\n",
            "accuracy score: 0.865\n",
            "precision score: 0.873\n",
            "recall score: 0.856\n",
            "F1 score: 0.864\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87     12500\n",
            "           1       0.87      0.86      0.86     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIVHwlm-R23"
      },
      "source": [
        "We add one more Dense layer before the last Dense layer, and experiment different nonlinear functions: ReLU, softmax, and sigmoid. The model with ReLU activation performs worst with F1 score = 0.836 on the test set. The model with softmax activation performs the best with a F1 score of 0.871, followed by the model with sigmoid activation (F1 score = 0.864)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOBe7jHOHLQ_"
      },
      "source": [
        "### With different Batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dPOERiaH501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec85e54c-6700-4f73-f6f5-27eb12df2c36"
      },
      "source": [
        "# Loading data from the directory\n",
        "batch_size = 64\n",
        "seed = 42\n",
        "raw_train = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                        batch_size =batch_size,\n",
        "                                                        validation_split = 0.2,\n",
        "                                                        subset = 'training',\n",
        "                                                        seed = seed)\n",
        "raw_val = tf.keras.utils.text_dataset_from_directory ('aclImdb/train',\n",
        "                                                      batch_size = batch_size,\n",
        "                                                      validation_split = 0.2,\n",
        "                                                      subset = 'validation',\n",
        "                                                      seed = seed)\n",
        "raw_test = tf.keras.utils.text_dataset_from_directory ('aclImdb/test',\n",
        "                                                       batch_size = batch_size)\n",
        "\n",
        "# storing text batch and label batch\n",
        "text_batch, label_batch = next(iter(raw_train))\n",
        "\n",
        "## print an instance with vectorized review and label for observing\n",
        "print ('REVIEW:', text_batch[0])\n",
        "print('LABEL:', raw_train.class_names[label_batch[0]] )\n",
        "\n",
        "\n",
        "# Creating the model\n",
        "embedding_dim = 16\n",
        "dropout =  0.1\n",
        "activation =  'softmax'\n",
        "\n",
        "print (\"======== activation function = {}, dropout = {}, batch size = {} ============\".format(activation, dropout, batch_size ))\n",
        "model = tf.keras.Sequential([layers.Embedding(max_features + 1,embedding_dim, name=\"embedding\"),\n",
        "                            layers.Dropout(dropout),\n",
        "                            layers.GlobalAveragePooling1D(),\n",
        "                            layers.Dropout(dropout),\n",
        "                            layers.Dense(32, activation= activation),\n",
        "                            layers.Dense(1)])\n",
        "print(model.summary())\n",
        "\n",
        "# configure the model uisng optimizer and loss function\n",
        "model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "              optimizer = 'adam',\n",
        "              metrics = tf.metrics.BinaryAccuracy(threshold = 0.5 )) \n",
        "# training the model\n",
        "epochs = 10\n",
        "history = model.fit(train,\n",
        "                    validation_data = val,\n",
        "                    epochs = epochs)\n",
        "# testing the model\n",
        "pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(test)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "REVIEW: tf.Tensor(b\"First of all, I liked very much the central idea of locating the '' intruders'', Others in the fragile Self, on various levels - mainly subconscious but sometimes more allegorical. In fact the intruders are omnipresent throughout the film : in the Swiss-French border where the pretagonist leads secluded life; in the his recurring daydream and nightmare; inside his ailing body after heart transplantation.... In the last half of the film, he becomes intruder himself, returning in ancient french colony in the hope of atoning for the past. <br /><br />The overall tone is bitter rather than pathetic, full of regrets and guilts, sense of failure being more or less dominant. This is a quite grim picture of an old age, ostensibly self-dependent but hopelessly void and lonely inside. The directer composes the images more to convey passing sensations of anxiety and desire than any explicit meanings. Some of them are mesmerizing, not devoid of humor though, kind of absurdist play only somnambulist can visualize.\", shape=(), dtype=string)\n",
            "LABEL: pos\n",
            "======== activation function = softmax, dropout = 0.1, batch size = 64 ============\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_10  (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,593\n",
            "Trainable params: 160,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6856 - binary_accuracy: 0.5019 - val_loss: 0.6591 - val_binary_accuracy: 0.4924\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5762 - binary_accuracy: 0.6364 - val_loss: 0.4920 - val_binary_accuracy: 0.7926\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4184 - binary_accuracy: 0.8357 - val_loss: 0.3802 - val_binary_accuracy: 0.8472\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3310 - binary_accuracy: 0.8753 - val_loss: 0.3330 - val_binary_accuracy: 0.8628\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2827 - binary_accuracy: 0.8936 - val_loss: 0.3120 - val_binary_accuracy: 0.8702\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2490 - binary_accuracy: 0.9079 - val_loss: 0.3024 - val_binary_accuracy: 0.8742\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2253 - binary_accuracy: 0.9177 - val_loss: 0.2981 - val_binary_accuracy: 0.8764\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2058 - binary_accuracy: 0.9265 - val_loss: 0.2982 - val_binary_accuracy: 0.8754\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1886 - binary_accuracy: 0.9348 - val_loss: 0.2993 - val_binary_accuracy: 0.8760\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1734 - binary_accuracy: 0.9422 - val_loss: 0.3029 - val_binary_accuracy: 0.8778\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3243 - binary_accuracy: 0.8699\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.324 - Accuracy: 0.870\n",
            "accuracy score: 0.870\n",
            "precision score: 0.897\n",
            "recall score: 0.836\n",
            "F1 score: 0.865\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87     12500\n",
            "           1       0.90      0.84      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.86992, 0.8967736399519478, 0.83608, 0.8653639148795231)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlClyC-BAnV"
      },
      "source": [
        "Previously, we have built the model Sequence_8 with a batch size of 32, dropout = 0.1, activation function = softmax, and it achieves F1 score = 0.871. In the above model (Sequence_9), we use the same architecture, but train it on a batch size of 64 to investigate whether batch size impacts performance of the model. We found that F1 score drops a bit to 0.865, meaning that batch size of 32 or 64 does not significantly impact the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7OsTdtjWfge"
      },
      "source": [
        "### With different training algorithms\n",
        "Here we mostly focus on adapting learning rate methods to train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnIwxcEVW4CC",
        "outputId": "0441f4ef-3e31-41d8-d08c-9afa41683289"
      },
      "source": [
        "# configure the model uisng optimizer and loss function\n",
        "optimizers = ['adagrad', 'rmsprop', 'adam']\n",
        "\n",
        "print (\"======== activation function = {}, dropout = {}, batch size = {} ============\".format(activation, dropout, batch_size ))\n",
        "for opt in optimizers:\n",
        "  print( '========== optimizer = %s' %opt)\n",
        "  model.compile(loss = losses.BinaryCrossentropy(from_logits = True),\n",
        "                optimizer = opt,\n",
        "                metrics = tf.metrics.BinaryAccuracy(threshold = 0.5 )) \n",
        "  # training the model\n",
        "  epochs = 10\n",
        "  history = model.fit(train,\n",
        "                      validation_data = val,\n",
        "                      epochs = epochs)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== activation function = softmax, dropout = 0.1, batch size = 64 ============\n",
            "========== optimizer = adagrad\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 5ms/step - loss: 0.1585 - binary_accuracy: 0.9485 - val_loss: 0.3038 - val_binary_accuracy: 0.8772\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1577 - binary_accuracy: 0.9495 - val_loss: 0.3040 - val_binary_accuracy: 0.8770\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1582 - binary_accuracy: 0.9485 - val_loss: 0.3041 - val_binary_accuracy: 0.8770\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1581 - binary_accuracy: 0.9502 - val_loss: 0.3041 - val_binary_accuracy: 0.8766\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1582 - binary_accuracy: 0.9495 - val_loss: 0.3043 - val_binary_accuracy: 0.8768\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1578 - binary_accuracy: 0.9493 - val_loss: 0.3043 - val_binary_accuracy: 0.8768\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1582 - binary_accuracy: 0.9488 - val_loss: 0.3043 - val_binary_accuracy: 0.8764\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1573 - binary_accuracy: 0.9504 - val_loss: 0.3044 - val_binary_accuracy: 0.8766\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1578 - binary_accuracy: 0.9499 - val_loss: 0.3044 - val_binary_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1584 - binary_accuracy: 0.9483 - val_loss: 0.3044 - val_binary_accuracy: 0.8764\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3245 - binary_accuracy: 0.8698\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.325 - Accuracy: 0.870\n",
            "accuracy score: 0.870\n",
            "precision score: 0.897\n",
            "recall score: 0.836\n",
            "F1 score: 0.865\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87     12500\n",
            "           1       0.90      0.84      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "========== optimizer = rmsprop\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1591 - binary_accuracy: 0.9475 - val_loss: 0.3111 - val_binary_accuracy: 0.8772\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1529 - binary_accuracy: 0.9509 - val_loss: 0.3169 - val_binary_accuracy: 0.8776\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1485 - binary_accuracy: 0.9514 - val_loss: 0.3211 - val_binary_accuracy: 0.8766\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1453 - binary_accuracy: 0.9530 - val_loss: 0.3241 - val_binary_accuracy: 0.8762\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1420 - binary_accuracy: 0.9531 - val_loss: 0.3268 - val_binary_accuracy: 0.8778\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1396 - binary_accuracy: 0.9551 - val_loss: 0.3297 - val_binary_accuracy: 0.8786\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1358 - binary_accuracy: 0.9554 - val_loss: 0.3326 - val_binary_accuracy: 0.8792\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1352 - binary_accuracy: 0.9552 - val_loss: 0.3349 - val_binary_accuracy: 0.8786\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1329 - binary_accuracy: 0.9564 - val_loss: 0.3367 - val_binary_accuracy: 0.8776\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1312 - binary_accuracy: 0.9567 - val_loss: 0.3389 - val_binary_accuracy: 0.8776\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.3800 - binary_accuracy: 0.8617\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.380 - Accuracy: 0.862\n",
            "accuracy score: 0.862\n",
            "precision score: 0.892\n",
            "recall score: 0.823\n",
            "F1 score: 0.856\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87     12500\n",
            "           1       0.89      0.82      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "========== optimizer = adam\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1315 - binary_accuracy: 0.9567 - val_loss: 0.3381 - val_binary_accuracy: 0.8790\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1221 - binary_accuracy: 0.9616 - val_loss: 0.3415 - val_binary_accuracy: 0.8814\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1152 - binary_accuracy: 0.9647 - val_loss: 0.3479 - val_binary_accuracy: 0.8806\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1070 - binary_accuracy: 0.9692 - val_loss: 0.3569 - val_binary_accuracy: 0.8806\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1004 - binary_accuracy: 0.9721 - val_loss: 0.3654 - val_binary_accuracy: 0.8786\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0938 - binary_accuracy: 0.9741 - val_loss: 0.3753 - val_binary_accuracy: 0.8784\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0874 - binary_accuracy: 0.9773 - val_loss: 0.3852 - val_binary_accuracy: 0.8776\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0807 - binary_accuracy: 0.9803 - val_loss: 0.3974 - val_binary_accuracy: 0.8752\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.0751 - binary_accuracy: 0.9819 - val_loss: 0.4097 - val_binary_accuracy: 0.8770\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.0708 - binary_accuracy: 0.9833 - val_loss: 0.4223 - val_binary_accuracy: 0.8756\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.4910 - binary_accuracy: 0.8525\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.491 - Accuracy: 0.852\n",
            "accuracy score: 0.852\n",
            "precision score: 0.865\n",
            "recall score: 0.835\n",
            "F1 score: 0.850\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85     12500\n",
            "           1       0.86      0.84      0.85     12500\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lBoPM-tGEPK"
      },
      "source": [
        "We explore three different optimizers: Adam, adagrad, and rmsprop for training the model with activation function = softmax, dropout = 0.1, batch size = 64. We found that adagrad opimizer helps the model achieve higher performance (F1 score: 0.865) than the model trained with Adam (F1 score = 0.856), and rmsprop (F1 score = 0.850)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln0waE00a6eE"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LsKSP4Sa-kC"
      },
      "source": [
        "# Getting the weights that we have trained on our model, and their corresponding vocabulary \n",
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im8CyYCyWuOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566dffe8-05bd-49d3-f37a-fe659176e018"
      },
      "source": [
        "# Defining a function to find the most similar words to a certain chosen word.\n",
        "def Find_similar_w (word, n):\n",
        "  all_cos_sim = {}\n",
        "  idx = vocab.index(word)\n",
        "  weight = weights[idx]\n",
        "  for i in range(len(weights)-1):\n",
        "    cosine_sim = cosine_similarity(weight.reshape(1, -1), weights[i].reshape(1, -1))\n",
        "    all_cos_sim[vocab[i]] = cosine_sim\n",
        "  # Sorting the dictionary in descending order\n",
        "  sorted_cos = {k:v for k, v in sorted(all_cos_sim.items(), key = lambda item: item[1], reverse=True)}\n",
        "  print (\"Top {} most similar with '{}' \\n\".format(n, word))\n",
        "  for k, v in list(sorted_cos.items())[:n]:\n",
        "    print ('{} =====> similarity score: {}'. format(k,v))\n",
        "  return  sorted_cos\n",
        "\n",
        "most_similar_w = Find_similar_w (word = 'boring', n = 20)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 most similar with 'boring' \n",
            "\n",
            "boring =====> [[1.]]\n",
            "disappointment =====> [[0.99934846]]\n",
            "unconvincing =====> [[0.99932355]]\n",
            "meyer =====> [[0.99920887]]\n",
            "dull =====> [[0.9991085]]\n",
            "410 =====> [[0.9991035]]\n",
            "worst =====> [[0.9990882]]\n",
            "terrible =====> [[0.9990096]]\n",
            "badness =====> [[0.99898285]]\n",
            "cena =====> [[0.9989134]]\n",
            "pointless =====> [[0.99889576]]\n",
            "nothing =====> [[0.9988869]]\n",
            "downhill =====> [[0.9987695]]\n",
            "lacks =====> [[0.998716]]\n",
            "giant =====> [[0.9987079]]\n",
            "unwatchable =====> [[0.99868995]]\n",
            "worse =====> [[0.998679]]\n",
            "thunderbirds =====> [[0.9986668]]\n",
            "options =====> [[0.9986569]]\n",
            "seconds =====> [[0.9986452]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4xanECiKcgB"
      },
      "source": [
        "We make use of the weights that have been trained in our network to find the most similar words to the chosen words. First, we obtain and store these weights into a list. Meanwhile, we store the vocabulary which corresponds to weights with index into another list. Next, we select a word, 'boring', and get its trained weight using index. Then, by looping over weights in the weight list, we compute cosine similarity between the weight of the word 'boring', and other words' weights. The higher cosine similarity scores are, the more similar the word pairs are. The above result shows the top 10 most similar words with the word 'boring'. Most of them make a lot of sense such as 'disappointment', 'unconvincing', 'dull', 'worst', 'terrible'. The term 'meyer', and '410' seem not related, but they are likely used with the term 'boring'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiijE5K_bARq"
      },
      "source": [
        "## Comparing with a Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKdXFowRwer9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dbdf32-3d82-48f7-e14b-b7772d5cf579"
      },
      "source": [
        "# loading the ds\n",
        "def Getdata(dir):\n",
        "  review, label = [], []\n",
        "  for folder in glob.glob (dir+'/*'):\n",
        "    for file in glob.glob(folder+'/*'): \n",
        "      fo = open(file)\n",
        "      doc = fo.read()\n",
        "      review.append (doc)\n",
        "      if 'pos' in file:\n",
        "        label.append(1)\n",
        "      elif 'neg' in file:\n",
        "        label.append(0)\n",
        "  df = pd.DataFrame(zip(review,label), columns = ['review', 'label'])\n",
        "  return df\n",
        "  \n",
        "train_df = Getdata(train_dir)\n",
        "test_df = Getdata(test_dir)\n",
        "# Spliting the dataset for training and testing\n",
        "X_train, X_val, y_train, y_val = train_test_split (train_df['review'],train_df['label'], train_size = 0.8, random_state = 42, shuffle = True)\n",
        "X_test, y_test = test_df['review'], test_df['label']\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_val, y_val: ', X_val.shape, y_val.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (20000,) (20000,)\n",
            "Shapes of X_val, y_val:  (5000,) (5000,)\n",
            "Shapes of X_test, y_test:  (25000,) (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhfG0yG8meEX"
      },
      "source": [
        "\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred)\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred)\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hALCMWMSmLhv"
      },
      "source": [
        "### With Countvectorizer text presentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9BcQ-UebIcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03f0a9b-a757-4e7c-9edc-60782dd796a3"
      },
      "source": [
        "\n",
        "# Vectorizing the documents\n",
        "vectorizer = CountVectorizer(binary = True)\n",
        "X_train_count = vectorizer.fit_transform(X_train.to_list())\n",
        "X_val_count = vectorizer.transform(X_val.to_list())\n",
        "X_test_count = vectorizer.transform(X_test.to_list())\n",
        "print ('Shapes of X_train, y_train: ', X_train_count.shape, y_train.shape)\n",
        "print ('Shapes of X_val, y_val: ', X_val_count.shape, y_val.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test_count.shape, y_test.shape)\n",
        "\n",
        "# Sklearn Logistic Regression Model\n",
        "sk_lr_1 = LogisticRegression(solver='lbfgs', max_iter=500).fit(X_train_count, y_train )\n",
        "y_predict = sk_lr_1.predict(X_test_count)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance with Countvectorizer: \\non validation set:')\n",
        "printing_eval_scores (y_val, sk_lr_1.predict(X_val_count))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('on test set:')\n",
        "printing_eval_scores (y_test, y_predict, report = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (20000, 68468) (20000,)\n",
            "Shapes of X_val, y_val:  (5000, 68468) (5000,)\n",
            "Shapes of X_test, y_test:  (25000, 68468) (25000,)\n",
            "Model performance with Countvectorizer: \n",
            "on validation set:\n",
            "accuracy score: 0.872\n",
            "precision score: 0.862\n",
            "recall score: 0.883\n",
            "F1 score: 0.873\n",
            "\n",
            "===========================\n",
            "on test set:\n",
            "accuracy score: 0.866\n",
            "precision score: 0.866\n",
            "recall score: 0.866\n",
            "F1 score: 0.866\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8658, 0.8659463785514205, 0.8656, 0.8657731546309262)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_ex3J6rmVKu"
      },
      "source": [
        "### With tf-idf text presentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av3pjC83mUbt",
        "outputId": "eb62c43d-9f43-497b-f2e2-8f2d2a8078b2"
      },
      "source": [
        "# Vectorizing the documents\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train.to_list())\n",
        "X_val_tfidf = tfidf.transform(X_val.to_list())\n",
        "X_test_tfidf = tfidf.transform(X_test.to_list())\n",
        "print ('Shapes of X_train, y_train: ', X_train_tfidf.shape, y_train.shape)\n",
        "print ('Shapes of X_val, y_val: ', X_val_tfidf.shape, y_val.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test_tfidf.shape, y_test.shape)\n",
        "\n",
        "# Sklearn Logistic Regression Model\n",
        "sk_lr_2 = LogisticRegression(solver='lbfgs', max_iter=500).fit(X_train_tfidf, y_train )\n",
        "y_predict = sk_lr_2.predict(X_test_tfidf)\n",
        "\n",
        "# Model performing\n",
        "## on training set\n",
        "print('Model performance with tfidf: \\non validation set:')\n",
        "printing_eval_scores (y_val, sk_lr_2.predict(X_val_tfidf))\n",
        "\n",
        "## on test set\n",
        "print('\\n===========================')\n",
        "print('on test set:')\n",
        "printing_eval_scores (y_test, y_predict, report = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of X_train, y_train:  (20000, 68468) (20000,)\n",
            "Shapes of X_val, y_val:  (5000, 68468) (5000,)\n",
            "Shapes of X_test, y_test:  (25000, 68468) (25000,)\n",
            "Model performance with tfidf: \n",
            "on validation set:\n",
            "accuracy score: 0.893\n",
            "precision score: 0.888\n",
            "recall score: 0.898\n",
            "F1 score: 0.893\n",
            "\n",
            "===========================\n",
            "on test set:\n",
            "accuracy score: 0.882\n",
            "precision score: 0.881\n",
            "recall score: 0.883\n",
            "F1 score: 0.882\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88     12500\n",
            "           1       0.88      0.88      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.882, 0.8811462324393359, 0.88312, 0.8821320121463961)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqjPbm6PNwCr"
      },
      "source": [
        "By comparing the two Logistic Regression models with two different text representations: Countvectorizer, and tfidf. We found that the model using tfidf to represent the reviews achieved higher performance with F1 score  = 0.882, higher than any models we built on tensorflow, while the another with countvectorizer has a bit lower performance (F1 = 0.866). In the future, we may need to try different text representations for the models on tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUMJOCJYERVd"
      },
      "source": [
        "# Part 2: Multiclass classification - Stackoverflow DS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WThVxknkGiXc"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xxae0yIEyQW",
        "outputId": "75b492ad-bd84-4155-ceb2-7f2429729885"
      },
      "source": [
        "url_2 = 'http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "train_dir = tf.keras.utils.get_file ('train',\n",
        "                                url_2,\n",
        "                                untar = True,\n",
        "                                cache_dir = '.',\n",
        "                                cache_subdir = '')\n",
        "test_dir = tf.keras.utils.get_file ('test',\n",
        "                                url_2,\n",
        "                                untar = True,\n",
        "                                cache_dir = '.',\n",
        "                                cache_subdir = '')\n",
        "\n",
        "print(os.listdir(train_dir))\n",
        "print(os.listdir(test_dir))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['java', 'javascript', 'python', 'csharp']\n",
            "['java', 'javascript', 'python', 'csharp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_BzBYOENa8D",
        "outputId": "a8d0e74d-05e2-4fa0-9080-2ae5111178c3"
      },
      "source": [
        "# Loading data from the directory\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "raw_train = tf.keras.utils.text_dataset_from_directory ('train',\n",
        "                                                        batch_size =batch_size,\n",
        "                                                        validation_split = 0.2,\n",
        "                                                        subset = 'training',\n",
        "                                                        seed = seed)\n",
        "raw_val = tf.keras.utils.text_dataset_from_directory ('train',\n",
        "                                                      batch_size = batch_size,\n",
        "                                                      validation_split = 0.2,\n",
        "                                                      subset = 'validation',\n",
        "                                                      seed = seed)\n",
        "raw_test = tf.keras.utils.text_dataset_from_directory ('test',\n",
        "                                                       batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n",
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n",
            "Found 8000 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKsqlUKNsmz"
      },
      "source": [
        "## Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPCScxOwNwQr",
        "outputId": "6f6b6f95-1963-4357-9ae7-9fada170dda8"
      },
      "source": [
        "def custom_preprocessing (text):\n",
        "  lowercase = tf.strings.lower (text)\n",
        "  stripped_html = tf.strings.regex_replace (lowercase,'<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), \n",
        "                                  '')\n",
        "  \n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(standardize = custom_preprocessing,\n",
        "                                           max_tokens = max_features,\n",
        "                                           output_mode = 'int',\n",
        "                                           output_sequence_length = sequence_length)\n",
        "# Extracting features for vectorizing using training set\n",
        "train_text = raw_train.map (lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "\n",
        "# Defining a function for fitting vectorizer function/layer to vectorize text (review)\n",
        "def fitting_vectorizer (text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer (text), label\n",
        "\n",
        "# storing text batch and label batch\n",
        "text_batch, label_batch = next(iter(raw_train))\n",
        "\n",
        "## print an instance with vectorized review and label for observing\n",
        "print ('text:', text_batch[0])\n",
        "print('label:', raw_train.class_names[label_batch[0]] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: tf.Tensor(b'\"unit testing of setters and getters teacher wanted us to do a comprehensive unit test. for me, this will be the first time that i use junit. i am confused about testing set and get methods. do you think should i test them? if the answer is yes; is this code enough for testing?..  public void testsetandget(){.    int a = 10;.    class firstclass = new class();.    firstclass.setvalue(10);.    int value = firstclass.getvalue();.    assert.asserttrue(\"\"error\"\", value==a);.  }...in my code, i think if there is an error, we can\\'t know that the error is deriving because of setter or getter.\"\\n', shape=(), dtype=string)\n",
            "label: java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJnvsj58OXds"
      },
      "source": [
        "train = raw_train.map(fitting_vectorizer)\n",
        "val = raw_val.map(fitting_vectorizer)\n",
        "test = raw_test.map(fitting_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8TSwV8VOdBD"
      },
      "source": [
        "# Configure the dataset for performance\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train = train.cache().prefetch (buffer_size = autotune)\n",
        "val = val.cache().prefetch (buffer_size = autotune)\n",
        "test = test.cache().prefetch (buffer_size = autotune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFknSi_VOfab"
      },
      "source": [
        "## Building a neural network multiclass classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRYKhiR6YU4-"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='macro')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='macro')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='macro')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXgAJWxAOmPU",
        "outputId": "2868fdac-77d7-4916-b2b9-b90979357776"
      },
      "source": [
        "# Creating the model\n",
        "embedding_dim = 16\n",
        "dropout =  0.1\n",
        "activation = 'relu'\n",
        "\n",
        "print (\"======== activation function = {}, dropout = {}, batch size = {} ============\".format(activation, dropout, batch_size ))\n",
        "model = tf.keras.Sequential([layers.Embedding(max_features + 1,embedding_dim, name=\"embedding_2\"),\n",
        "                            layers.Dropout(dropout),\n",
        "                            layers.GlobalAveragePooling1D(),\n",
        "                            layers.Dropout(dropout),\n",
        "                            layers.Dense(32, activation= activation),\n",
        "                            layers.Dense(4)])\n",
        "print(model.summary())\n",
        "\n",
        "# configure the model uisng optimizer and loss function\n",
        "model.compile(loss = losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              optimizer = 'adam',\n",
        "              metrics = 'accuracy') \n",
        "# training the model\n",
        "epochs = 10\n",
        "history = model.fit(train,\n",
        "                    validation_data = val,\n",
        "                    epochs = epochs)\n",
        "# testing the model\n",
        "pred_label = tf.argmax(model.predict(test),1)\n",
        "true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(test)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(loss, accuracy))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== activation function = relu, dropout = 0.1, batch size = 32 ============\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_11  (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 160,692\n",
            "Trainable params: 160,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 4s 16ms/step - loss: 1.3713 - accuracy: 0.3203 - val_loss: 1.3353 - val_accuracy: 0.4650\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 1.2401 - accuracy: 0.5211 - val_loss: 1.0999 - val_accuracy: 0.6575\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.9833 - accuracy: 0.6511 - val_loss: 0.8571 - val_accuracy: 0.7019\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.7678 - accuracy: 0.7294 - val_loss: 0.7134 - val_accuracy: 0.7419\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.7728 - val_loss: 0.6340 - val_accuracy: 0.7625\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.5484 - accuracy: 0.8037 - val_loss: 0.5872 - val_accuracy: 0.7694\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.4817 - accuracy: 0.8325 - val_loss: 0.5574 - val_accuracy: 0.7769\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.4262 - accuracy: 0.8595 - val_loss: 0.5413 - val_accuracy: 0.7856\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.3795 - accuracy: 0.8769 - val_loss: 0.5320 - val_accuracy: 0.7894\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 0.3359 - accuracy: 0.8928 - val_loss: 0.5287 - val_accuracy: 0.7994\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 0.5906 - accuracy: 0.7728\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.591 - Accuracy: 0.773\n",
            "accuracy score: 0.773\n",
            "precision score: 0.773\n",
            "recall score: 0.773\n",
            "F1 score: 0.771\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.72      0.72      2000\n",
            "           1       0.79      0.67      0.73      2000\n",
            "           2       0.81      0.84      0.82      2000\n",
            "           3       0.77      0.86      0.81      2000\n",
            "\n",
            "    accuracy                           0.77      8000\n",
            "   macro avg       0.77      0.77      0.77      8000\n",
            "weighted avg       0.77      0.77      0.77      8000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.77275, 0.7734619159160259, 0.77275, 0.7710657825470874)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}