{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LING5412_Final_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HuyenNguyenHelen/LING-5412/blob/main/LING5412_Final_project.ipynb",
      "authorship_tag": "ABX9TyMzGiz98amaff+fawYATubR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/LING5412_Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G99qOVzxSOwP",
        "outputId": "bf023b06-125a-4f03-dabd-ea9d92cf2bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tarfile\n",
        "import pandas as pd\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree             # tree.DecisionTreeClassifier()\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm #clf = svm.SVC(decision_function_shape='ovo')\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "!pip install imbalanced-learn\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import recall_score, roc_auc_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5u8SwyQcnwY",
        "outputId": "c8abb5a2-71fe-4631-f180-5079bbf04594"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "nlp = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=-------------------------------------------------] 3.5% 58.9/1662.8MB downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHh5b74YomXY"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvw2VYN_dTtE"
      },
      "source": [
        "\n",
        "# Unzip the dataset\n",
        "#!unzip \"/content/dontpatronizeme_v1.3.zip\" -d \"/content/drive/MyDrive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5DJ3zR_fCre"
      },
      "source": [
        "# Opening the file from MyDrive\n",
        "file = open(r'/content/drive/MyDrive/dontpatronizeme_v1.3/dontpatronizeme_pcl.tsv')\n",
        "reader = csv.reader(file, delimiter=\"\\t\")\n",
        "data = []\n",
        "for row in reader:\n",
        "  data.append(row)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W8pqT5PjG-L"
      },
      "source": [
        "df = pd.DataFrame(data[5:],  columns = ['docID', 'keyword', 'country', 'paragraph', 'label' ] )\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtakLrrIoo9_"
      },
      "source": [
        "# Exploring data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6BfIKFam55W"
      },
      "source": [
        "# Length of text\n",
        "def length (txt):\n",
        "  length = len(txt.split())\n",
        "  return length\n",
        "\n",
        "txt_length = df['paragraph'].apply(lambda x: length(x))\n",
        "txt_length.sort_values(ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nAz-vJxo_wn"
      },
      "source": [
        "# Observing labels\n",
        "df['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy38u3cFVWjW"
      },
      "source": [
        "df['label'] = df['label'].astype(str)\n",
        "df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlKftt3JVk-n"
      },
      "source": [
        "# Missing data\n",
        "#checking missing values\n",
        "print('Is null: \\n', df.isnull().sum() )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyvG1B0pvEI"
      },
      "source": [
        "# Turning labels to binary\n",
        "\n",
        "label_dic = {'0':0,\n",
        "             '1':0,\n",
        "             '2':1,\n",
        "             '3':1,\n",
        "             '4':1}\n",
        "df['label'] = df['label'].map(label_dic)\n",
        "print(df['label'].value_counts())\n",
        "sns.countplot(x='label', data=df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcAMX5reo0e1"
      },
      "source": [
        "# Developing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvza5K7YbJ4H"
      },
      "source": [
        "# Splitting the data into training (80%) and test set(20%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df['paragraph']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split (X, y, train_size = 0.8, random_state = 42, shuffle = True, stratify=y)\n",
        "print ('Shapes of X_train, y_train: ', X_train.shape, y_train.shape)\n",
        "print ('Shapes of X_test, y_test: ', X_test.shape, y_test.shape)\n",
        "print(y_train.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvxZkMMco7sB"
      },
      "source": [
        "## Classic ML models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kso-92jebtft"
      },
      "source": [
        "### Text representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ez7hEFCa5_U"
      },
      "source": [
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "stopwords = get_stop_words('en')\n",
        "from textblob import Word\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#BOW based approaches\n",
        "nlp.init_sims(replace=True) # calling for using syn0norm\n",
        "\n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aopPSRVb4lL"
      },
      "source": [
        "# Tokenize, and apply word vector averaging to tokenized text\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import logging\n",
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens\n",
        "    \n",
        "\n",
        "X_train_tokenized = X_train.apply(lambda x: w2v_tokenize_text(x)).values\n",
        "X_test_tokenized = X_test.apply(lambda x: w2v_tokenize_text(x)).values\n",
        "\n",
        "X_train_word_average = word_averaging_list(nlp,X_train_tokenized)\n",
        "X_test_word_average = word_averaging_list(nlp,X_test_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0O8PduWdnjt"
      },
      "source": [
        "### Creating models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdUeuNUdmij"
      },
      "source": [
        "# Printing model performance \n",
        "def printing_eval_scores (y_true, y_pred):\n",
        "  print('accuracy score: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "  print('precision score: {}'.format(sklearn.metrics.precision_score(y_true, y_pred, average = 'weighted', zero_division=1)))\n",
        "  print('recall score: {}'.format(sklearn.metrics.recall_score(y_true, y_pred,  average = 'weighted', zero_division=1)))\n",
        "  print('F1 score: {}'.format(sklearn.metrics.f1_score(y_true, y_pred,  average = 'weighted', zero_division=1)))\n",
        "  print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2JRHYC7dxi8"
      },
      "source": [
        "# Define a function for creating over sampling \n",
        "def score_model(model):\n",
        "    cv = StratifiedKFold(n_splits=5, random_state=42, shuffle = True)\n",
        "\n",
        "    oversampler = SMOTE(random_state=42)\n",
        "    #oversampler = RandomOverSampler(sampling_strategy='minority')\n",
        "    scores = []\n",
        "\n",
        "    ## on training set, do cv\n",
        "    for train_fold_index, val_fold_index in cv.split(X_train_word_average, y_train):\n",
        "        # Get the training data\n",
        "        X_train_fold, y_train_fold = X_train_word_average[train_fold_index], y_train.iloc[train_fold_index]\n",
        "        # Get the validation data\n",
        "        X_val_fold, y_val_fold = X_train_word_average[val_fold_index], y_train.iloc[val_fold_index]\n",
        "\n",
        "        # Upsample only the data in the training section\n",
        "        X_train_fold_upsample, y_train_fold_upsample = oversampler.fit_resample(X_train_fold,\n",
        "                                                                           y_train_fold)\n",
        "        # Fit the model on the upsampled training data\n",
        "        model.fit(X_train_fold_upsample, y_train_fold_upsample)\n",
        "        # Score the model on the (non-upsampled) validation data\n",
        "        score = accuracy_score(y_val_fold, model.predict(X_val_fold)) #  average= 'weighted' for F1\n",
        "        scores.append(score)\n",
        "    print('Average of acuracy score in training: %s' % np.array(scores).mean())\n",
        "\n",
        "    ## on test set\n",
        "    y_pred = model.predict(X_test_word_average)\n",
        "    test_score = accuracy_score( y_test,y_pred)\n",
        "    printing_eval_scores (y_test, y_pred)\n",
        "    report_scores = {'accuracy_folds':np.array(scores),'accuracy_test':test_score, 'y_predicted':y_pred }\n",
        "    return report_scores #(np.array(scores),test_score,y_pred )       #(np.array(scores).mean(), np.array(scores).std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzbE1tOCiUp9"
      },
      "source": [
        "# Running all models together\n",
        "# Compare Algorithms\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=700)))\n",
        "models.append(('RF', RandomForestClassifier()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('DT', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(decision_function_shape='ovo', probability=True)))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "score_test = []\n",
        "names = []\n",
        "report_scores_all = []\n",
        "for name, model in models:\n",
        "  #fold_scores, test_score = score_model(model)\n",
        "  report_scores = score_model(model)\n",
        "  report_scores_all.append(report_scores)\n",
        "  #results.append(fold_scores)\n",
        "  #score_test.append(test_score)\n",
        "  results.append(report_scores['accuracy_folds'])\n",
        "  score_test.append(report_scores['accuracy_test'])\n",
        "  names.append(name)\n",
        "  # msg = \"%s: %f (%f)\" % (name, fold_scores.mean(), fold_scores.std())\n",
        "  msg = \"%s: %f (%f)\" % (name, report_scores['accuracy_folds'].mean(), report_scores['accuracy_folds'].std())\n",
        "  print(msg)\n",
        "# boxplot algorithm comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}