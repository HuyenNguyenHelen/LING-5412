{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMcv4/74GX+XtucXqnb5Jr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment1_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoyWL2I4hcNZ"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuKWcjuKGIU"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALIRR75kHO2"
      },
      "source": [
        "# Un\n",
        "my_tar = tarfile.open('/content/lingspam_public.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n",
        "train_path = '/content/lingspam_public/lemm_stop/part1'  # for training      #spams: spmsg*.txt\n",
        "test_path = '/content/lingspam_public/lemm_stop/part10'   # for testing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h8XN_Db61aE",
        "outputId": "868bfbdb-7b67-4cef-8b89-4dcff2d3f08b"
      },
      "source": [
        "# Changing the data format of the dataset\n",
        "def to_dict (path):\n",
        "  data_dict = dict()\n",
        "  data_dict[1] = []   # Spam \n",
        "  data_dict[0] = []   # Not Spam\n",
        "  for file in os.listdir(path):  \n",
        "    doc = open (path + '/'+ file, 'r')\n",
        "    if 'spmsg' in file:\n",
        "      data_dict[1].append(doc.read())\n",
        "    else:\n",
        "      data_dict[0].append(doc.read())\n",
        "  print ('number of spams: {}'.format(len(data_dict[1])))\n",
        "  print ('number of not_spams: {}'.format(len(data_dict[0])))\n",
        "  n_docs = len(os.listdir(path))\n",
        "  return data_dict, n_docs\n",
        "\n",
        "print('training set:')\n",
        "training, n_docs_train = to_dict (train_path)\n",
        "print('number of doc: {}'.format(n_docs_train))\n",
        "\n",
        "print('\\ntesting set:')\n",
        "testing, n_docs_test = to_dict (test_path)\n",
        "print('number of doc: {}'.format(n_docs_test))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set:\n",
            "number of spams: 48\n",
            "number of not_spams: 241\n",
            "number of doc: 289\n",
            "\n",
            "testing set:\n",
            "number of spams: 49\n",
            "number of not_spams: 242\n",
            "number of doc: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36hb6INe7ZA"
      },
      "source": [
        "# Building a Naive Bayes classifier from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vd1ITkgIlk"
      },
      "source": [
        "# Defining some util functions \n",
        "\n",
        "## Defining function to tokenize the documents\n",
        "def tokenizer (doc):\n",
        "  doc = doc.lower() # Lowercase documents\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "## Defining a function to count frequency of words in each class\n",
        "def count_words(data_dict):\n",
        "  tf = {}\n",
        "  tf[0], tf[1] = {}, {}\n",
        "  all_docs = data_dict[0] + data_dict[1]\n",
        "  temp0, temp1 = {},{}\n",
        "  for doc in data_dict[0]:\n",
        "    for token in tokenizer(doc):\n",
        "      temp0[token] = temp0.get(token, 0)+1\n",
        "    tf[0] = temp0\n",
        "  for doc in data_dict[1]: \n",
        "    for token in tokenizer(doc):\n",
        "      temp1[token] = temp1.get(token, 0)+1\n",
        "    tf[1] = temp1\n",
        "  # print('sum of tf0: {}, sum of tf1 {}'. format(sum(tf[0].values()), sum(tf[1].values())))\n",
        "  return tf\n",
        "\n",
        "## Defining a function to store tokens in classes with two different BOWS\n",
        "def to_bow (data_dict):\n",
        "  bows = {}\n",
        "  bows[0], bows[1] = [], []\n",
        "  for doc in data_dict[0]:\n",
        "    bows[0].extend(tokenizer(doc))\n",
        "  for doc in data_dict[1]:\n",
        "    bows[1].extend(tokenizer(doc))\n",
        "  return bows\n",
        "\n",
        "## Defining a function to calculate log prior for each class\n",
        "### Return a dict of classes' log priors\n",
        "def logprior(data_dict, n_docs):\n",
        "  logprior = {}\n",
        "  n_spams = len(data_dict[1])\n",
        "  n_nonspams = len(data_dict[0])\n",
        "  print('length of spams: {}, nonspam: {}'.format(n_spams, n_nonspams))\n",
        "  logprior[1] = np.log(n_spams/n_docs)\n",
        "  logprior[0] = np.log(n_nonspams/n_docs)\n",
        "  return logprior\n",
        "\n",
        "## Defining a function to train the Naive Bayes model \n",
        "### Returning logpriors of classes, a word loglikelihood list , set of vocabulary\n",
        "def train_NB (training, alpha):\n",
        "  bows = to_bow (training) \n",
        "  set_V = set(to_bow (training)[1] + to_bow (training)[0])\n",
        "  tf = count_words(training)\n",
        "  logpriors = logprior(training,n_docs_train)\n",
        "  # calculating loglikelihood for each class\n",
        "  loglikelihood = {}\n",
        "  loglikelihood[0], loglikelihood[1] = {}, {}\n",
        "  for c in training.keys():\n",
        "    bow_c = bows[c]\n",
        "    # print('.....length of bow {}:  {}.......'.format(c, len(bow_c)))\n",
        "    loglikelihood_c = {}\n",
        "    for w in set_V:\n",
        "      if w in tf[c]:\n",
        "        count_w = tf[c][w]  \n",
        "      else:\n",
        "        count_w = 0\n",
        "      # print('count ---{} ---in c: {} '.format(w, count_w))\n",
        "      loglikelihood_w = np.log((count_w + alpha)/(len(bow_c)+(len(set_V)*alpha)))\n",
        "      loglikelihood_c[w] = loglikelihood_w\n",
        "    loglikelihood[c] = loglikelihood_c\n",
        "  return logpriors, loglikelihood, set_V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMiZjikgknL6"
      },
      "source": [
        "# Defining TRAINING and TESTING functions\n",
        "## Defining a function for NaiveBayes testing, returning predicted class\n",
        "def test_NB (testing):\n",
        "  prediction = []\n",
        "  X_test = testing[0]+ testing[1]\n",
        "  # Recalling BOW, log prior of each class, and vocabulary from training\n",
        "  spam_bow = training_result[1][1]\n",
        "  nonspam_bow = training_result[1][0]\n",
        "  spam_logprior = training_result[0][1]\n",
        "  nonspam_logprior = training_result[0][0]\n",
        "  set_V = training_result[2]\n",
        "  \n",
        "  ## Calculating probability of class occurred given trained class priors,\n",
        "  ## and trained word likelihoods\n",
        "  for doc in X_test:\n",
        "    spam_loglikelihoods, nonspam_loglikelihoods = [], []\n",
        "    spam_score=0\n",
        "    nonspam_score = 0\n",
        "    for w in tokenizer(doc):\n",
        "      if w not in set_V: \n",
        "        continue\n",
        "      if w in spam_bow.keys():\n",
        "        spam_score += spam_bow[w]\n",
        "      if w in nonspam_bow.keys():\n",
        "        nonspam_score += nonspam_bow[w]\n",
        "    spam_score += spam_logprior\n",
        "    nonspam_score += nonspam_logprior\n",
        "\n",
        "    ## Determining class by taking class\n",
        "    if spam_score > nonspam_score:\n",
        "      prediction.append(1)\n",
        "    else:\n",
        "      prediction.append(0)\n",
        "  return prediction"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcOl5T4TGHWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b318d3a2-0353-4ad4-8ce6-fc59900ab0e7"
      },
      "source": [
        "# Data formating for using functions built in Sklearn\n",
        "X_train = training[0]+ training[1]\n",
        "y_train = [0]*len(training[0]) + [1]*len(training[1])\n",
        "\n",
        "X_test = testing[0]+ testing[1]\n",
        "y_true = [0]*len(testing[0]) + [1]*len(testing[1])\n",
        "\n",
        "# Training model on the training set\n",
        "training_result = train_NB (training, alpha = 1)\n",
        "\n",
        "# Making prediction\n",
        "y_pred = test_NB (testing)\n",
        "\n",
        "# Printing model performance \n",
        "## on training set\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, test_NB(training))))\n",
        "print(classification_report(y_train, test_NB(training)))\n",
        "\n",
        "## on test set\n",
        "print('\\naccuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of spams: 48, nonspam: 241\n",
            "accuracy on training set: 0.9896193771626297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       241\n",
            "           1       0.94      1.00      0.97        48\n",
            "\n",
            "    accuracy                           0.99       289\n",
            "   macro avg       0.97      0.99      0.98       289\n",
            "weighted avg       0.99      0.99      0.99       289\n",
            "\n",
            "\n",
            "accuracy on test set: 0.9381443298969072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96       242\n",
            "           1       0.75      0.94      0.84        49\n",
            "\n",
            "    accuracy                           0.94       291\n",
            "   macro avg       0.87      0.94      0.90       291\n",
            "weighted avg       0.95      0.94      0.94       291\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrofQiv0fFEv"
      },
      "source": [
        "# Building a Naive Bayes classifier from Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvLBIIvzn9j8",
        "outputId": "9579cedf-2f15-45b3-866a-ed62edca84a4"
      },
      "source": [
        "# Creating a vectorizer model that converts a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer(lowercase = True)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "# Converting  sparse matrix to a dense matrix\n",
        "X_train_vec = X_train_vec.toarray()\n",
        "X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "# Creating NB classifier\n",
        "nb_2 = MultinomialNB()\n",
        "\n",
        "# Fitting the model into the training set for training\n",
        "nb_2.fit(X_train_vec, y_train)\n",
        "\n",
        "# Using NB clssifier to make prediction on test set\n",
        "y_pred_2 = nb_2.predict(X_test_vec)\n",
        "\n",
        "# Printing model performance \n",
        "## on training set\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, nb_2.predict(X_train_vec))))\n",
        "print(classification_report(y_train, nb_2.predict(X_train_vec)))\n",
        "\n",
        "## on test set\n",
        "print('accuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred_2)))\n",
        "print(classification_report(y_true, y_pred_2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training set: 0.9965397923875432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       241\n",
            "           1       0.98      1.00      0.99        48\n",
            "\n",
            "    accuracy                           1.00       289\n",
            "   macro avg       0.99      1.00      0.99       289\n",
            "weighted avg       1.00      1.00      1.00       289\n",
            "\n",
            "accuracy on test set: 0.9896907216494846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       242\n",
            "           1       1.00      0.94      0.97        49\n",
            "\n",
            "    accuracy                           0.99       291\n",
            "   macro avg       0.99      0.97      0.98       291\n",
            "weighted avg       0.99      0.99      0.99       291\n",
            "\n"
          ]
        }
      ]
    }
  ]
}