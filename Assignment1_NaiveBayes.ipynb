{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPm9CRRTny9m1D2axlyb/dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment1_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoyWL2I4hcNZ"
      },
      "source": [
        "# Importing libraries that will be used \n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuKWcjuKGIU"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALIRR75kHO2"
      },
      "source": [
        "# Untar the dataset\n",
        "my_tar = tarfile.open('/content/lingspam_public.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n",
        "train_path = '/content/lingspam_public/lemm_stop/part1'  # for training      #spams: spmsg*.txt\n",
        "test_path = '/content/lingspam_public/lemm_stop/part10'   # for testing"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "0h8XN_Db61aE",
        "outputId": "cc26236b-cab3-4683-8be9-eabb5a8d64be"
      },
      "source": [
        "# Changing the data format of the dataset\n",
        "def to_dict (path):\n",
        "  data_dict = dict()\n",
        "  data_dict[1] = []   # Spam \n",
        "  data_dict[0] = []   # Not Spam\n",
        "  for file in os.listdir(path):  \n",
        "    doc = open (path + '/'+ file, 'r')\n",
        "    if 'spmsg' in file:\n",
        "      data_dict[1].append(doc.read())\n",
        "    else:\n",
        "      data_dict[0].append(doc.read())\n",
        "  print ('number of spams: {}'.format(len(data_dict[1])))\n",
        "  print ('number of not_spams: {}'.format(len(data_dict[0])))\n",
        "  sns.countplot(x=['spam']* len(data_dict[1])+['nonspam']* len(data_dict[0]))\n",
        "  plt.show()  \n",
        "  n_docs = len(os.listdir(path))\n",
        "  return data_dict, n_docs\n",
        "\n",
        "print('training set:')\n",
        "training, n_docs_train = to_dict (train_path)\n",
        "print('number of doc: {}'.format(n_docs_train))\n",
        "\n",
        "print('\\ntesting set:')\n",
        "testing, n_docs_test = to_dict (test_path)\n",
        "print('number of doc: {}'.format(n_docs_test))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set:\n",
            "number of spams: 48\n",
            "number of not_spams: 241\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of doc: 289\n",
            "\n",
            "testing set:\n",
            "number of spams: 49\n",
            "number of not_spams: 242\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of doc: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIBOPmdZlyiR"
      },
      "source": [
        "sns.countplot(x='Label_name', data=df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36hb6INe7ZA"
      },
      "source": [
        "# Building a Naive Bayes classifier from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vd1ITkgIlk"
      },
      "source": [
        "# Defining some util functions \n",
        "\n",
        "## Defining function to tokenize the documents\n",
        "def tokenizer (doc):\n",
        "  doc = doc.lower() # Lowercase documents\n",
        "  return re.split(\"\\W+\", doc)   # return a list of tokens without punctuations\n",
        "\n",
        "## Defining a function to count frequency of words in each class\n",
        "def count_words(data_dict):\n",
        "  tf = {}\n",
        "  tf[0], tf[1] = {}, {}\n",
        "  all_docs = data_dict[0] + data_dict[1]\n",
        "  temp0, temp1 = {},{}\n",
        "  for doc in data_dict[0]:\n",
        "    for token in tokenizer(doc):\n",
        "      temp0[token] = temp0.get(token, 0)+1\n",
        "    tf[0] = temp0\n",
        "  for doc in data_dict[1]: \n",
        "    for token in tokenizer(doc):\n",
        "      temp1[token] = temp1.get(token, 0)+1\n",
        "    tf[1] = temp1\n",
        "  # print('sum of tf0: {}, sum of tf1 {}'. format(sum(tf[0].values()), sum(tf[1].values())))\n",
        "  return tf\n",
        "\n",
        "## Defining a function to store tokens in classes with two different BOWS\n",
        "def to_bow (data_dict):\n",
        "  bows = {}\n",
        "  bows[0], bows[1] = [], []\n",
        "  for doc in data_dict[0]:\n",
        "    bows[0].extend(tokenizer(doc))\n",
        "  for doc in data_dict[1]:\n",
        "    bows[1].extend(tokenizer(doc))\n",
        "  return bows\n",
        "\n",
        "## Defining a function to calculate log prior for each class\n",
        "### Return a dict of classes' log priors\n",
        "def logprior(data_dict, n_docs):\n",
        "  logprior = {}\n",
        "  n_spams = len(data_dict[1])\n",
        "  n_nonspams = len(data_dict[0])\n",
        "  print('length of spams: {}, nonspam: {}'.format(n_spams, n_nonspams))\n",
        "  logprior[1] = np.log(n_spams/n_docs)\n",
        "  logprior[0] = np.log(n_nonspams/n_docs)\n",
        "  return logprior\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMiZjikgknL6"
      },
      "source": [
        "# Defining TRAINING and TESTING functions\n",
        "\n",
        "## Defining a function to train the Naive Bayes model \n",
        "### Returning logpriors of classes, a word loglikelihood list , set of vocabulary\n",
        "def train_NB (training, alpha):\n",
        "  bows = to_bow (training) \n",
        "  set_V = set(to_bow (training)[1] + to_bow (training)[0])\n",
        "  tf = count_words(training)\n",
        "  logpriors = logprior(training,n_docs_train)\n",
        "  # calculating loglikelihood for each class\n",
        "  loglikelihood = {}\n",
        "  loglikelihood[0], loglikelihood[1] = {}, {}\n",
        "  for c in training.keys():\n",
        "    bow_c = bows[c]\n",
        "    # print('.....length of bow {}:  {}.......'.format(c, len(bow_c)))\n",
        "    loglikelihood_c = {}\n",
        "    for w in set_V:\n",
        "      if w in tf[c]:\n",
        "        count_w = tf[c][w]  \n",
        "      else:\n",
        "        count_w = 0\n",
        "      # print('count ---{} ---in c: {} '.format(w, count_w))\n",
        "      loglikelihood_w = np.log((count_w + alpha)/(len(bow_c)+(len(set_V)*alpha)))\n",
        "      loglikelihood_c[w] = loglikelihood_w\n",
        "    loglikelihood[c] = loglikelihood_c\n",
        "  return logpriors, loglikelihood, set_V\n",
        "\n",
        "\n",
        "## Defining a function for NaiveBayes testing, returning predicted class\n",
        "def test_NB (testing):\n",
        "  prediction = []\n",
        "  X_test = testing[0]+ testing[1]\n",
        "  # Recalling BOW, log prior of each class, and vocabulary from training\n",
        "  spam_bow = training_result[1][1]\n",
        "  nonspam_bow = training_result[1][0]\n",
        "  spam_logprior = training_result[0][1]\n",
        "  nonspam_logprior = training_result[0][0]\n",
        "  set_V = training_result[2]\n",
        "  \n",
        "  ## Calculating probability of class occurred given trained class priors,\n",
        "  ## and trained word likelihoods\n",
        "  for doc in X_test:\n",
        "    spam_loglikelihoods, nonspam_loglikelihoods = [], []\n",
        "    spam_score=0\n",
        "    nonspam_score = 0\n",
        "    for w in tokenizer(doc):\n",
        "      if w not in set_V: \n",
        "        continue\n",
        "      if w in spam_bow.keys():\n",
        "        spam_score += spam_bow[w]\n",
        "      if w in nonspam_bow.keys():\n",
        "        nonspam_score += nonspam_bow[w]\n",
        "    spam_score += spam_logprior\n",
        "    nonspam_score += nonspam_logprior\n",
        "\n",
        "    ## Determining class by taking class\n",
        "    if spam_score > nonspam_score:\n",
        "      prediction.append(1)\n",
        "    else:\n",
        "      prediction.append(0)\n",
        "  return prediction"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcOl5T4TGHWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b318d3a2-0353-4ad4-8ce6-fc59900ab0e7"
      },
      "source": [
        "# Data formating for using functions built in Sklearn\n",
        "X_train = training[0]+ training[1]\n",
        "y_train = [0]*len(training[0]) + [1]*len(training[1])\n",
        "\n",
        "X_test = testing[0]+ testing[1]\n",
        "y_true = [0]*len(testing[0]) + [1]*len(testing[1])\n",
        "\n",
        "# Training model on the training set\n",
        "training_result = train_NB (training, alpha = 1)\n",
        "\n",
        "# Making prediction\n",
        "y_pred = test_NB (testing)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of spams: 48, nonspam: 241\n",
            "accuracy on training set: 0.9896193771626297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       241\n",
            "           1       0.94      1.00      0.97        48\n",
            "\n",
            "    accuracy                           0.99       289\n",
            "   macro avg       0.97      0.99      0.98       289\n",
            "weighted avg       0.99      0.99      0.99       289\n",
            "\n",
            "\n",
            "accuracy on test set: 0.9381443298969072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96       242\n",
            "           1       0.75      0.94      0.84        49\n",
            "\n",
            "    accuracy                           0.94       291\n",
            "   macro avg       0.87      0.94      0.90       291\n",
            "weighted avg       0.95      0.94      0.94       291\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQu-1F9VsVsr",
        "outputId": "963cdcb0-5983-4ac4-c8ad-657f5be69594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Printing model performance \n",
        "def printing_eval_scores (y_true, y_pred):\n",
        "  print('accuracy score: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "  print('precision score: {}'.format(sklearn.metrics.precision_score(y_true, y_pred)))\n",
        "  print('recall score: {}'.format(sklearn.metrics.recall_score(y_true, y_pred)))\n",
        "  print('F1 score: {}'.format(sklearn.metrics.f1_score(y_true, y_pred)))\n",
        "  print(classification_report(y_true, y_pred))\n",
        "\n",
        "## on training set\n",
        "print('Model performance on training set:')\n",
        "printing_eval_scores (y_train, test_NB(training))\n",
        "\n",
        "## on test set\n",
        "print('--------------------------------')\n",
        "print('\\nModel performance on test set:')\n",
        "printing_eval_scores (y_true, y_pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance on training set:\n",
            "accuracy score: 0.9896193771626297\n",
            "precision score: 0.9411764705882353\n",
            "recall score: 1.0\n",
            "F1 score: 0.9696969696969697\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       241\n",
            "           1       0.94      1.00      0.97        48\n",
            "\n",
            "    accuracy                           0.99       289\n",
            "   macro avg       0.97      0.99      0.98       289\n",
            "weighted avg       0.99      0.99      0.99       289\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Model performance on test set:\n",
            "accuracy score: 0.9381443298969072\n",
            "precision score: 0.7540983606557377\n",
            "recall score: 0.9387755102040817\n",
            "F1 score: 0.8363636363636363\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96       242\n",
            "           1       0.75      0.94      0.84        49\n",
            "\n",
            "    accuracy                           0.94       291\n",
            "   macro avg       0.87      0.94      0.90       291\n",
            "weighted avg       0.95      0.94      0.94       291\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrofQiv0fFEv"
      },
      "source": [
        "# Building a Naive Bayes classifier from Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvLBIIvzn9j8",
        "outputId": "9579cedf-2f15-45b3-866a-ed62edca84a4"
      },
      "source": [
        "# Creating a vectorizer model that converts a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer(lowercase = True)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "# Converting  sparse matrix to a dense matrix\n",
        "X_train_vec = X_train_vec.toarray()\n",
        "X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "# Creating NB classifier\n",
        "nb_2 = MultinomialNB()\n",
        "\n",
        "# Fitting the model into the training set for training\n",
        "nb_2.fit(X_train_vec, y_train)\n",
        "\n",
        "# Using NB clssifier to make prediction on test set\n",
        "y_pred_2 = nb_2.predict(X_test_vec)\n",
        "\n",
        "# Printing model performance \n",
        "## on training set\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, nb_2.predict(X_train_vec))))\n",
        "print(classification_report(y_train, nb_2.predict(X_train_vec)))\n",
        "\n",
        "## on test set\n",
        "print('accuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred_2)))\n",
        "print(classification_report(y_true, y_pred_2))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training set: 0.9965397923875432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       241\n",
            "           1       0.98      1.00      0.99        48\n",
            "\n",
            "    accuracy                           1.00       289\n",
            "   macro avg       0.99      1.00      0.99       289\n",
            "weighted avg       1.00      1.00      1.00       289\n",
            "\n",
            "accuracy on test set: 0.9896907216494846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       242\n",
            "           1       1.00      0.94      0.97        49\n",
            "\n",
            "    accuracy                           0.99       291\n",
            "   macro avg       0.99      0.97      0.98       291\n",
            "weighted avg       0.99      0.99      0.99       291\n",
            "\n"
          ]
        }
      ]
    }
  ]
}