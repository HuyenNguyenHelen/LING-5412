{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZA26CqKFh2VZjYgjAQrSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment1_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuKWcjuKGIU"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoyWL2I4hcNZ"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALIRR75kHO2"
      },
      "source": [
        "import tarfile\n",
        "my_tar = tarfile.open('/content/lingspam_public.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMiZjikgknL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacab070-8926-4c93-9afb-80367c242075"
      },
      "source": [
        "import os\n",
        "train_path = '/content/lingspam_public/lemm_stop/part1'  # for training      #spams: spmsg*.txt\n",
        "test_path = '/content/lingspam_public/lemm_stop/part1'   # for testing\n",
        "\n",
        "def to_dict (path):\n",
        "  data_dict = dict()\n",
        "  data_dict[1] = []\n",
        "  data_dict[0] = []\n",
        "  for file in os.listdir(path):  \n",
        "    doc = open (path + '/'+ file, 'r')\n",
        "    if 'spmsg' in file:\n",
        "      data_dict[1].append(doc.read())\n",
        "    else:\n",
        "      data_dict[0].append(doc.read())\n",
        "  print ('number of spams: {}'.format(len(data_dict[1])))\n",
        "  print ('number of not_spams: {}'.format(len(data_dict[0])))\n",
        "  n_docs = len(os.listdir(path))\n",
        "  return data_dict, n_docs\n",
        "\n",
        "# def to_bow (doc, class):\n",
        "#   wbag[class] = []\n",
        "#   for token in doc.split():\n",
        "#     wbag.append(token)\n",
        "#   return wbag\n",
        "\n",
        "def count_words(data_dict):\n",
        "  tf = {}\n",
        "  tf[0], tf[1] = {}, {}\n",
        "  all_docs = data_dict[0] + data_dict[1]\n",
        "  print('herrrrrrrrrrrrrrreeeee', len(all_docs))\n",
        "  temp0, temp1 = {},{}\n",
        "  for doc in data_dict[0]:\n",
        "    for token in doc.split():\n",
        "      temp0[token] = temp0.get(token, 0)+1\n",
        "    tf[0] = temp0\n",
        "  for doc in data_dict[1]: \n",
        "    for token in doc.split():\n",
        "      temp1[token] = temp1.get(token, 0)+1\n",
        "    tf[1] = temp1\n",
        "  return tf\n",
        "\n",
        "def to_bow (data_dict):\n",
        "  bows = {}\n",
        "  bows[0], bows[1] = [], []\n",
        "  for doc in data_dict[0]:\n",
        "    bows[0].extend(doc.split())\n",
        "  for doc in data_dict[1]:\n",
        "    bows[1].extend(doc.split())\n",
        "  return bows\n",
        "\n",
        "def logprior(data_dict, n_docs):\n",
        "  logprior = {}\n",
        "  n_spams = len(data_dict[1])\n",
        "  n_nonspams = len(data_dict[0])\n",
        "  logprior[1] = np.log(n_docs/n_spams)\n",
        "  logprior[0] = np.log(n_docs/n_nonspams)\n",
        "  return logprior\n",
        "\n",
        "# spam_bow = to_bow (training)[1]\n",
        "# nonspam_bow = to_bow (training)[0]\n",
        "# tf_spam = count_words(training)[1]\n",
        "# tf_nonspam = count_words(training)[0]\n",
        "# logprior_spam = logprior(training, n_docs_train)[1]\n",
        "# logprior_notspam = logprior(training, n_docs_train)[0]\n",
        "\n",
        "def train_NB (train_path):\n",
        "  training, n_docs_train = to_dict (train_path)\n",
        "  bows = to_bow (training)\n",
        "  set_V = set(to_bow (training)[1] + to_bow (training)[0])\n",
        "  tf = count_words(training)\n",
        "  logpriors = logprior(training,n_docs_train)\n",
        "  loglikelihood = {}\n",
        "  loglikelihood[0], loglikelihood[1] = {}, {}\n",
        "  for c in training.keys():\n",
        "    # n_c = len(training[c])\n",
        "    # logprior_c = np.log(n_c/n_docs_train)\n",
        "    bow_c = bows[c]\n",
        "    loglikelihood_c = {}\n",
        "    for w in set_V:\n",
        "      if w in tf[c]:\n",
        "        count_w = tf[c][w]\n",
        "        #print(count_w)\n",
        "      else:\n",
        "        count_w = 0\n",
        "      loglikelihood_w = np.log((count_w + 1)/(len(bow_c)+1))\n",
        "      loglikelihood_c[w] = loglikelihood_w\n",
        "    loglikelihood[c] = loglikelihood_c\n",
        "  return logpriors, loglikelihood, set_V\n",
        "\n",
        "training_result = train_NB (train_path)\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of spams: 48\n",
            "number of not_spams: 241\n",
            "herrrrrrrrrrrrrrreeeee 289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcOl5T4TGHWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a9c424-807e-4ebd-9da0-dd14c281b9e7"
      },
      "source": [
        "testing, n_docs_test = to_dict (test_path)\n",
        "\n",
        "X_test = testing[0]+ testing[1]\n",
        "y_true = [0]*len(testing[0]) + [1]*len(testing[1])\n",
        "def test_NB (test_path):\n",
        "  logpriors= logprior(testing,n_docs_test)\n",
        "  prediction = []\n",
        "  X_test = testing[0]+ testing[1]\n",
        "  for doc in X_test:\n",
        "    spam_loglikelihoods, nonspam_loglikelihoods = [], []\n",
        "    spam_score=0\n",
        "    nonspam_score = 0\n",
        "    for w in doc.split():\n",
        "      if w not in training_result[2]: continue\n",
        "      spam_loglikelihoods.append(training_result[1][1][w] if w in training_result[1][1].keys() else 0)\n",
        "      nonspam_loglikelihoods.append(training_result[1][0][w] if w in training_result[1][0].keys() else 0)\n",
        "    spam_score += sum(spam_loglikelihoods)\n",
        "    nonspam_score += sum(nonspam_loglikelihoods)\n",
        "    spam_score += logpriors [1]\n",
        "    nonspam_score += logpriors [0]\n",
        "    if spam_score > nonspam_score:\n",
        "      prediction.append(1)\n",
        "    else:\n",
        "      prediction.append(0)\n",
        "  return prediction\n",
        "\n",
        "y_pred = test_NB (test_path)\n",
        "print(y_true)\n",
        "print(y_pred)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of spams: 48\n",
            "number of not_spams: 241\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6uw1u7SLm96",
        "outputId": "3100e792-4735-4252-bbb4-5f3abd43ef1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('accuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on test set: 0.9826989619377162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyKEys7pRn4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec327fd-190a-4a29-99a2-23eb024c5b6e"
      },
      "source": [
        "# Testing\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18162975462177713"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fql9wDB8Dx4v"
      },
      "source": [
        "def fitNB (X, y):\n",
        "\n",
        "def predict (test_set):\n",
        "  for doc in test_set:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8CKxGiSmHvl"
      },
      "source": [
        "\n",
        "class NaiveBayes:\n",
        "    \n",
        "    def __init__(self,unique_classes):\n",
        "        \n",
        "        self.classes=unique_classes # Constructor is sinply passed with unique number of classes of the training set\n",
        "        \n",
        "\n",
        "    def addToBow(self,example,dict_index):\n",
        "        \n",
        "        '''\n",
        "            Parameters:\n",
        "            1. example \n",
        "            2. dict_index - implies to which BoW category this example belongs to\n",
        "            What the function does?\n",
        "            -----------------------\n",
        "            It simply splits the example on the basis of space as a tokenizer and adds every tokenized word to\n",
        "            its corresponding dictionary/BoW\n",
        "            Returns:\n",
        "            ---------\n",
        "            Nothing\n",
        "        \n",
        "       '''\n",
        "        \n",
        "        if isinstance(example,np.ndarray): example=example[0]\n",
        "     \n",
        "        for token_word in example.split(): #for every word in preprocessed example\n",
        "          \n",
        "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
        "            \n",
        "    def train(self,dataset,labels):\n",
        "        \n",
        "        '''\n",
        "            Parameters:\n",
        "            1. dataset - shape = (m X d)\n",
        "            2. labels - shape = (m,)\n",
        "            What the function does?\n",
        "            -----------------------\n",
        "            This is the training function which will train the Naive Bayes Model i.e compute a BoW for each\n",
        "            category/class. \n",
        "            Returns:\n",
        "            ---------\n",
        "            Nothing\n",
        "        \n",
        "        '''\n",
        "    \n",
        "        self.examples=dataset\n",
        "        self.labels=labels\n",
        "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
        "        \n",
        "        #only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
        "        \n",
        "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
        "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
        "            \n",
        "        #constructing BoW for each category\n",
        "        for cat_index,cat in enumerate(self.classes):\n",
        "          \n",
        "            all_cat_examples=self.examples[self.labels==cat] #filter all examples of category == cat\n",
        "            \n",
        "            #get examples preprocessed\n",
        "            \n",
        "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
        "            \n",
        "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
        "            \n",
        "            #now costruct BoW of this particular category\n",
        "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
        "            \n",
        "                \n",
        "        ###################################################################################################\n",
        "        \n",
        "        '''\n",
        "            Although we are done with the training of Naive Bayes Model BUT!!!!!!\n",
        "            ------------------------------------------------------------------------------------\n",
        "            Remember The Test Time Forumla ? : {for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ] } * p(c)\n",
        "            ------------------------------------------------------------------------------------\n",
        "            \n",
        "            We are done with constructing of BoW for each category. But we need to precompute a few \n",
        "            other calculations at training time too:\n",
        "            1. prior probability of each class - p(c)\n",
        "            2. vocabulary |V| \n",
        "            3. denominator value of each class - [ count(c) + |V| + 1 ] \n",
        "            \n",
        "            Reason for doing this precomputing calculations stuff ???\n",
        "            ---------------------\n",
        "            We can do all these 3 calculations at test time too BUT doing so means to re-compute these \n",
        "            again and again every time the test function will be called - this would significantly\n",
        "            increase the computation time especially when we have a lot of test examples to classify!!!).  \n",
        "            And moreover, it doensot make sense to repeatedly compute the same thing - \n",
        "            why do extra computations ???\n",
        "            So we will precompute all of them & use them during test time to speed up predictions.\n",
        "            \n",
        "        '''\n",
        "        \n",
        "        ###################################################################################################\n",
        "      \n",
        "        prob_classes=np.empty(self.classes.shape[0])\n",
        "        all_words=[]\n",
        "        cat_word_counts=np.empty(self.classes.shape[0])\n",
        "        for cat_index,cat in enumerate(self.classes):\n",
        "           \n",
        "            #Calculating prior probability p(c) for each class\n",
        "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
        "            \n",
        "            #Calculating total counts of all the words of each class \n",
        "            count=list(self.bow_dicts[cat_index].values())\n",
        "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
        "            \n",
        "            #get all words of this category                                \n",
        "            all_words+=self.bow_dicts[cat_index].keys()\n",
        "                                                     \n",
        "        \n",
        "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
        "        \n",
        "        self.vocab=np.unique(np.array(all_words))\n",
        "        self.vocab_length=self.vocab.shape[0]\n",
        "                                  \n",
        "        #computing denominator value                                      \n",
        "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
        "      \n",
        "        '''\n",
        "            Now that we have everything precomputed as well, its better to organize everything in a tuple \n",
        "            rather than to have a separate list for every thing.\n",
        "            \n",
        "            Every element of self.cats_info has a tuple of values\n",
        "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
        "        '''\n",
        "        \n",
        "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
        "        self.cats_info=np.array(self.cats_info)                                 \n",
        "                                              \n",
        "                                              \n",
        "    def getExampleProb(self,test_example):                                \n",
        "        \n",
        "        '''\n",
        "            Parameters:\n",
        "            -----------\n",
        "            1. a single test example \n",
        "            What the function does?\n",
        "            -----------------------\n",
        "            Function that estimates posterior probability of the given test example\n",
        "            Returns:\n",
        "            ---------\n",
        "            probability of test example in ALL CLASSES\n",
        "        '''                                      \n",
        "                                              \n",
        "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
        "        \n",
        "        #finding probability w.r.t each class of the given test example\n",
        "        for cat_index,cat in enumerate(self.classes): \n",
        "                             \n",
        "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
        "                \n",
        "                ####################################################################################\n",
        "                                              \n",
        "                #This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]                               \n",
        "                                              \n",
        "                ####################################################################################                              \n",
        "                \n",
        "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
        "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
        "                \n",
        "                #now get likelihood of this test_token word                              \n",
        "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
        "                \n",
        "                #remember why taking log? To prevent underflow!\n",
        "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
        "                                              \n",
        "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
        "        post_prob=np.empty(self.classes.shape[0])\n",
        "        for cat_index,cat in enumerate(self.classes):\n",
        "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
        "      \n",
        "        return post_prob\n",
        "    \n",
        "   \n",
        "    def test(self,test_set):\n",
        "      \n",
        "        '''\n",
        "            Parameters:\n",
        "            -----------\n",
        "            1. A complete test set of shape (m,)\n",
        "            \n",
        "            What the function does?\n",
        "            -----------------------\n",
        "            Determines probability of each test example against all classes and predicts the label\n",
        "            against which the class probability is maximum\n",
        "            Returns:\n",
        "            ---------\n",
        "            Predictions of test examples - A single prediction against every test example\n",
        "        '''       \n",
        "       \n",
        "        predictions=[] #to store prediction of each test example\n",
        "        for example in test_set: \n",
        "                                              \n",
        "            #preprocess the test example the same way we did for training set exampels                                  \n",
        "            cleaned_example=preprocess_string(example) \n",
        "             \n",
        "            #simply get the posterior probability of every example                                  \n",
        "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
        "            \n",
        "            #simply pick the max value and map against self.classes!\n",
        "            predictions.append(self.classes[np.argmax(post_prob)])\n",
        "                \n",
        "        return np.array(predictions) \n",
        "view rawNB_class.py hosted with ‚ù§ by GitHub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "KvLBIIvzn9j8",
        "outputId": "89eb011a-d496-4997-dbde-4b9636c24d3e"
      },
      "source": [
        "sentence = 'ajccdkckc ojcdojdco oef9ff ojsosso fofekd ihijso ihis hisj hios'\n",
        "bow_dicts = dict()\n",
        "for token in sentence.split():\n",
        "  bow_dicts['spam'][token]+=1\n",
        "bow_dicts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-017e4adeb98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbow_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mbow_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbow_dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'spam'"
          ]
        }
      ]
    }
  ]
}