{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_NaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiY8DiDnUbWIXxjmGYuZ/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment1_NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuKWcjuKGIU"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoyWL2I4hcNZ"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ALIRR75kHO2"
      },
      "source": [
        "my_tar = tarfile.open('/content/lingspam_public.tar.gz')\n",
        "my_tar.extractall('/content/') \n",
        "my_tar.close()\n",
        "train_path = '/content/lingspam_public/lemm_stop/part1'  # for training      #spams: spmsg*.txt\n",
        "test_path = '/content/lingspam_public/lemm_stop/part10'   # for testing"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h8XN_Db61aE",
        "outputId": "fa7e7247-8841-4af8-cd00-9760a6bd94d3"
      },
      "source": [
        "def to_dict (path):\n",
        "  data_dict = dict()\n",
        "  data_dict[1] = []\n",
        "  data_dict[0] = []\n",
        "  for file in os.listdir(path):  \n",
        "    doc = open (path + '/'+ file, 'r')\n",
        "    if 'spmsg' in file:\n",
        "      data_dict[1].append(doc.read())\n",
        "    else:\n",
        "      data_dict[0].append(doc.read())\n",
        "  print ('number of spams: {}'.format(len(data_dict[1])))\n",
        "  print ('number of not_spams: {}'.format(len(data_dict[0])))\n",
        "  n_docs = len(os.listdir(path))\n",
        "  return data_dict, n_docs\n",
        "\n",
        "print('training set:')\n",
        "training, n_docs_train = to_dict (train_path)\n",
        "print('number of doc: {}'.format(n_docs_train))\n",
        "\n",
        "print('\\ntesting set:')\n",
        "testing, n_docs_test = to_dict (test_path)\n",
        "print('number of doc: {}'.format(n_docs_test))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set:\n",
            "number of spams: 48\n",
            "number of not_spams: 241\n",
            "number of doc: 289\n",
            "\n",
            "testing set:\n",
            "number of spams: 49\n",
            "number of not_spams: 242\n",
            "number of doc: 291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMiZjikgknL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce8ab2a-bdc6-4c52-8588-e4b6c266e87a"
      },
      "source": [
        "def tokenizer (doc):\n",
        "  doc = doc.lower()\n",
        "  doc_clean = doc.lower()\n",
        "  return re.split(\"\\W+\", doc)\n",
        "  \n",
        "def count_words(data_dict):\n",
        "  tf = {}\n",
        "  tf[0], tf[1] = {}, {}\n",
        "  all_docs = data_dict[0] + data_dict[1]\n",
        "  temp0, temp1 = {},{}\n",
        "  for doc in data_dict[0]:\n",
        "    for token in tokenizer(doc):\n",
        "      temp0[token] = temp0.get(token, 0)+1\n",
        "    tf[0] = temp0\n",
        "  for doc in data_dict[1]: \n",
        "    for token in tokenizer(doc):\n",
        "      temp1[token] = temp1.get(token, 0)+1\n",
        "    tf[1] = temp1\n",
        "  print('sum of tf0: {}, sum of tf1 {}'. format(sum(tf[0].values()), sum(tf[1].values())))\n",
        "  return tf\n",
        "\n",
        "def to_bow (data_dict):\n",
        "  bows = {}\n",
        "  bows[0], bows[1] = [], []\n",
        "  for doc in data_dict[0]:\n",
        "    bows[0].extend(tokenizer(doc))\n",
        "  for doc in data_dict[1]:\n",
        "    bows[1].extend(tokenizer(doc))\n",
        "  return bows\n",
        "\n",
        "def logprior(data_dict, n_docs):\n",
        "  logprior = {}\n",
        "  n_spams = len(data_dict[1])\n",
        "  n_nonspams = len(data_dict[0])\n",
        "  print('length of spams: {}, nonspam: {}'.format(n_spams, n_nonspams))\n",
        "  logprior[1] = np.log(n_spams/n_docs)\n",
        "  logprior[0] = np.log(n_nonspams/n_docs)\n",
        "  return logprior\n",
        "\n",
        "\n",
        "def train_NB (training, alpha):\n",
        "  bows = to_bow (training)\n",
        "  set_V = set(to_bow (training)[1] + to_bow (training)[0])\n",
        "  tf = count_words(training)\n",
        "  logpriors = logprior(training,n_docs_train)\n",
        "  loglikelihood = {}\n",
        "  loglikelihood[0], loglikelihood[1] = {}, {}\n",
        "  for c in training.keys():\n",
        "    bow_c = bows[c]\n",
        "    print('.....length of bow {}:  {}.......'.format(c, len(bow_c)))\n",
        "    loglikelihood_c = {}\n",
        "    for w in set_V:\n",
        "      if w in tf[c]:\n",
        "        count_w = tf[c][w]  \n",
        "      else:\n",
        "        count_w = 0\n",
        "      # print('count ---{} ---in c: {} '.format(w, count_w))\n",
        "      loglikelihood_w = np.log((count_w + alpha)/(len(bow_c)+(len(set_V)*alpha)))\n",
        "      loglikelihood_c[w] = loglikelihood_w\n",
        "    loglikelihood[c] = loglikelihood_c\n",
        "  print('length of set V: {}'. format(len(set_V)))\n",
        "  print('log prior class {}: {}'. format(0, logpriors[0]))\n",
        "  print('log prior class {}: {}'. format(1, logpriors[1]))\n",
        "\n",
        "  return logpriors, loglikelihood, set_V\n",
        "\n",
        "training_result = train_NB (training, alpha = 1)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum of tf0: 47459, sum of tf1 21015\n",
            "length of spams: 48, nonspam: 241\n",
            ".....length of bow 1:  21015.......\n",
            ".....length of bow 0:  47459.......\n",
            "length of set V: 11315\n",
            "log prior class 0: -0.18162975462177716\n",
            "log prior class 1: -1.7952256772045412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcOl5T4TGHWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3d2458-2cc4-4e6f-bb96-cfd0d22895ae"
      },
      "source": [
        "def test_NB (testing):\n",
        "  # logpriors= logprior(testing,n_docs_test)\n",
        "  prediction = []\n",
        "  X_test = testing[0]+ testing[1]\n",
        "  for doc in X_test:\n",
        "    spam_loglikelihoods, nonspam_loglikelihoods = [], []\n",
        "    spam_score=0\n",
        "    nonspam_score = 0\n",
        "    spam_bow = training_result[1][1]\n",
        "    nonspam_bow = training_result[1][0]\n",
        "    spam_logprior = training_result[0][1]\n",
        "    nonspam_logprior = training_result[0][0]\n",
        "    set_V = training_result[2]\n",
        "    for w in tokenizer(doc):\n",
        "      if w not in set_V: \n",
        "        continue\n",
        "      if w in spam_bow.keys():\n",
        "        spam_score += spam_bow[w]\n",
        "      if w in nonspam_bow.keys():\n",
        "        nonspam_score += nonspam_bow[w]\n",
        "        \n",
        "    spam_score += spam_logprior\n",
        "    nonspam_score += nonspam_logprior\n",
        "\n",
        "    if spam_score > nonspam_score:\n",
        "      prediction.append(1)\n",
        "    else:\n",
        "      prediction.append(0)\n",
        "  return prediction\n",
        "\n",
        "X_train = training[0]+ training[1]\n",
        "y_train = [0]*len(training[0]) + [1]*len(training[1])\n",
        "\n",
        "X_test = testing[0]+ testing[1]\n",
        "y_true = [0]*len(testing[0]) + [1]*len(testing[1])\n",
        "\n",
        "y_pred = test_NB (testing)\n",
        "\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, test_NB(training))))\n",
        "print(classification_report(y_train, test_NB(training)))\n",
        "\n",
        "print('accuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred)))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training set: 0.9896193771626297\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       241\n",
            "           1       0.94      1.00      0.97        48\n",
            "\n",
            "    accuracy                           0.99       289\n",
            "   macro avg       0.97      0.99      0.98       289\n",
            "weighted avg       0.99      0.99      0.99       289\n",
            "\n",
            "accuracy on test set: 0.9381443298969072\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96       242\n",
            "           1       0.75      0.94      0.84        49\n",
            "\n",
            "    accuracy                           0.94       291\n",
            "   macro avg       0.87      0.94      0.90       291\n",
            "weighted avg       0.95      0.94      0.94       291\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvLBIIvzn9j8",
        "outputId": "961243e8-253d-40d4-a6a8-211795695a63"
      },
      "source": [
        "\n",
        "\n",
        "X_train = training[0]+ training[1]\n",
        "y_train= [0]*len(training[0]) + [1]*len(training[1])\n",
        "\n",
        "# Creating a vectorizer model that convert a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer(lowercase = False)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "# Converting  sparse matrix to a dense matrix\n",
        "X_train_vec = X_train_vec.toarray()\n",
        "X_test_vec = X_test_vec.toarray()\n",
        "# y_train = y_train.reshape(y_train.shape[0],-1)\n",
        "\n",
        "nb_2 = MultinomialNB()\n",
        "nb_2.fit(X_train_vec, y_train)\n",
        "y_pred_2 = nb_2.predict(X_test_vec)\n",
        "\n",
        "print('accuracy on training set: {}'.format(sklearn.metrics.accuracy_score(y_train, nb_2.predict(X_train_vec))))\n",
        "print(classification_report(y_train, nb_2.predict(X_train_vec)))\n",
        "\n",
        "print('accuracy on test set: {}'.format(sklearn.metrics.accuracy_score(y_true, y_pred_2)))\n",
        "print(classification_report(y_true, y_pred_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on training set: 0.9965397923875432\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       241\n",
            "           1       0.98      1.00      0.99        48\n",
            "\n",
            "    accuracy                           1.00       289\n",
            "   macro avg       0.99      1.00      0.99       289\n",
            "weighted avg       1.00      1.00      1.00       289\n",
            "\n",
            "accuracy on test set: 0.9896907216494846\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       242\n",
            "           1       1.00      0.94      0.97        49\n",
            "\n",
            "    accuracy                           0.99       291\n",
            "   macro avg       0.99      0.97      0.98       291\n",
            "weighted avg       0.99      0.99      0.99       291\n",
            "\n"
          ]
        }
      ]
    }
  ]
}