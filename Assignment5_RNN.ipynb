{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeZEFJaVvBxcBplMF0oBl8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment5_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1MLrmUyHKE"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtFRj7_akl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611b2f38-ef10-4d7f-f55e-34ff8f518c65"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYaW1wJqzwev"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6Tc5GAz2dG",
        "outputId": "21c86e5b-949e-4b6e-ce5d-b83aa285de87"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete6X169B/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete6X169B/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete6X169B/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWUJmFLu5Ehl",
        "outputId": "5e5a68b4-6ec4-49bf-8a9f-da2f75e23b09"
      },
      "source": [
        "info"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n5pIYyEYsgN"
      },
      "source": [
        "# Shuffling the dataset\n",
        "buffer_size = 10000\n",
        "batch_size = 64\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1JaB5g6Y4F"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rZ1n_fJ64hL"
      },
      "source": [
        "## Representing the  text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tBy-8US6XxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993f5e6f-8ad9-49d9-ed36-d8674101c70f"
      },
      "source": [
        "vocab_size = 10000\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "# Store vocabulary\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uDkzuD-ZMG"
      },
      "source": [
        "## Building the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAqyl7gHzmS"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='binary')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='binary')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='binary')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNH5dRsqgOCt"
      },
      "source": [
        "### With different embedding sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH0TtCkH5Xkh",
        "outputId": "9ffeb3a0-f197-4964-86c4-4878e2fa8560",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Creating the model\n",
        "embedding_sizes = [32,64,128]\n",
        "for size in embedding_sizes:\n",
        "  print (\"\\n========= embedding vectors'size= %s ============\" %size)\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = size,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = 10, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  # print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 109s 251ms/step - loss: 0.6309 - accuracy: 0.6450 - val_loss: 0.4827 - val_accuracy: 0.7969\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 94s 238ms/step - loss: 0.3926 - accuracy: 0.8348 - val_loss: 0.4405 - val_accuracy: 0.8016\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.3033 - accuracy: 0.8807 - val_loss: 0.3374 - val_accuracy: 0.8708\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 95s 239ms/step - loss: 0.2338 - accuracy: 0.9110 - val_loss: 0.3155 - val_accuracy: 0.8708\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.1947 - accuracy: 0.9294 - val_loss: 0.3460 - val_accuracy: 0.8776\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 93s 234ms/step - loss: 0.1676 - accuracy: 0.9414 - val_loss: 0.3469 - val_accuracy: 0.8734\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.1411 - accuracy: 0.9526 - val_loss: 0.3368 - val_accuracy: 0.8755\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.1228 - accuracy: 0.9600 - val_loss: 0.3893 - val_accuracy: 0.8708\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.1082 - accuracy: 0.9653 - val_loss: 0.3901 - val_accuracy: 0.8719\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0935 - accuracy: 0.9727 - val_loss: 0.4231 - val_accuracy: 0.8703\n",
            "391/391 [==============================] - 49s 124ms/step - loss: 0.4096 - accuracy: 0.8652\n",
            "Test loss:  0.4095645844936371\n",
            "Test acurracy:  0.8651599884033203\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 108s 249ms/step - loss: 0.5886 - accuracy: 0.6764 - val_loss: 0.4114 - val_accuracy: 0.8297\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.2981 - accuracy: 0.8820 - val_loss: 0.3738 - val_accuracy: 0.8458\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.2180 - accuracy: 0.9168 - val_loss: 0.3047 - val_accuracy: 0.8797\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.1769 - accuracy: 0.9352 - val_loss: 0.3190 - val_accuracy: 0.8807\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.1461 - accuracy: 0.9496 - val_loss: 0.3580 - val_accuracy: 0.8766\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.1216 - accuracy: 0.9583 - val_loss: 0.4192 - val_accuracy: 0.8724\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.1076 - accuracy: 0.9647 - val_loss: 0.4267 - val_accuracy: 0.8641\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0911 - accuracy: 0.9724 - val_loss: 0.4758 - val_accuracy: 0.8557\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.0883 - accuracy: 0.9722 - val_loss: 0.4287 - val_accuracy: 0.8552\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.4927 - val_accuracy: 0.8526\n",
            "391/391 [==============================] - 48s 122ms/step - loss: 0.4873 - accuracy: 0.8572\n",
            "Test loss:  0.4872962236404419\n",
            "Test acurracy:  0.8571599721908569\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 108s 251ms/step - loss: 0.5648 - accuracy: 0.7047 - val_loss: 0.3737 - val_accuracy: 0.8479\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 96s 242ms/step - loss: 0.2858 - accuracy: 0.8881 - val_loss: 0.3024 - val_accuracy: 0.8781\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.2061 - accuracy: 0.9232 - val_loss: 0.2948 - val_accuracy: 0.8833\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 96s 242ms/step - loss: 0.1650 - accuracy: 0.9405 - val_loss: 0.3260 - val_accuracy: 0.8760\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.1339 - accuracy: 0.9559 - val_loss: 0.3356 - val_accuracy: 0.8818\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.1153 - accuracy: 0.9636 - val_loss: 0.3872 - val_accuracy: 0.8641\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.0974 - accuracy: 0.9696 - val_loss: 0.4019 - val_accuracy: 0.8698\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.0806 - accuracy: 0.9773 - val_loss: 0.4272 - val_accuracy: 0.8672\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.0691 - accuracy: 0.9808 - val_loss: 0.4887 - val_accuracy: 0.8510\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.0644 - accuracy: 0.9828 - val_loss: 0.5319 - val_accuracy: 0.8542\n",
            "391/391 [==============================] - 49s 126ms/step - loss: 0.5303 - accuracy: 0.8560\n",
            "Test loss:  0.5302598476409912\n",
            "Test acurracy:  0.8559600114822388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CYT3-JSJAG4"
      },
      "source": [
        "### With different vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxxVqkyaJFer"
      },
      "source": [
        "vocab_sizes = [5000, 7000, 10000]\n",
        "for size in vocab_sizes:\n",
        "  encoder = tf.keras.layers.TextVectorization(max_tokens=size)\n",
        "  encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "  # Store vocabulary\n",
        "  vocab = np.array(encoder.get_vocabulary())\n",
        "\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = size, # change this !!!!\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = 10, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  # print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGV57v7igjIk"
      },
      "source": [
        "### With an average of all hidden states to fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFiFjk2g-SX"
      },
      "source": [
        "# Creating the model\n",
        "model = tf.keras.Sequential([encoder,\n",
        "                             tf.keras.layers.Embedding(\n",
        "                                 input_dim = len(encoder.get_vocabulary()),\n",
        "                                 output_dim = 64,\n",
        "                                 mask_zero = True),\n",
        "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True)),\n",
        "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True)),\n",
        "                            tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(1, activation = 'sigmoid')])\n",
        "\n",
        "\n",
        "# Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                     epochs = 10, \n",
        "                     validation_data = test_dataset,\n",
        "                     validation_steps = 30)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test loss: ', test_loss)\n",
        "print('Test acurracy: ', test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}