{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment5_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1MLrmUyHKE"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "tfds.disable_progress_bar()\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGyfk4aQPDG"
      },
      "source": [
        "# Setting hypermeters\n",
        "batch_size = 32\n",
        "units = 64\n",
        "max_length = 120\n",
        "n_epochs = 5"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtFRj7_akl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07497534-fafc-4e77-d784-cf18078d2bd4"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYaW1wJqzwev"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6Tc5GAz2dG",
        "outputId": "c529f195-362d-48ec-dc55-178d379fbbc2"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteP1C28C/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteP1C28C/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteP1C28C/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n5pIYyEYsgN"
      },
      "source": [
        "# Shuffling the dataset\n",
        "buffer_size = 10000\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size) #.prefetch (tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size) #.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1JaB5g6Y4F"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rCKDLmQ_Wy"
      },
      "source": [
        "### Representing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tBy-8US6XxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77555921-ab4e-495f-8f0f-58dc6aeceab6"
      },
      "source": [
        "## Representing the  text\n",
        "vocab_size = 10000\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "# Store vocabulary\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# # Defining a function for fitting vectorizer function/layer to vectorize text (review)\n",
        "# def fitting_vectorizer (text, label):\n",
        "#   text = tf.expand_dims(text, -1)\n",
        "#   return encoder (text), label\n",
        "\n",
        "# # storing text batch and label batch\n",
        "# text_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "# # ## print an instance with vectorized review and label for observing\n",
        "# # print ('REVIEW:', text_batch[0])\n",
        "# # print('LABEL:', raw_train.class_names[label_batch[0]] )\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uDkzuD-ZMG"
      },
      "source": [
        "## Vanilla Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAqyl7gHzmS"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='binary')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='binary')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='binary')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNH5dRsqgOCt"
      },
      "source": [
        "### With different embedding sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH0TtCkH5Xkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d2dc96-378f-4a16-e385-11140ee5c1cc"
      },
      "source": [
        "# Creating the model\n",
        "embedding_sizes = [32,64,128]\n",
        "for size in embedding_sizes:\n",
        "  print (\"\\n========= embedding vectors'size= %s ============\" %size)\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = size,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  # print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= embedding vectors'size= 32 ============\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              49664     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 377,985\n",
            "Trainable params: 377,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 187s 225ms/step - loss: 0.5895 - accuracy: 0.6728 - val_loss: 0.5214 - val_accuracy: 0.7969\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.3419 - accuracy: 0.8611 - val_loss: 0.3598 - val_accuracy: 0.8552\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 172s 219ms/step - loss: 0.2520 - accuracy: 0.9036 - val_loss: 0.3216 - val_accuracy: 0.8656\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 166s 211ms/step - loss: 0.1991 - accuracy: 0.9263 - val_loss: 0.3706 - val_accuracy: 0.8594\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 165s 209ms/step - loss: 0.1674 - accuracy: 0.9413 - val_loss: 0.3160 - val_accuracy: 0.8781\n",
            "782/782 [==============================] - 84s 108ms/step - loss: 0.2854 - accuracy: 0.8850\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.285 - Accuracy: 0.885\n",
            "accuracy score: 0.885\n",
            "precision score: 0.883\n",
            "recall score: 0.888\n",
            "F1 score: 0.885\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88     12500\n",
            "           1       0.88      0.89      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "\n",
            "========= embedding vectors'size= 64 ============\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 714,369\n",
            "Trainable params: 714,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 188s 225ms/step - loss: 0.5754 - accuracy: 0.6980 - val_loss: 0.4399 - val_accuracy: 0.8438\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 169s 214ms/step - loss: 0.2962 - accuracy: 0.8838 - val_loss: 0.3143 - val_accuracy: 0.8740\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 171s 217ms/step - loss: 0.2138 - accuracy: 0.9183 - val_loss: 0.3124 - val_accuracy: 0.8729\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.1666 - accuracy: 0.9397 - val_loss: 0.3243 - val_accuracy: 0.8771\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 172s 219ms/step - loss: 0.1408 - accuracy: 0.9514 - val_loss: 0.3370 - val_accuracy: 0.8760\n",
            "782/782 [==============================] - 84s 107ms/step - loss: 0.3109 - accuracy: 0.8770\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.311 - Accuracy: 0.877\n",
            "accuracy score: 0.877\n",
            "precision score: 0.884\n",
            "recall score: 0.868\n",
            "F1 score: 0.876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88     12500\n",
            "           1       0.88      0.87      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========= embedding vectors'size= 128 ============\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 128)         1280000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,387,137\n",
            "Trainable params: 1,387,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 183s 221ms/step - loss: 0.5261 - accuracy: 0.7168 - val_loss: 0.3746 - val_accuracy: 0.8521\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 167s 213ms/step - loss: 0.2795 - accuracy: 0.8883 - val_loss: 0.3372 - val_accuracy: 0.8719\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 166s 212ms/step - loss: 0.2006 - accuracy: 0.9243 - val_loss: 0.3194 - val_accuracy: 0.8771\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 172s 218ms/step - loss: 0.1592 - accuracy: 0.9445 - val_loss: 0.3240 - val_accuracy: 0.8760\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 170s 217ms/step - loss: 0.1321 - accuracy: 0.9556 - val_loss: 0.3640 - val_accuracy: 0.8750\n",
            "782/782 [==============================] - 85s 108ms/step - loss: 0.3295 - accuracy: 0.8761\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.330 - Accuracy: 0.876\n",
            "accuracy score: 0.876\n",
            "precision score: 0.890\n",
            "recall score: 0.858\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88     12500\n",
            "           1       0.89      0.86      0.87     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CYT3-JSJAG4"
      },
      "source": [
        "### With different vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxxVqkyaJFer",
        "outputId": "d9d7d9cd-3d9d-49b9-b7df-52eec601ae09"
      },
      "source": [
        "vocab_sizes = [5000, 7000, 10000]\n",
        "for size in vocab_sizes:\n",
        "  print (\"\\n========= vocabulary size = %s ============\" %size)\n",
        "  encoder = tf.keras.layers.TextVectorization(max_tokens=size)\n",
        "  encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "  # Store vocabulary\n",
        "  vocab = np.array(encoder.get_vocabulary())\n",
        "\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 64,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= vocabulary size = 5000 ============\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,209\n",
            "Trainable params: 390,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 193s 233ms/step - loss: 0.5557 - accuracy: 0.7031 - val_loss: 0.4250 - val_accuracy: 0.8260\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 174s 221ms/step - loss: 0.3392 - accuracy: 0.8643 - val_loss: 0.3425 - val_accuracy: 0.8635\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 171s 217ms/step - loss: 0.2661 - accuracy: 0.8950 - val_loss: 0.3087 - val_accuracy: 0.8740\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 164s 208ms/step - loss: 0.2307 - accuracy: 0.9106 - val_loss: 0.3188 - val_accuracy: 0.8687\n",
            "Epoch 5/5\n",
            "415/782 [==============>...............] - ETA: 1:21 - loss: 0.2060 - accuracy: 0.9245"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttViIdaICVC2"
      },
      "source": [
        "### With different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeQy7iEUCaZG"
      },
      "source": [
        "# configure the model uisng optimizer and loss function\n",
        "print(model.summary())\n",
        "\n",
        "optimizers = ['adagrad', 'rmsprop', 'adam']\n",
        "for opt in optimizers:\n",
        "  print( '\\n========== optimizer = %s' %opt)\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = opt,\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZLUQDQFeYq"
      },
      "source": [
        "### Replacing with LSTM with GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRvMetfCF8Ir"
      },
      "source": [
        "model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 32,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.GRU(64,\n",
        "                                                  activation = 'tanh',\n",
        "                                                  recurrent_activation = 'sigmoid',\n",
        "                                                  recurrent_dropout = 0.0,\n",
        "                                                  use_bias = True),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "print(model.summary())\n",
        "\n",
        "# Compile the model for training\n",
        "print( '\\nTraining GRU model...')\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = 'adagrad',\n",
        "              metrics = ['accuracy'])\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGV57v7igjIk"
      },
      "source": [
        "### With an average of all hidden states to fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KSzM6nDl1CV"
      },
      "source": [
        "#### Creating the model ########\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Dense(32, activation = 'relu') (x)\n",
        "x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)) (x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D() (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "## Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dIYKgTuFNb"
      },
      "source": [
        "## Stacked bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFiFjk2g-SX"
      },
      "source": [
        "####   Creating the model  ####\n",
        "\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)) (x)   #return_sequences = True\n",
        "x = tf.keras.layers.Dense(64, activation = 'relu') (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "\n",
        "### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "##### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5K-7fI-GguP"
      },
      "source": [
        "## BiLSTM with attention layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvTyvy8MGpoP"
      },
      "source": [
        "#### Creating the model ####\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "vectorizer = encoder (_input) \n",
        "embeddings = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, input_length=max_length, mask_zero = True) (vectorizer)\n",
        "\n",
        "lstm_hs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (embeddings)\n",
        "# attention layer\n",
        "attention = tf.keras.layers.Dense(1, activation = 'tanh') (lstm_hs)\n",
        "attention = tf.keras.layers.Flatten()(attention)\n",
        "attention = tf.keras.layers.Activation('softmax') (attention)\n",
        "attention = tf.keras.layers.RepeatVector (units*2) (attention)\n",
        "attention = tf.keras.layers.Permute ([2,1]) (attention)\n",
        "\n",
        "attention_weight = tf.keras.layers.Multiply()([lstm_hs, attention])\n",
        "attention_weight = tf.keras.layers.Lambda(lambda x: K.sum(x, axis = 1)) (attention_weight)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (attention_weight)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset)\n",
        "#### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}