{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment5_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1MLrmUyHKE"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "tfds.disable_progress_bar()\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGyfk4aQPDG"
      },
      "source": [
        "# Setting hypermeters\n",
        "batch_size = 32\n",
        "units = 64\n",
        "max_length = 120\n",
        "n_epochs = 5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtFRj7_akl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f9d98f-8929-469b-f0d3-4a170ee6203d"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYaW1wJqzwev"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6Tc5GAz2dG",
        "outputId": "55918652-0fa1-4550-bc41-cd1f515df7e9"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTCJDDM/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTCJDDM/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteTCJDDM/imdb_reviews-unsupervised.tfrecord\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n5pIYyEYsgN"
      },
      "source": [
        "# Shuffling the dataset\n",
        "buffer_size = 10000\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size) #.prefetch (tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size) #.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1JaB5g6Y4F"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rCKDLmQ_Wy"
      },
      "source": [
        "### Representing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tBy-8US6XxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc666b1a-ce21-4645-80f6-6faac310dda3"
      },
      "source": [
        "## Representing the  text\n",
        "vocab_size = 10000\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "# Store vocabulary\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U17')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uDkzuD-ZMG"
      },
      "source": [
        "## Vanilla Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAqyl7gHzmS"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='binary')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='binary')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='binary')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNH5dRsqgOCt"
      },
      "source": [
        "### With different embedding sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH0TtCkH5Xkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d2dc96-378f-4a16-e385-11140ee5c1cc"
      },
      "source": [
        "# Creating the model\n",
        "embedding_sizes = [32,64,128]\n",
        "for size in embedding_sizes:\n",
        "  print (\"\\n========= embedding vectors'size= %s ============\" %size)\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = size,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  # print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= embedding vectors'size= 32 ============\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              49664     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 377,985\n",
            "Trainable params: 377,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 187s 225ms/step - loss: 0.5895 - accuracy: 0.6728 - val_loss: 0.5214 - val_accuracy: 0.7969\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.3419 - accuracy: 0.8611 - val_loss: 0.3598 - val_accuracy: 0.8552\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 172s 219ms/step - loss: 0.2520 - accuracy: 0.9036 - val_loss: 0.3216 - val_accuracy: 0.8656\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 166s 211ms/step - loss: 0.1991 - accuracy: 0.9263 - val_loss: 0.3706 - val_accuracy: 0.8594\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 165s 209ms/step - loss: 0.1674 - accuracy: 0.9413 - val_loss: 0.3160 - val_accuracy: 0.8781\n",
            "782/782 [==============================] - 84s 108ms/step - loss: 0.2854 - accuracy: 0.8850\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.285 - Accuracy: 0.885\n",
            "accuracy score: 0.885\n",
            "precision score: 0.883\n",
            "recall score: 0.888\n",
            "F1 score: 0.885\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88     12500\n",
            "           1       0.88      0.89      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "\n",
            "========= embedding vectors'size= 64 ============\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 714,369\n",
            "Trainable params: 714,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 188s 225ms/step - loss: 0.5754 - accuracy: 0.6980 - val_loss: 0.4399 - val_accuracy: 0.8438\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 169s 214ms/step - loss: 0.2962 - accuracy: 0.8838 - val_loss: 0.3143 - val_accuracy: 0.8740\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 171s 217ms/step - loss: 0.2138 - accuracy: 0.9183 - val_loss: 0.3124 - val_accuracy: 0.8729\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.1666 - accuracy: 0.9397 - val_loss: 0.3243 - val_accuracy: 0.8771\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 172s 219ms/step - loss: 0.1408 - accuracy: 0.9514 - val_loss: 0.3370 - val_accuracy: 0.8760\n",
            "782/782 [==============================] - 84s 107ms/step - loss: 0.3109 - accuracy: 0.8770\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.311 - Accuracy: 0.877\n",
            "accuracy score: 0.877\n",
            "precision score: 0.884\n",
            "recall score: 0.868\n",
            "F1 score: 0.876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88     12500\n",
            "           1       0.88      0.87      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========= embedding vectors'size= 128 ============\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 128)         1280000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,387,137\n",
            "Trainable params: 1,387,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 183s 221ms/step - loss: 0.5261 - accuracy: 0.7168 - val_loss: 0.3746 - val_accuracy: 0.8521\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 167s 213ms/step - loss: 0.2795 - accuracy: 0.8883 - val_loss: 0.3372 - val_accuracy: 0.8719\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 166s 212ms/step - loss: 0.2006 - accuracy: 0.9243 - val_loss: 0.3194 - val_accuracy: 0.8771\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 172s 218ms/step - loss: 0.1592 - accuracy: 0.9445 - val_loss: 0.3240 - val_accuracy: 0.8760\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 170s 217ms/step - loss: 0.1321 - accuracy: 0.9556 - val_loss: 0.3640 - val_accuracy: 0.8750\n",
            "782/782 [==============================] - 85s 108ms/step - loss: 0.3295 - accuracy: 0.8761\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.330 - Accuracy: 0.876\n",
            "accuracy score: 0.876\n",
            "precision score: 0.890\n",
            "recall score: 0.858\n",
            "F1 score: 0.874\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88     12500\n",
            "           1       0.89      0.86      0.87     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9f3jJ1qr_ge"
      },
      "source": [
        "We can see that the model with the embedding size = 32 yielded the best performance, with a F1 score of 0.885, followed by the model with the embedding size of 64. The models with the embedding sizes of 64 and 128 did not show much difference in the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CYT3-JSJAG4"
      },
      "source": [
        "### With different vocabulary sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxxVqkyaJFer",
        "outputId": "16084201-1a7c-4991-f30d-04f22cff9873"
      },
      "source": [
        "vocab_sizes = [5000, 7000, 10000]\n",
        "for size in vocab_sizes:\n",
        "  print (\"\\n========= vocabulary size = %s ============\" %size)\n",
        "  encoder = tf.keras.layers.TextVectorization(max_tokens=size)\n",
        "  encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "  # Store vocabulary\n",
        "  vocab = np.array(encoder.get_vocabulary())\n",
        "\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 64,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= vocabulary size = 5000 ============\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 390,209\n",
            "Trainable params: 390,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 193s 233ms/step - loss: 0.5557 - accuracy: 0.7031 - val_loss: 0.4250 - val_accuracy: 0.8260\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 174s 221ms/step - loss: 0.3392 - accuracy: 0.8643 - val_loss: 0.3425 - val_accuracy: 0.8635\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 171s 217ms/step - loss: 0.2661 - accuracy: 0.8950 - val_loss: 0.3087 - val_accuracy: 0.8740\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 164s 208ms/step - loss: 0.2307 - accuracy: 0.9106 - val_loss: 0.3188 - val_accuracy: 0.8687\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 175s 223ms/step - loss: 0.2064 - accuracy: 0.9223 - val_loss: 0.3135 - val_accuracy: 0.8760\n",
            "782/782 [==============================] - 85s 108ms/step - loss: 0.2904 - accuracy: 0.8807\n",
            "Test acurracy:  0.8806800246238708\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.290 - Accuracy: 0.881\n",
            "accuracy score: 0.881\n",
            "precision score: 0.863\n",
            "recall score: 0.905\n",
            "F1 score: 0.884\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88     12500\n",
            "           1       0.86      0.90      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========= vocabulary size = 7000 ============\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_2 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, None, 64)          448000    \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 518,209\n",
            "Trainable params: 518,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 188s 226ms/step - loss: 0.5476 - accuracy: 0.7184 - val_loss: 0.3788 - val_accuracy: 0.8490\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 176s 224ms/step - loss: 0.2843 - accuracy: 0.8869 - val_loss: 0.3280 - val_accuracy: 0.8635\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 176s 224ms/step - loss: 0.2203 - accuracy: 0.9148 - val_loss: 0.2984 - val_accuracy: 0.8823\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.1868 - accuracy: 0.9298 - val_loss: 0.3118 - val_accuracy: 0.8740\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 167s 213ms/step - loss: 0.1637 - accuracy: 0.9394 - val_loss: 0.3478 - val_accuracy: 0.8750\n",
            "782/782 [==============================] - 84s 108ms/step - loss: 0.3142 - accuracy: 0.8789\n",
            "Test acurracy:  0.8789200186729431\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.314 - Accuracy: 0.879\n",
            "accuracy score: 0.879\n",
            "precision score: 0.889\n",
            "recall score: 0.866\n",
            "F1 score: 0.877\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88     12500\n",
            "           1       0.89      0.87      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========= vocabulary size = 10000 ============\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 710,209\n",
            "Trainable params: 710,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 190s 230ms/step - loss: 0.5575 - accuracy: 0.6908 - val_loss: 0.4525 - val_accuracy: 0.8104\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 169s 216ms/step - loss: 0.3663 - accuracy: 0.8433 - val_loss: 0.4047 - val_accuracy: 0.8365\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 168s 213ms/step - loss: 0.3747 - accuracy: 0.8498 - val_loss: 0.4060 - val_accuracy: 0.8313\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 167s 212ms/step - loss: 0.3483 - accuracy: 0.8532 - val_loss: 0.4092 - val_accuracy: 0.8052\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 175s 222ms/step - loss: 0.2447 - accuracy: 0.9052 - val_loss: 0.3461 - val_accuracy: 0.8677\n",
            "782/782 [==============================] - 84s 108ms/step - loss: 0.3179 - accuracy: 0.8656\n",
            "Test acurracy:  0.865559995174408\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.318 - Accuracy: 0.866\n",
            "accuracy score: 0.866\n",
            "precision score: 0.898\n",
            "recall score: 0.825\n",
            "F1 score: 0.860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87     12500\n",
            "           1       0.90      0.82      0.86     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYVanrQXtZvB"
      },
      "source": [
        "We experimented the models with three vocabulary sizes (number of features): 5000, 7000, 10000. From the result, we found that the smaller vocabulary size was, the better F1 the model could achieve. For example, the model with 5000 features achieved 0.884 on F1 score, while the 1000-feature model got a F1 of 0.860."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttViIdaICVC2"
      },
      "source": [
        "### With different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeQy7iEUCaZG",
        "outputId": "d9a15f53-b741-4b70-e9cb-6db38018c130"
      },
      "source": [
        "# configure the model uisng optimizer and loss function\n",
        "print(model.summary())\n",
        "\n",
        "optimizers = ['adagrad', 'rmsprop', 'adam']\n",
        "for opt in optimizers:\n",
        "  print( '\\n========== optimizer = %s' %opt)\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = opt,\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 128)              66048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 710,209\n",
            "Trainable params: 710,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "========== optimizer = adagrad\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 184s 222ms/step - loss: 0.2119 - accuracy: 0.9206 - val_loss: 0.3694 - val_accuracy: 0.8646\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 172s 219ms/step - loss: 0.2006 - accuracy: 0.9280 - val_loss: 0.3476 - val_accuracy: 0.8667\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 167s 212ms/step - loss: 0.1923 - accuracy: 0.9282 - val_loss: 0.3375 - val_accuracy: 0.8729\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 163s 207ms/step - loss: 0.1895 - accuracy: 0.9319 - val_loss: 0.3469 - val_accuracy: 0.8667\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.1813 - accuracy: 0.9346 - val_loss: 0.3559 - val_accuracy: 0.8646\n",
            "782/782 [==============================] - 83s 106ms/step - loss: 0.3170 - accuracy: 0.8746\n",
            "Test acurracy:  0.8745999932289124\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.317 - Accuracy: 0.875\n",
            "accuracy score: 0.875\n",
            "precision score: 0.900\n",
            "recall score: 0.843\n",
            "F1 score: 0.871\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88     12500\n",
            "           1       0.90      0.84      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.88      0.87      0.87     25000\n",
            "weighted avg       0.88      0.87      0.87     25000\n",
            "\n",
            "\n",
            "========== optimizer = rmsprop\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 191s 231ms/step - loss: 0.3063 - accuracy: 0.8756 - val_loss: 0.3869 - val_accuracy: 0.8625\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 170s 217ms/step - loss: 0.2327 - accuracy: 0.9106 - val_loss: 0.3056 - val_accuracy: 0.8677\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.1944 - accuracy: 0.9272 - val_loss: 0.3629 - val_accuracy: 0.8552\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 170s 216ms/step - loss: 0.1609 - accuracy: 0.9410 - val_loss: 0.3428 - val_accuracy: 0.8677\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 177s 225ms/step - loss: 0.1358 - accuracy: 0.9498 - val_loss: 0.3910 - val_accuracy: 0.8771\n",
            "782/782 [==============================] - 83s 106ms/step - loss: 0.3462 - accuracy: 0.8811\n",
            "Test acurracy:  0.8810799717903137\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.346 - Accuracy: 0.881\n",
            "accuracy score: 0.881\n",
            "precision score: 0.872\n",
            "recall score: 0.893\n",
            "F1 score: 0.882\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.87      0.88     12500\n",
            "           1       0.87      0.89      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========== optimizer = adam\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 190s 230ms/step - loss: 0.1221 - accuracy: 0.9549 - val_loss: 0.3972 - val_accuracy: 0.8604\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 176s 224ms/step - loss: 0.0712 - accuracy: 0.9743 - val_loss: 0.5055 - val_accuracy: 0.8448\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 169s 215ms/step - loss: 0.0444 - accuracy: 0.9851 - val_loss: 0.5501 - val_accuracy: 0.8573\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 171s 217ms/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 0.7225 - val_accuracy: 0.8406\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 174s 221ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.7743 - val_accuracy: 0.8552\n",
            "782/782 [==============================] - 83s 106ms/step - loss: 0.6842 - accuracy: 0.8554\n",
            "Test acurracy:  0.8553599715232849\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.684 - Accuracy: 0.855\n",
            "accuracy score: 0.855\n",
            "precision score: 0.848\n",
            "recall score: 0.867\n",
            "F1 score: 0.857\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.84      0.85     12500\n",
            "           1       0.85      0.87      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVMP0KjDvNnW"
      },
      "source": [
        "We explored three different opitimizers: adagrad, rmsprop, and adam. adagrad, rmsprop opimizers did not show significantly differences on the performance, 0.871 and 0.882 on F1 scores respectively. Adam optimizer did not perform as good as the two other optimizers, with F1 score of 0.857."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZLUQDQFeYq"
      },
      "source": [
        "### Replacing LSTM with GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRvMetfCF8Ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be47ecd-5663-4b9b-ae9c-5e55da906562"
      },
      "source": [
        "model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 32,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.GRU(64,\n",
        "                                                  activation = 'tanh',\n",
        "                                                  recurrent_activation = 'sigmoid',\n",
        "                                                  recurrent_dropout = 0.0,\n",
        "                                                  use_bias = True),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "print(model.summary())\n",
        "\n",
        "# Compile the model for training\n",
        "print( '\\nTraining GRU model...')\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = 'adagrad',\n",
        "              metrics = ['accuracy'])\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                18816     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 340,929\n",
            "Trainable params: 340,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "Training GRU model...\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 113s 138ms/step - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6928 - val_accuracy: 0.5281\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 107s 135ms/step - loss: 0.6930 - accuracy: 0.5117 - val_loss: 0.6927 - val_accuracy: 0.5271\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 106s 135ms/step - loss: 0.6930 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5354\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 107s 136ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6926 - val_accuracy: 0.5354\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 107s 136ms/step - loss: 0.6929 - accuracy: 0.5209 - val_loss: 0.6926 - val_accuracy: 0.5406\n",
            "782/782 [==============================] - 44s 56ms/step - loss: 0.6929 - accuracy: 0.5250\n",
            "Test acurracy:  0.5249599814414978\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.693 - Accuracy: 0.525\n",
            "accuracy score: 0.525\n",
            "precision score: 0.542\n",
            "recall score: 0.321\n",
            "F1 score: 0.403\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.73      0.61     12500\n",
            "           1       0.54      0.32      0.40     12500\n",
            "\n",
            "    accuracy                           0.52     25000\n",
            "   macro avg       0.53      0.52      0.50     25000\n",
            "weighted avg       0.53      0.52      0.50     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52496, 0.5422192151556157, 0.32056, 0.4029160382101558)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGnsj_gvxCo8"
      },
      "source": [
        "In this model, we replaced the vanilla bidirectional LSTM model with GRU model. However, the result shows that the GRU model performed much worse than the biLSTM, with a F1 score of 0.403. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGV57v7igjIk"
      },
      "source": [
        "### With an average of all hidden states to fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KSzM6nDl1CV",
        "outputId": "8cdabd7e-a164-473b-8268-4359fc55b9ed"
      },
      "source": [
        "#### Creating the model ########\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Dense(32, activation = 'relu') (x)\n",
        "x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)) (x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D() (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "## Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 64)          640000    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 128)        66048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 32)          4128      \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 1)          33        \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 1)                0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 710,211\n",
            "Trainable params: 710,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 186s 221ms/step - loss: 0.4766 - accuracy: 0.7848 - val_loss: 0.4137 - val_accuracy: 0.8406\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 168s 214ms/step - loss: 0.2664 - accuracy: 0.8976 - val_loss: 0.3163 - val_accuracy: 0.8771\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 160s 204ms/step - loss: 0.2036 - accuracy: 0.9238 - val_loss: 0.3225 - val_accuracy: 0.8687\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 167s 212ms/step - loss: 0.1682 - accuracy: 0.9416 - val_loss: 0.3204 - val_accuracy: 0.8698\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 159s 202ms/step - loss: 0.1396 - accuracy: 0.9513 - val_loss: 0.3891 - val_accuracy: 0.8677\n",
            "782/782 [==============================] - 83s 107ms/step - loss: 0.3488 - accuracy: 0.8708\n",
            "Test acurracy:  0.8707600235939026\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.349 - Accuracy: 0.871\n",
            "accuracy score: 0.871\n",
            "precision score: 0.905\n",
            "recall score: 0.829\n",
            "F1 score: 0.865\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88     12500\n",
            "           1       0.90      0.83      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.87076, 0.9047244782115099, 0.8288, 0.8650995783057074)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-5w2L4x7J9"
      },
      "source": [
        "In this model, we attempted to adjust the last layer before fully-connected layer. In previous LSTM models, only the last hidden states were transfered to the classifying layers. In many cases, especially when the sentence length is too long, the last hidden states could not carry on enough information. Taking the average of all hidden states to fully-connected layer is beneficial in these cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dIYKgTuFNb"
      },
      "source": [
        "## Stacked bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFiFjk2g-SX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79d1a43-5673-4d9d-adbc-7abf42066fc9"
      },
      "source": [
        "####   Creating the model  ####\n",
        "\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)) (x)   #return_sequences = True\n",
        "x = tf.keras.layers.Dense(64, activation = 'relu') (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "\n",
        "### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "##### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 316s 382ms/step - loss: 0.4712 - accuracy: 0.7550 - val_loss: 0.3421 - val_accuracy: 0.8615\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 288s 367ms/step - loss: 0.2374 - accuracy: 0.9074 - val_loss: 0.3133 - val_accuracy: 0.8813\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 285s 363ms/step - loss: 0.1773 - accuracy: 0.9347 - val_loss: 0.3274 - val_accuracy: 0.8813\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 288s 367ms/step - loss: 0.1397 - accuracy: 0.9513 - val_loss: 0.3658 - val_accuracy: 0.8740\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 289s 368ms/step - loss: 0.1112 - accuracy: 0.9624 - val_loss: 0.4399 - val_accuracy: 0.8708\n",
            "782/782 [==============================] - 153s 196ms/step - loss: 0.4014 - accuracy: 0.8703\n",
            "Test acurracy:  0.8703200221061707\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.401 - Accuracy: 0.870\n",
            "accuracy score: 0.870\n",
            "precision score: 0.869\n",
            "recall score: 0.872\n",
            "F1 score: 0.871\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     12500\n",
            "           1       0.87      0.87      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.87032, 0.8688446215139443, 0.87232, 0.8705788423153692)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5K-7fI-GguP"
      },
      "source": [
        "## BiLSTM with attention layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvTyvy8MGpoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579acc57-a853-4e9a-ff22-58d70f6580a6"
      },
      "source": [
        "#### Creating the model ####\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "vectorizer = encoder (_input) \n",
        "embeddings = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, input_length=max_length, mask_zero = True) (vectorizer)\n",
        "\n",
        "lstm_hs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (embeddings)\n",
        "# attention layer\n",
        "attention = tf.keras.layers.Dense(1, activation = 'tanh') (lstm_hs)\n",
        "attention = tf.keras.layers.Flatten()(attention)\n",
        "attention = tf.keras.layers.Activation('softmax') (attention)\n",
        "attention = tf.keras.layers.RepeatVector (units*2) (attention)\n",
        "attention = tf.keras.layers.Permute ([2,1]) (attention)\n",
        "\n",
        "attention_weight = tf.keras.layers.Multiply()([lstm_hs, attention])\n",
        "attention_weight = tf.keras.layers.Lambda(lambda x: K.sum(x, axis = 1)) (attention_weight)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (attention_weight)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset)\n",
        "#### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " text_vectorization (TextVector  (None, None)        0           ['input_3[0][0]']                \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 64)     640000      ['text_vectorization[2][0]']     \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, None, 128)   66048       ['embedding_2[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, None, 1)      129         ['bidirectional_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, None)         0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, None)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 128, None)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, None, 128)    0           ['repeat_vector[0][0]']          \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, None, 128)    0           ['bidirectional_3[0][0]',        \n",
            "                                                                  'permute[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 128)          0           ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            129         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 706,306\n",
            "Trainable params: 706,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 256s 314ms/step - loss: 0.5986 - accuracy: 0.7180 - val_loss: 0.4974 - val_accuracy: 0.8201\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 231s 294ms/step - loss: 0.4528 - accuracy: 0.8389 - val_loss: 0.4351 - val_accuracy: 0.8415\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 234s 298ms/step - loss: 0.3799 - accuracy: 0.8672 - val_loss: 0.3980 - val_accuracy: 0.8516\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 233s 297ms/step - loss: 0.3684 - accuracy: 0.8766 - val_loss: 0.4335 - val_accuracy: 0.8369\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 226s 288ms/step - loss: 0.3228 - accuracy: 0.8945 - val_loss: 0.4515 - val_accuracy: 0.8363\n",
            "782/782 [==============================] - 84s 107ms/step - loss: 0.4515 - accuracy: 0.8363\n",
            "Test acurracy:  0.8363199830055237\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.451 - Accuracy: 0.836\n",
            "accuracy score: 0.836\n",
            "precision score: 0.866\n",
            "recall score: 0.796\n",
            "F1 score: 0.829\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84     12500\n",
            "           1       0.87      0.80      0.83     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.83632, 0.8659470752089137, 0.79584, 0.8294147073536768)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}