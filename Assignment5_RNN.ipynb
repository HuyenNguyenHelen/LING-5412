{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/LING-5412/blob/main/Assignment5_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH1MLrmUyHKE"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "tfds.disable_progress_bar()\n",
        "from keras import backend as K\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVGyfk4aQPDG"
      },
      "source": [
        "# Setting hypermeters\n",
        "batch_size = 32\n",
        "units = 64\n",
        "max_length = 120\n",
        "n_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLtFRj7_akl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2ee893-f116-4582-a8a7-0303153265c6"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYaW1wJqzwev"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6Tc5GAz2dG",
        "outputId": "a4053b85-5fb2-47ee-f12d-57a9fb49dc31"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n5pIYyEYsgN"
      },
      "source": [
        "# Shuffling the dataset\n",
        "buffer_size = 10000\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size) #.prefetch (tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(batch_size) #.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1JaB5g6Y4F"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rCKDLmQ_Wy"
      },
      "source": [
        "### Representing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tBy-8US6XxQ"
      },
      "source": [
        "## Representing the  text\n",
        "vocab_size = 10000\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "\n",
        "# Store vocabulary\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]\n",
        "\n",
        "# # Defining a function for fitting vectorizer function/layer to vectorize text (review)\n",
        "# def fitting_vectorizer (text, label):\n",
        "#   text = tf.expand_dims(text, -1)\n",
        "#   return encoder (text), label\n",
        "\n",
        "# # storing text batch and label batch\n",
        "# text_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "# # ## print an instance with vectorized review and label for observing\n",
        "# # print ('REVIEW:', text_batch[0])\n",
        "# # print('LABEL:', raw_train.class_names[label_batch[0]] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uDkzuD-ZMG"
      },
      "source": [
        "## Vanilla Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAqyl7gHzmS"
      },
      "source": [
        "# Defining an evaluation metric function\n",
        "def printing_eval_scores (y_true, y_pred, report=''):\n",
        "  accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "  precision = sklearn.metrics.precision_score(y_true, y_pred, average='binary')\n",
        "  recall = sklearn.metrics.recall_score(y_true, y_pred, average='binary')\n",
        "  f1 = sklearn.metrics.f1_score(y_true, y_pred , average='binary')\n",
        "  print('accuracy score: {:.3f}'.format(accuracy))\n",
        "  print('precision score: {:.3f}'.format(precision))\n",
        "  print('recall score: {:.3f}'.format(recall))\n",
        "  print('F1 score: {:.3f}'.format(f1))\n",
        "  if report is True:\n",
        "    print(classification_report(y_true, y_pred))\n",
        "  else:\n",
        "    pass\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNH5dRsqgOCt"
      },
      "source": [
        "### With different embedding sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH0TtCkH5Xkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffeb3a0-f197-4964-86c4-4878e2fa8560"
      },
      "source": [
        "# Creating the model\n",
        "embedding_sizes = [32,64,128]\n",
        "for size in embedding_sizes:\n",
        "  print (\"\\n========= embedding vectors'size= %s ============\" %size)\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = size,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  # print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 109s 251ms/step - loss: 0.6309 - accuracy: 0.6450 - val_loss: 0.4827 - val_accuracy: 0.7969\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 94s 238ms/step - loss: 0.3926 - accuracy: 0.8348 - val_loss: 0.4405 - val_accuracy: 0.8016\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.3033 - accuracy: 0.8807 - val_loss: 0.3374 - val_accuracy: 0.8708\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 95s 239ms/step - loss: 0.2338 - accuracy: 0.9110 - val_loss: 0.3155 - val_accuracy: 0.8708\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.1947 - accuracy: 0.9294 - val_loss: 0.3460 - val_accuracy: 0.8776\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 93s 234ms/step - loss: 0.1676 - accuracy: 0.9414 - val_loss: 0.3469 - val_accuracy: 0.8734\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.1411 - accuracy: 0.9526 - val_loss: 0.3368 - val_accuracy: 0.8755\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.1228 - accuracy: 0.9600 - val_loss: 0.3893 - val_accuracy: 0.8708\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.1082 - accuracy: 0.9653 - val_loss: 0.3901 - val_accuracy: 0.8719\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0935 - accuracy: 0.9727 - val_loss: 0.4231 - val_accuracy: 0.8703\n",
            "391/391 [==============================] - 49s 124ms/step - loss: 0.4096 - accuracy: 0.8652\n",
            "Test loss:  0.4095645844936371\n",
            "Test acurracy:  0.8651599884033203\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 108s 249ms/step - loss: 0.5886 - accuracy: 0.6764 - val_loss: 0.4114 - val_accuracy: 0.8297\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.2981 - accuracy: 0.8820 - val_loss: 0.3738 - val_accuracy: 0.8458\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.2180 - accuracy: 0.9168 - val_loss: 0.3047 - val_accuracy: 0.8797\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.1769 - accuracy: 0.9352 - val_loss: 0.3190 - val_accuracy: 0.8807\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.1461 - accuracy: 0.9496 - val_loss: 0.3580 - val_accuracy: 0.8766\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.1216 - accuracy: 0.9583 - val_loss: 0.4192 - val_accuracy: 0.8724\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.1076 - accuracy: 0.9647 - val_loss: 0.4267 - val_accuracy: 0.8641\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0911 - accuracy: 0.9724 - val_loss: 0.4758 - val_accuracy: 0.8557\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.0883 - accuracy: 0.9722 - val_loss: 0.4287 - val_accuracy: 0.8552\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 0.4927 - val_accuracy: 0.8526\n",
            "391/391 [==============================] - 48s 122ms/step - loss: 0.4873 - accuracy: 0.8572\n",
            "Test loss:  0.4872962236404419\n",
            "Test acurracy:  0.8571599721908569\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 108s 251ms/step - loss: 0.5648 - accuracy: 0.7047 - val_loss: 0.3737 - val_accuracy: 0.8479\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 96s 242ms/step - loss: 0.2858 - accuracy: 0.8881 - val_loss: 0.3024 - val_accuracy: 0.8781\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.2061 - accuracy: 0.9232 - val_loss: 0.2948 - val_accuracy: 0.8833\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 96s 242ms/step - loss: 0.1650 - accuracy: 0.9405 - val_loss: 0.3260 - val_accuracy: 0.8760\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.1339 - accuracy: 0.9559 - val_loss: 0.3356 - val_accuracy: 0.8818\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.1153 - accuracy: 0.9636 - val_loss: 0.3872 - val_accuracy: 0.8641\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.0974 - accuracy: 0.9696 - val_loss: 0.4019 - val_accuracy: 0.8698\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 95s 241ms/step - loss: 0.0806 - accuracy: 0.9773 - val_loss: 0.4272 - val_accuracy: 0.8672\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.0691 - accuracy: 0.9808 - val_loss: 0.4887 - val_accuracy: 0.8510\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.0644 - accuracy: 0.9828 - val_loss: 0.5319 - val_accuracy: 0.8542\n",
            "391/391 [==============================] - 49s 126ms/step - loss: 0.5303 - accuracy: 0.8560\n",
            "Test loss:  0.5302598476409912\n",
            "Test acurracy:  0.8559600114822388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CYT3-JSJAG4"
      },
      "source": [
        "### With different vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "XxxVqkyaJFer",
        "outputId": "771f8106-b748-468a-a77d-2b8e3eaea8e4"
      },
      "source": [
        "vocab_sizes = [5000, 7000, 10000]\n",
        "for size in vocab_sizes:\n",
        "  print (\"\\n========= vocabulary size = %s ============\" %size)\n",
        "  encoder = tf.keras.layers.TextVectorization(max_tokens=size)\n",
        "  encoder.adapt(train_dataset.map(lambda x,y: x))\n",
        "  # Store vocabulary\n",
        "  vocab = np.array(encoder.get_vocabulary())\n",
        "\n",
        "  model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 64,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "  print(model.summary())\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  ### pred_label = tf.argmax(model.predict(test),1)\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= vocabulary size = 5000 ============\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 32)          160000    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              49664     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,825\n",
            "Trainable params: 213,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 107s 247ms/step - loss: 0.6601 - accuracy: 0.5976 - val_loss: 0.5872 - val_accuracy: 0.7302\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.5656 - accuracy: 0.7126 - val_loss: 0.4870 - val_accuracy: 0.7969\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.3888 - accuracy: 0.8472 - val_loss: 0.3598 - val_accuracy: 0.8510\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.2986 - accuracy: 0.8794 - val_loss: 0.3354 - val_accuracy: 0.8542\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.2613 - accuracy: 0.8982 - val_loss: 0.3115 - val_accuracy: 0.8708\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.2374 - accuracy: 0.9111 - val_loss: 0.3076 - val_accuracy: 0.8755\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.2169 - accuracy: 0.9201 - val_loss: 0.3013 - val_accuracy: 0.8786\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.2036 - accuracy: 0.9270 - val_loss: 0.3003 - val_accuracy: 0.8792\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 91s 230ms/step - loss: 0.1865 - accuracy: 0.9334 - val_loss: 0.3156 - val_accuracy: 0.8661\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.1792 - accuracy: 0.9361 - val_loss: 0.3298 - val_accuracy: 0.8760\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6e852167d4a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m### pred_label = tf.argmax(model.predict(test),1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttViIdaICVC2"
      },
      "source": [
        "### With different optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeQy7iEUCaZG",
        "outputId": "bd4de441-02c1-4043-f2f8-b6569de4693b"
      },
      "source": [
        "# configure the model uisng optimizer and loss function\n",
        "print(model.summary())\n",
        "\n",
        "optimizers = ['adagrad', 'rmsprop', 'adam']\n",
        "for opt in optimizers:\n",
        "  print( '\\n========== optimizer = %s' %opt)\n",
        "  # Compile the model for training\n",
        "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                optimizer = opt,\n",
        "                metrics = ['accuracy'])\n",
        "  # Training the model\n",
        "  history = model.fit (train_dataset,\n",
        "                      epochs = n_epochs, \n",
        "                      validation_data = test_dataset,\n",
        "                      validation_steps = 30)\n",
        "  # testing the model\n",
        "  pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "  true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "  test_loss, test_acc = model.evaluate (test_dataset)\n",
        "  # print('Test loss: ', test_loss)\n",
        "  print('Test acurracy: ', test_acc)\n",
        "  print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "  printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 32)          160000    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128)              49664     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,825\n",
            "Trainable params: 213,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "========== optimizer = adagrad\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 105s 245ms/step - loss: 0.1638 - accuracy: 0.9444 - val_loss: 0.3378 - val_accuracy: 0.8776\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.1568 - accuracy: 0.9477 - val_loss: 0.3186 - val_accuracy: 0.8823\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.1545 - accuracy: 0.9482 - val_loss: 0.3371 - val_accuracy: 0.8781\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.1530 - accuracy: 0.9496 - val_loss: 0.3281 - val_accuracy: 0.8745\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.1519 - accuracy: 0.9494 - val_loss: 0.3413 - val_accuracy: 0.8745\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.1506 - accuracy: 0.9502 - val_loss: 0.3347 - val_accuracy: 0.8781\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.1503 - accuracy: 0.9509 - val_loss: 0.3363 - val_accuracy: 0.8724\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.1486 - accuracy: 0.9516 - val_loss: 0.3527 - val_accuracy: 0.8734\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.1480 - accuracy: 0.9517 - val_loss: 0.3477 - val_accuracy: 0.8755\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 91s 229ms/step - loss: 0.1475 - accuracy: 0.9526 - val_loss: 0.3387 - val_accuracy: 0.8781\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 0.3258 - accuracy: 0.8775\n",
            "Test acurracy:  0.8775200247764587\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.326 - Accuracy: 0.878\n",
            "accuracy score: 0.878\n",
            "precision score: 0.883\n",
            "recall score: 0.870\n",
            "F1 score: 0.877\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.88      0.88     12500\n",
            "           1       0.88      0.87      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "========== optimizer = rmsprop\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 106s 247ms/step - loss: 0.3176 - accuracy: 0.8691 - val_loss: 0.3257 - val_accuracy: 0.8615\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.2457 - accuracy: 0.9041 - val_loss: 0.3064 - val_accuracy: 0.8729\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 95s 240ms/step - loss: 0.2137 - accuracy: 0.9158 - val_loss: 0.3233 - val_accuracy: 0.8604\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.1888 - accuracy: 0.9279 - val_loss: 0.3578 - val_accuracy: 0.8771\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.1652 - accuracy: 0.9373 - val_loss: 0.3558 - val_accuracy: 0.8734\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 93s 236ms/step - loss: 0.1448 - accuracy: 0.9472 - val_loss: 0.4141 - val_accuracy: 0.8505\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 93s 235ms/step - loss: 0.1252 - accuracy: 0.9548 - val_loss: 0.4920 - val_accuracy: 0.8526\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.1050 - accuracy: 0.9640 - val_loss: 0.4635 - val_accuracy: 0.8505\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 92s 232ms/step - loss: 0.0825 - accuracy: 0.9719 - val_loss: 0.5052 - val_accuracy: 0.8599\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.0674 - accuracy: 0.9769 - val_loss: 0.5612 - val_accuracy: 0.8510\n",
            "391/391 [==============================] - 48s 122ms/step - loss: 0.5264 - accuracy: 0.8555\n",
            "Test acurracy:  0.8554800152778625\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.526 - Accuracy: 0.855\n",
            "accuracy score: 0.855\n",
            "precision score: 0.849\n",
            "recall score: 0.865\n",
            "F1 score: 0.857\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85     12500\n",
            "           1       0.85      0.87      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "\n",
            "========== optimizer = adam\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 104s 244ms/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.6076 - val_accuracy: 0.8286\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 94s 237ms/step - loss: 0.0323 - accuracy: 0.9886 - val_loss: 0.7733 - val_accuracy: 0.8453\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 92s 233ms/step - loss: 0.0295 - accuracy: 0.9895 - val_loss: 0.7509 - val_accuracy: 0.8516\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.0237 - accuracy: 0.9914 - val_loss: 0.8455 - val_accuracy: 0.8500\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 91s 230ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.9761 - val_accuracy: 0.8359\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 91s 230ms/step - loss: 0.0155 - accuracy: 0.9943 - val_loss: 1.0173 - val_accuracy: 0.8495\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 94s 237ms/step - loss: 0.0195 - accuracy: 0.9932 - val_loss: 0.9098 - val_accuracy: 0.8323\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.0177 - accuracy: 0.9938 - val_loss: 0.9077 - val_accuracy: 0.8359\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 91s 230ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 1.1079 - val_accuracy: 0.8479\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 91s 231ms/step - loss: 0.0131 - accuracy: 0.9952 - val_loss: 1.0697 - val_accuracy: 0.8339\n",
            "391/391 [==============================] - 47s 121ms/step - loss: 0.9642 - accuracy: 0.8403\n",
            "Test acurracy:  0.8403199911117554\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.964 - Accuracy: 0.840\n",
            "accuracy score: 0.840\n",
            "precision score: 0.812\n",
            "recall score: 0.885\n",
            "F1 score: 0.847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83     12500\n",
            "           1       0.81      0.89      0.85     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CZLUQDQFeYq"
      },
      "source": [
        "### Replacing with LSTM with GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRvMetfCF8Ir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cadb3524-af72-4c3a-da4b-27eb703fc379"
      },
      "source": [
        "model = tf.keras.Sequential([encoder,\n",
        "                              tf.keras.layers.Embedding(\n",
        "                                  input_dim = len(encoder.get_vocabulary()),\n",
        "                                  output_dim = 32,\n",
        "                                  mask_zero = True),\n",
        "                              tf.keras.layers.GRU(64,\n",
        "                                                  activation = 'tanh',\n",
        "                                                  recurrent_activation = 'sigmoid',\n",
        "                                                  recurrent_dropout = 0.0,\n",
        "                                                  use_bias = True),\n",
        "                              tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                              tf.keras.layers.Dense(1, activation = 'sigmoid')]) \n",
        "print(model.summary())\n",
        "\n",
        "# Compile the model for training\n",
        "print( '\\nTraining GRU model...')\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = 'adagrad',\n",
        "              metrics = ['accuracy'])\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 32)          320000    \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                18816     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 340,929\n",
            "Trainable params: 340,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training GRU model...\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 76s 167ms/step - loss: 0.6931 - accuracy: 0.4962 - val_loss: 0.6931 - val_accuracy: 0.5063\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.6930 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5141\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 64s 163ms/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6929 - val_accuracy: 0.5203\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.6929 - accuracy: 0.5220 - val_loss: 0.6929 - val_accuracy: 0.5271\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 64s 162ms/step - loss: 0.6929 - accuracy: 0.5299 - val_loss: 0.6929 - val_accuracy: 0.5333\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 64s 161ms/step - loss: 0.6929 - accuracy: 0.5285 - val_loss: 0.6929 - val_accuracy: 0.5333\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 64s 161ms/step - loss: 0.6928 - accuracy: 0.5316 - val_loss: 0.6928 - val_accuracy: 0.5385\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 64s 161ms/step - loss: 0.6928 - accuracy: 0.5328 - val_loss: 0.6928 - val_accuracy: 0.5391\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 64s 161ms/step - loss: 0.6928 - accuracy: 0.5351 - val_loss: 0.6928 - val_accuracy: 0.5401\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 64s 161ms/step - loss: 0.6928 - accuracy: 0.5330 - val_loss: 0.6928 - val_accuracy: 0.5380\n",
            "391/391 [==============================] - 27s 68ms/step - loss: 0.6928 - accuracy: 0.5362\n",
            "Test acurracy:  0.5361599922180176\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.693 - Accuracy: 0.536\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7c70b8458b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test acurracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprinting_eval_scores\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'printing_eval_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGV57v7igjIk"
      },
      "source": [
        "### With an average of all hidden states to fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KSzM6nDl1CV",
        "outputId": "fb73ddd8-3c45-4e19-80f7-4f9dcc94f9ea"
      },
      "source": [
        "#### Creating the model ########\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Dense(32, activation = 'relu') (x)\n",
        "x = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)) (x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D() (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "## Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "# testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 180s 216ms/step - loss: 0.4890 - accuracy: 0.7668 - val_loss: 0.3999 - val_accuracy: 0.8354\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 166s 211ms/step - loss: 0.3009 - accuracy: 0.8858 - val_loss: 0.3760 - val_accuracy: 0.8573\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 163s 208ms/step - loss: 0.2411 - accuracy: 0.9113 - val_loss: 0.3725 - val_accuracy: 0.8417\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 165s 209ms/step - loss: 0.2452 - accuracy: 0.9049 - val_loss: 0.3574 - val_accuracy: 0.8531\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 168s 214ms/step - loss: 0.1980 - accuracy: 0.9266 - val_loss: 0.3730 - val_accuracy: 0.8594\n",
            "782/782 [==============================] - 83s 107ms/step - loss: 0.3519 - accuracy: 0.8630\n",
            "Test acurracy:  0.8629599809646606\n",
            "\n",
            "Testing performance:\n",
            " Loss: 0.352 - Accuracy: 0.863\n",
            "accuracy score: 0.863\n",
            "precision score: 0.863\n",
            "recall score: 0.863\n",
            "F1 score: 0.863\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86     12500\n",
            "           1       0.86      0.86      0.86     12500\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.86296, 0.8627278541733291, 0.86328, 0.8630038387715933)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dIYKgTuFNb"
      },
      "source": [
        "## Stacked bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFiFjk2g-SX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "outputId": "b8954ff2-f742-45b3-992d-4f687ee7a3cf"
      },
      "source": [
        "####   Creating the model  ####\n",
        "\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "\n",
        "## Block 1\n",
        "x = encoder (_input) \n",
        "x = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, mask_zero = True) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (x)\n",
        "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)) (x)   #return_sequences = True\n",
        "x = tf.keras.layers.Dense(64, activation = 'relu') (x)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (x)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "\n",
        "### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset,\n",
        "                    validation_steps = 30)\n",
        "##### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-56e326faa985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m history = model.fit (train_dataset,\n\u001b[1;32m     21\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     validation_data = test_dataset)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m### pred_label = tf.argmax(model.predict(test),1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1807, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5158, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, None, 1) vs (None,)).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5K-7fI-GguP"
      },
      "source": [
        "## BiLSTM with attention layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvTyvy8MGpoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd93518e-b822-4d09-8a3f-ea93f13729d7"
      },
      "source": [
        "#### Creating the model ####\n",
        "input_dim = len(encoder.get_vocabulary())\n",
        "_input = tf.keras.Input(shape = (1,), dtype=tf.string)\n",
        "vectorizer = encoder (_input) \n",
        "embeddings = tf.keras.layers.Embedding(input_dim = input_dim,output_dim = 64, input_length=max_length, mask_zero = True) (vectorizer)\n",
        "\n",
        "lstm_hs = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units,  return_sequences = True)) (embeddings)\n",
        "# attention layer\n",
        "attention = tf.keras.layers.Dense(1, activation = 'tanh') (lstm_hs)\n",
        "attention = tf.keras.layers.Flatten()(attention)\n",
        "attention = tf.keras.layers.Activation('softmax') (attention)\n",
        "attention = tf.keras.layers.RepeatVector (units*2) (attention)\n",
        "attention = tf.keras.layers.Permute ([2,1]) (attention)\n",
        "\n",
        "attention_weight = tf.keras.layers.Multiply()([lstm_hs, attention])\n",
        "attention_weight = tf.keras.layers.Lambda(lambda x: K.sum(x, axis = 1)) (attention_weight)\n",
        "\n",
        "## output layer\n",
        "output = tf.keras.layers.Dense(1, activation = 'sigmoid') (attention_weight)\n",
        "\n",
        "## combine in one\n",
        "model = tf.keras.Model(_input,output)\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "#### Compile the model for training\n",
        "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "#### Training the model\n",
        "history = model.fit (train_dataset,\n",
        "                    epochs = n_epochs, \n",
        "                    validation_data = test_dataset)\n",
        "#### testing the model\n",
        "### pred_label = tf.argmax(model.predict(test),1)\n",
        "pred_label = (model.predict(test_dataset) > 0.5).astype(\"int32\")\n",
        "true_label = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "test_loss, test_acc = model.evaluate (test_dataset)\n",
        "print('Test acurracy: ', test_acc)\n",
        "print('\\nTesting performance:\\n Loss: {:.3f} - Accuracy: {:.3f}'. format(test_loss, test_acc))\n",
        "printing_eval_scores (true_label, pred_label, report=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " text_vectorization (TextVector  (None, None)        0           ['input_3[0][0]']                \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 64)     640000      ['text_vectorization[1][0]']     \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, None, 128)    66048       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 1)      129         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, None)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, None)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 128, None)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, None, 128)    0           ['repeat_vector[0][0]']          \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, None, 128)    0           ['bidirectional[0][0]',          \n",
            "                                                                  'permute[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 128)          0           ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 706,306\n",
            "Trainable params: 706,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 254s 307ms/step - loss: 0.6168 - accuracy: 0.6839 - val_loss: 0.5473 - val_accuracy: 0.7920\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 235s 300ms/step - loss: 0.4639 - accuracy: 0.8350 - val_loss: 0.4399 - val_accuracy: 0.8554\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 243s 309ms/step - loss: 0.3882 - accuracy: 0.8696 - val_loss: 0.4944 - val_accuracy: 0.8053\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 243s 310ms/step - loss: 0.3882 - accuracy: 0.8620 - val_loss: 0.4299 - val_accuracy: 0.8455\n",
            "Epoch 5/5\n",
            "584/782 [=====================>........] - ETA: 43s - loss: 0.3832 - accuracy: 0.8756"
          ]
        }
      ]
    }
  ]
}